{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPl6j6_TIQzo"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9j5aw-FKS8k",
        "outputId": "3acc9763-5e16-4f00-9152-4764ca565aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0KViwZ0z8qx",
        "outputId": "cd249f2a-1df7-45ba-c351-dce225cde62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHrYitfxP1RF",
        "outputId": "e7353910-1be5-411c-bc14-b799f6b39fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_scatter-2.1.2%2Bpt23cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcu121/torch_sparse-0.6.18%2Bpt23cu121-cp310-cp310-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-scatter, torch-sparse\n",
            "Successfully installed torch-scatter-2.1.2+pt23cu121 torch-sparse-0.6.18+pt23cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting networkx<3.0\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: networkx\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "Successfully installed networkx-2.8.8\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=1.8) (1.25.2)\n",
            "Collecting csrgraph\n",
            "  Downloading csrgraph-0.1.28.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from csrgraph) (2.8.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from csrgraph) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from csrgraph) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from csrgraph) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from csrgraph) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from csrgraph) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from csrgraph) (4.66.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->csrgraph) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->csrgraph) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->csrgraph) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->csrgraph) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->csrgraph) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->csrgraph) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->csrgraph) (1.16.0)\n",
            "Building wheels for collected packages: csrgraph\n",
            "  Building wheel for csrgraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for csrgraph: filename=csrgraph-0.1.28-py3-none-any.whl size=17614 sha256=4f981d101f41466fdbbe46c38effeb61e22cb6644696b68e3db61f6c34c717d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/b5/63/fd61f029bb51e69d1d3ab578bc3361159f52bd99e46bd8d5c3\n",
            "Successfully built csrgraph\n",
            "Installing collected packages: csrgraph\n",
            "Successfully installed csrgraph-0.1.28\n",
            "Collecting DeepRobust\n",
            "  Downloading deeprobust-0.2.10-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.4/219.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (2.3.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (1.11.4)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (0.18.0+cu121)\n",
            "Collecting texttable>=1.6.2 (from DeepRobust)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (2.8.8)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (0.58.1)\n",
            "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (9.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.0 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (0.19.3)\n",
            "Collecting tensorboardX>=2.0 (from DeepRobust)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=3.0 in /usr/local/lib/python3.10/dist-packages (from DeepRobust) (4.66.4)\n",
            "Collecting gensim<4.0,>=3.8 (from DeepRobust)\n",
            "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim<4.0,>=3.8->DeepRobust) (1.16.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<4.0,>=3.8->DeepRobust) (6.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DeepRobust) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DeepRobust) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DeepRobust) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DeepRobust) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DeepRobust) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DeepRobust) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.1->DeepRobust) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.48.0->DeepRobust) (0.41.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.0->DeepRobust) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.0->DeepRobust) (2024.5.10)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.0->DeepRobust) (1.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->DeepRobust) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->DeepRobust) (3.5.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.0->DeepRobust) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->DeepRobust) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.2.0->DeepRobust) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.2.0->DeepRobust) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.2.0->DeepRobust) (1.3.0)\n",
            "Building wheels for collected packages: gensim\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n",
            "Failed to build gensim\n",
            "\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric\n",
        "!pip install tqdm\n",
        "!pip install torch\n",
        "\n",
        "# Required versions for plotting networkx\n",
        "!pip install 'networkx<3.0'\n",
        "!pip install 'scipy>=1.8'\n",
        "\n",
        "# Random walk library\n",
        "!pip install csrgraph\n",
        "\n",
        "# Dataset Prep\n",
        "!pip install DeepRobust\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyO04oO2QgS9"
      },
      "outputs": [],
      "source": [
        "class Arguments:\n",
        "  def __init__(self):\n",
        "    self.seed = 123\n",
        "    self.dataset = 'cora'\n",
        "    self.model_lr = 0.01\n",
        "    self.weight_decay = 5e-4\n",
        "    self.hidden_layers = 32\n",
        "    self.dropout = 0.5\n",
        "    self.protect_size = 0.1\n",
        "    self.ptb_rate = 0.25\n",
        "    self.do_sampling = 'Y'\n",
        "    self.sample_size = 500\n",
        "    self.num_samples = 20\n",
        "    self.ptb_epochs = 30\n",
        "    self.reg_epochs = 100\n",
        "args = Arguments()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -q"
      ],
      "metadata": {
        "id": "eaarBxEXQ9QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrTsaCAEREUR",
        "outputId": "fb37b277-262b-4a41-8621-34881907c539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt23cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt23cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "298PyBsyPq2X",
        "outputId": "07761851-f8d7-4efa-a1a5-4f5466604127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFpJzQJswUwE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "\n",
        "# libaries for GNN\n",
        "from torch_geometric.nn import DenseGCNConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldrxhtPlV21c"
      },
      "outputs": [],
      "source": [
        "# Optional libraries for plotting\n",
        "import networkx as nx\n",
        "from networkx.convert import to_networkx_graph\n",
        "from networkx.generators.classic import Graph\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "# Random Walk library\n",
        "import csrgraph as cg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import DenseGCNConv, GCNConv"
      ],
      "metadata": {
        "id": "PUYvxG-mTsYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtMibwtgVK7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bdac74-300f-4ead-e3a7-b6b13aa8b579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Environment ====\n",
            "  torch version: 2.3.0+cu121\n",
            "  device: cpu\n",
            "  torch seed: 123\n"
          ]
        }
      ],
      "source": [
        "################################################\n",
        "# Environment\n",
        "###############################################\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(\"cpu\")\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if device != 'cpu':\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "print('==== Environment ====')\n",
        "print(f'  torch version: {torch.__version__}')\n",
        "print(f'  device: {device}')\n",
        "print(f'  torch seed: {args.seed}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils Methods\n"
      ],
      "metadata": {
        "id": "KilfcTHnCp2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "8mqtcEnSCtR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6whXi57ZDRwi"
      },
      "outputs": [],
      "source": [
        "class Utils:\n",
        "  def idx_to_bool(idx, max_len=None):\n",
        "    \"\"\"\n",
        "    Converts an array of indices into a boolean array (where desired indices are True)\n",
        "    \"\"\"\n",
        "\n",
        "    if not max_len:\n",
        "        max_len = max(idx) + 1\n",
        "    arr = torch.zeros(max_len)\n",
        "    arr[idx] = 1\n",
        "    return arr > 0\n",
        "\n",
        "  def get_modified_adj(adj, perturbations):\n",
        "    \"\"\"\n",
        "    Inverts the adjacency matrix by a perturbation matrix (where 1 is to perturb, 0 is to not perturb)\n",
        "    Uses only the bottom triangle of the perturbation matrix\n",
        "    \"\"\"\n",
        "    tri = (adj + perturbations) - torch.mul(adj * perturbations, 2)\n",
        "    return tri\n",
        "\n",
        "  def make_symmetric(adj):\n",
        "    \"\"\"\n",
        "    Makes adj. matrix symmetric about the diagonal and sets the diagonal to 0.\n",
        "    Keeps the upper triangle.\n",
        "    \"\"\"\n",
        "    upper = torch.triu(adj)\n",
        "\n",
        "    lower = torch.rot90(torch.flip(\n",
        "        torch.triu(adj, diagonal=1), [0]), 3, [0, 1])\n",
        "\n",
        "    result = (upper + lower).fill_diagonal_(0)\n",
        "    return result\n",
        "\n",
        "  def to_edges(adj):\n",
        "    \"\"\"\n",
        "    Converts an adjacency matrix to a list of edges\n",
        "    \"\"\"\n",
        "    res = torch.triu(adj).float().nonzero().permute(1, 0)\n",
        "    return res\n",
        "\n",
        "  def bool_to_idx(bool_list):\n",
        "    \"\"\"\n",
        "    Converts a boolean array (where desired indices are True) into an array of indices\n",
        "    \"\"\"\n",
        "    return bool_list.nonzero()\n",
        "\n",
        "  def get_modified_adj(adj, perturbations):\n",
        "    \"\"\"\n",
        "    Inverts the adjacency matrix by a perturbation matrix (where 1 is to perturb, 0 is to not perturb)\n",
        "    Uses only the bottom triangle of the perturbation matrix\n",
        "    \"\"\"\n",
        "\n",
        "    tri = (adj + perturbations) - torch.mul(adj * perturbations, 2)\n",
        "    return tri\n",
        "\n",
        "  def projection(perturbations, n_perturbations):\n",
        "    \"\"\"\n",
        "    Get the projection of a perturbation matrix such that the sum over the distribution of perturbations is n_perturbations\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    perturbations : torch.tensor\n",
        "        probability distribution of perturbations\n",
        "    n_perturbations : int\n",
        "        desired number of perturbations\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "    out : torch.tensor\n",
        "        projected perturbation matrix\n",
        "\n",
        "    Examples\n",
        "    ---\n",
        "    >>>example\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def bisection(perturbations, a, b, n_perturbations, epsilon):\n",
        "\n",
        "          def func(perturbations, x, n_perturbations):\n",
        "            return torch.clamp(perturbations-x, 0, 1).sum() - n_perturbations\n",
        "\n",
        "          miu = a\n",
        "          while ((b-a) >= epsilon):\n",
        "            miu = (a+b)/2\n",
        "            # Check if middle point is root\n",
        "            if (func(perturbations, miu, n_perturbations) == 0.0):\n",
        "                break\n",
        "            # Decide the side to repeat the steps\n",
        "            if (func(perturbations, miu, n_perturbations)*func(perturbations, a, n_perturbations) < 0):\n",
        "                b = miu\n",
        "            else:\n",
        "                a = miu\n",
        "        # print(\"The value of root is : \",\"%.4f\" % miu)\n",
        "          return miu\n",
        "\n",
        "    # projected = torch.clamp(self.adj_changes, 0, 1)\n",
        "    if torch.clamp(perturbations, 0, 1).sum() > n_perturbations:\n",
        "          left = (perturbations - 1).min()\n",
        "          right = perturbations.max()\n",
        "          miu = bisection(perturbations, left, right, n_perturbations, epsilon=1e-5)\n",
        "          perturbations.data.copy_(torch.clamp(\n",
        "              perturbations.data - miu, min=0, max=1))\n",
        "    else:\n",
        "          perturbations.data.copy_(torch.clamp(\n",
        "              perturbations.data, min=0, max=1))\n",
        "\n",
        "    return perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Matrix"
      ],
      "metadata": {
        "id": "ZOIBE78ICxPH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSvJWFuMb3vZ"
      },
      "outputs": [],
      "source": [
        "class SamplingMatrix:\n",
        "    def __init__(self, g0, gX, adj, sample_size=3):\n",
        "        '''\n",
        "        input: g0 -> boolean tensor denoting protected set of nodes [num_nodes],\n",
        "                gX -> boolean tensor denoting authorized set of nodes [number of nodes],\n",
        "                adj -> tensor for adjacency matrix [num_nodes, num_nodes],\n",
        "                sample_size -> size of sample to be taken [int]\n",
        "\n",
        "        function: creates tensor of indices for the protected [len(gX)] and authorized [len(g0)] set of nodes,\n",
        "                  creates symmetric sampling matrix tensor [num_nodes, num_nodes] for both protected and authorized set where value at a given idx\n",
        "                    is 1 if the node is a member of the prescribed set and zero if it is not (similar to adjacency matrix)\n",
        "                  calls updateSamplingMatrix method\n",
        "        '''\n",
        "        self.prev_sampling_matrix = torch.tensor(1)\n",
        "\n",
        "        self.g0_ratio = torch.tensor(1)\n",
        "        self.gX_ratio = torch.tensor(1)\n",
        "        self.g0gX_ratio = torch.tensor(1)\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "        self.g0 = g0\n",
        "        self.g0_idx = Utils.bool_to_idx(g0).t()\n",
        "        self.gX = gX\n",
        "        self.gX_idx = Utils.bool_to_idx(gX).t()\n",
        "\n",
        "        self.g0_sampling = torch.zeros_like(adj)\n",
        "        ####\n",
        "        self.g0_sampling.index_fill_(0, Utils.bool_to_idx(g0).squeeze(), 1)\n",
        "        self.g0_sampling.index_fill_(1, Utils.bool_to_idx(gX).squeeze(), 0)\n",
        "        self.g0_sampling.fill_diagonal_(0)\n",
        "\n",
        "        self.gX_sampling = torch.zeros_like(adj)\n",
        "        self.gX_sampling.index_fill_(0, Utils.bool_to_idx(gX).squeeze(), 1)\n",
        "        self.gX_sampling.index_fill_(1, Utils.bool_to_idx(g0).squeeze(), 0)\n",
        "        self.gX_sampling.fill_diagonal_(0)\n",
        "\n",
        "        self.g0gX_sampling = torch.ones_like(adj)\n",
        "        self.g0gX_sampling -= self.g0_sampling + self.gX_sampling\n",
        "        self.g0gX_sampling.fill_diagonal_(0)\n",
        "\n",
        "        self.updateSamplingMatrix()\n",
        "\n",
        "    def getRatio(self):\n",
        "        '''\n",
        "        sums the ratios of g0, gX, and g0gX\n",
        "        '''\n",
        "        total = self.g0_ratio + self.gX_ratio + self.g0gX_ratio\n",
        "        print(f\"G0:  {self.g0_ratio / total:.2f}\")\n",
        "        print(f\"GX:  {self.gX_ratio / total:.2f}\")\n",
        "        print(f\"G0GX:{self.g0gX_ratio / total:.2f}\")\n",
        "\n",
        "    def updateSamplingMatrix(self):\n",
        "        try:\n",
        "          self.prev_sampling_matrix = self.sampling_matrix\n",
        "        except:\n",
        "          print('first run')\n",
        "\n",
        "        total = self.g0_ratio + self.gX_ratio + self.g0gX_ratio\n",
        "        constant_g0 =       ((self.g0_ratio * self.sample_size * 2) / (total * self.g0_sampling.sum()))\n",
        "        constant_gX =       ((self.gX_ratio * self.sample_size * 2) / (total * self.gX_sampling.sum()))\n",
        "        constant_g0gX =    ((self.g0gX_ratio * self.sample_size * 2) / (total * self.g0gX_sampling.sum()))\n",
        "\n",
        "        self.sampling_matrix = \\\n",
        "            self.g0gX_sampling * constant_g0gX + \\\n",
        "            self.g0_sampling * constant_g0 + \\\n",
        "            self.gX_sampling * constant_gX\n",
        "\n",
        "        self.sampling_matrix = torch.clamp(self.sampling_matrix, min=0, max=1)\n",
        "\n",
        "        self.sampling_matrix.triu_(diagonal=1)\n",
        "        # self.sampling_matrix.fill_diagonal_(0)\n",
        "\n",
        "    def get_sample(self):\n",
        "        \"\"\"\n",
        "        input: self -> sampling matrix [num nodes, num nodes]\n",
        "\n",
        "        draws random binary numbers using sampling_matrix values as probability distribution /\n",
        "          converts matrix to a list of edges\n",
        "\n",
        "        output: edgelist of size [2, ~0.20 * num nodes]\n",
        "        \"\"\"\n",
        "        if torch.isnan(self.sampling_matrix).any():\n",
        "          self.handle_nan_values(replace_with=1)\n",
        "        return Utils.to_edges(torch.bernoulli(self.sampling_matrix))\n",
        "\n",
        "    def updateRatio(self, g0_ratio, gX_ratio, g0gX_ratio):\n",
        "        \"\"\"\n",
        "        input: g0_ratio -> tensor value of ratio of g0-g0 edge in sampling matrix\n",
        "                gX_ratio -> tensor value of ratio of gX-gX edge in sampling matrix\n",
        "                g0gX_ratio ->tensor value of ratio of g0-gX edge in sampling matrix\n",
        "\n",
        "        output: changes ratio values and updates sampling matrix according to input\n",
        "        \"\"\"\n",
        "        self.g0_ratio = g0_ratio\n",
        "        self.gX_ratio = gX_ratio\n",
        "        self.g0gX_ratio = g0gX_ratio\n",
        "        total = self.g0_ratio + self.gX_ratio + self.g0gX_ratio\n",
        "        self.g0_ratio /= total\n",
        "        self.gX_ratio /= total\n",
        "        self.g0gX_ratio /= total\n",
        "\n",
        "        self.updateSamplingMatrix()\n",
        "\n",
        "    def updateByGrad(self, adj_grad, count):\n",
        "        \"\"\"\n",
        "        input: adj_grad -> gradient values for adjacency matrix [num nodes, num nodes],\n",
        "                count -> number of times a given idx has been sampled by get_ratio method [num nodes, num nodes]\n",
        "\n",
        "        output: sums the sampling matrix values after multiplying by count and updates the\n",
        "                  ratio values of all sets (g0, gX, g0gX) in the sampling matrix\n",
        "        \"\"\"\n",
        "        min_sample = self.sample_size / 10\n",
        "\n",
        "        g0r_count = (count * self.g0_sampling).sum() + min_sample\n",
        "        gXr_count = (count * self.gX_sampling).sum() + min_sample\n",
        "        g0gXr_count = count.sum() - (g0r_count + gXr_count) + min_sample\n",
        "\n",
        "        abs_grad = adj_grad.abs()\n",
        "        g0r = (abs_grad * self.g0_sampling).sum() / g0r_count\n",
        "        gXr = (abs_grad * self.gX_sampling).sum() / gXr_count\n",
        "        g0gXr = (abs_grad.sum() - (g0r + gXr)) / g0gXr_count\n",
        "\n",
        "        # abs_grad = adj_grad.abs()\n",
        "        # g0r = (abs_grad * self.g0_sampling)\n",
        "        # g0r = torch.median(g0r[g0r.nonzero()])\n",
        "\n",
        "        # gXr = (abs_grad * self.gX_sampling)\n",
        "        # gXr = torch.median(gXr[gXr.nonzero()])\n",
        "\n",
        "        # g0gXr = (abs_grad * self.g0gX_sampling)\n",
        "        # g0gXr = torch.median(g0gXr[g0gXr.nonzero()])\n",
        "\n",
        "        # print(g0r, gXr, g0gXr)\n",
        "        # print(g0r_count, gXr_count, g0gXr_count)\n",
        "\n",
        "        total = g0r + gXr + g0gXr\n",
        "        g0r /= total\n",
        "        gXr /= total\n",
        "        g0gXr /= total\n",
        "\n",
        "        self.updateRatio(\n",
        "            g0_ratio=(self.g0_ratio + g0r) / 2,\n",
        "            gX_ratio=(self.gX_ratio + gXr) / 2,\n",
        "            g0gX_ratio=(self.g0gX_ratio + g0gXr) / 2\n",
        "        )\n",
        "\n",
        "    def handle_nan_values(self, replace_with=1):\n",
        "      \"\"\"\n",
        "      replace nan values in sampling matrix with 1 or previous value\n",
        "      \"\"\"\n",
        "      if replace_with != 1:\n",
        "        idx = Utils.bool_to_idx(torch.isnan(self.sampling_matrix))\n",
        "        self.sampling_matrix[idx] = self.prev_sampling_matrix[idx]\n",
        "      else:\n",
        "        self.sampling_matrix = torch.nan_to_num(self.sampling_matrix, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ro09gkgBqxME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBMaJM-O_fkC"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7OZkGKj_tp7"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "import os.path as osp\n",
        "import urllib.request\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import zipfile\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class Dataset():\n",
        "    \"\"\"Dataset class contains four citation network datasets \"cora\", \"cora-ml\", \"citeseer\" and \"pubmed\",\n",
        "    and one blog dataset \"Polblogs\". Datasets \"ACM\", \"BlogCatalog\", \"Flickr\", \"UAI\",\n",
        "    \"Flickr\" are also available. See more details in https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph#supported-datasets.\n",
        "    The 'cora', 'cora-ml', 'polblogs' and 'citeseer' are downloaded from https://github.com/danielzuegner/gnn-meta-attack/tree/master/data, and 'pubmed' is from https://github.com/tkipf/gcn/tree/master/gcn/data.\n",
        "    Parameters\n",
        "    ----------\n",
        "    root : string\n",
        "        root directory where the dataset should be saved.\n",
        "    name : string\n",
        "        dataset name, it can be chosen from ['cora', 'citeseer', 'cora_ml', 'polblogs',\n",
        "        'pubmed', 'acm', 'blogcatalog', 'uai', 'flickr']\n",
        "    setting : string\n",
        "        there are three data splits settings. It can be chosen from ['nettack', 'gcn', 'prognn', 'traditional']\n",
        "        The 'nettack' setting follows nettack paper where they select the largest connected\n",
        "        components of the graph and use 10%/10%/80% nodes for training/validation/test .\n",
        "        The 'gcn' setting follows gcn paper where they use the full graph and 20 samples\n",
        "        in each class for traing, 500 nodes for validation, and 1000\n",
        "        nodes for test. (Note here 'netack' and 'gcn' setting do not provide fixed split, i.e.,\n",
        "        different random seed would return different data splits)\n",
        "        The 'traditional' setting uses a 0.7/0.2/0.1 fixed split for the train/val/test sets respectively. An\n",
        "        approximately equal ratio of all classes is used per set.\n",
        "    seed : int\n",
        "        random seed for splitting training/validation/test.\n",
        "    require_mask : bool\n",
        "        setting require_mask True to get training, validation and test mask\n",
        "        (self.train_mask, self.val_mask, self.test_mask)\n",
        "    Examples\n",
        "    --------\n",
        "\tWe can first create an instance of the Dataset class and then take out its attributes.\n",
        "\t>>> from deeprobust.graph.data import Dataset\n",
        "\t>>> data = Dataset(root='/tmp/', name='cora', seed=15)\n",
        "\t>>> adj, features, labels = data.adj, data.features, data.labels\n",
        "\t>>> idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, name, setting='gcn', seed=None, require_mask=False):\n",
        "        self.name = name.lower()\n",
        "        self.setting = setting.lower()\n",
        "\n",
        "        assert self.name in ['cora', 'citeseer', 'cora_ml', 'polblogs',\n",
        "                'pubmed', 'acm', 'blogcatalog', 'uai', 'flickr'], \\\n",
        "                'Currently only support cora, citeseer, cora_ml, ' + \\\n",
        "                'polblogs, pubmed, acm, blogcatalog, flickr'\n",
        "        assert self.setting in ['gcn', 'nettack', 'prognn', 'traditional', 'realistic'], \"Settings should be\" + \\\n",
        "                        \" choosen from ['gcn', 'nettack', 'prognn', 'traditional', 'realistic']\"\n",
        "\n",
        "        self.seed = seed\n",
        "        # self.url =  'https://raw.githubusercontent.com/danielzuegner/nettack/master/data/%s.npz' % self.name\n",
        "        self.url =  'https://raw.githubusercontent.com/danielzuegner/gnn-meta-attack/master/data/%s.npz' % self.name\n",
        "        self.root = osp.expanduser(osp.normpath(root))\n",
        "        self.data_folder = osp.join(root, self.name)\n",
        "        self.data_filename = self.data_folder + '.npz'\n",
        "        self.require_mask = require_mask\n",
        "\n",
        "        self.require_lcc = False if setting == 'gcn' else True\n",
        "        self.adj, self.features, self.labels = self.load_data()\n",
        "\n",
        "        if setting == 'prognn':\n",
        "            assert name in ['cora', 'citeseer', 'pubmed', 'cora_ml', 'polblogs'], \"ProGNN splits only \" + \\\n",
        "                        \"cora, citeseer, pubmed, cora_ml, polblogs\"\n",
        "            self.idx_train, self.idx_val, self.idx_test = self.get_prognn_splits()\n",
        "        else:\n",
        "            self.idx_train, self.idx_val, self.idx_test = self.get_train_val_test()\n",
        "        if self.require_mask:\n",
        "            self.get_mask()\n",
        "\n",
        "    def get_train_val_test(self):\n",
        "        \"\"\"Get training, validation, test splits according to self.setting (either 'nettack' or 'gcn' or 'traditional').\n",
        "        \"\"\"\n",
        "        if self.setting == 'nettack':\n",
        "            return get_train_val_test(nnodes=self.adj.shape[0], val_size=0.1, test_size=0.8, stratify=self.labels, seed=self.seed)\n",
        "        if self.setting == 'gcn':\n",
        "            return get_train_val_test_gcn(self.labels, seed=self.seed)\n",
        "        if self.setting == 'traditional':\n",
        "            return get_train_val_test_trad(self.labels, seed=self.seed)\n",
        "        if self.setting == 'realistic':\n",
        "            return get_train_val_test_real(self.labels, seed=self.seed)\n",
        "\n",
        "    def get_prognn_splits(self):\n",
        "        \"\"\"Get target nodes incides, which is the nodes with degree > 10 in the test set.\"\"\"\n",
        "        url = 'https://raw.githubusercontent.com/ChandlerBang/Pro-GNN/' + \\\n",
        "                     'master/splits/{}_prognn_splits.json'.format(self.name)\n",
        "        json_file = osp.join(self.root,\n",
        "                '{}_prognn_splits.json'.format(self.name))\n",
        "\n",
        "        if not osp.exists(json_file):\n",
        "            self.download_file(url, json_file)\n",
        "        # with open(f'/mnt/home/jinwei2/Projects/nettack/{dataset}_nettacked_nodes.json', 'r') as f:\n",
        "        with open(json_file, 'r') as f:\n",
        "            idx = json.loads(f.read())\n",
        "        return np.array(idx['idx_train']), \\\n",
        "               np.array(idx['idx_val']), np.array(idx['idx_test'])\n",
        "\n",
        "    def load_data(self):\n",
        "        print('Loading {} dataset...'.format(self.name))\n",
        "        if self.name == 'pubmed':\n",
        "            return self.load_pubmed()\n",
        "\n",
        "        if self.name in ['acm', 'blogcatalog', 'uai', 'flickr']:\n",
        "            return self.load_zip()\n",
        "\n",
        "        if not osp.exists(self.data_filename):\n",
        "            self.download_npz()\n",
        "\n",
        "        adj, features, labels = self.get_adj()\n",
        "        return adj, features, labels\n",
        "\n",
        "    def download_file(self, url, file):\n",
        "        print('Dowloading from {} to {}'.format(url, file))\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, file)\n",
        "        except:\n",
        "            raise Exception(\"Download failed! Make sure you have \\\n",
        "                    stable Internet connection and enter the right name\")\n",
        "\n",
        "    def download_npz(self):\n",
        "        \"\"\"Download adjacen matrix npz file from self.url.\n",
        "        \"\"\"\n",
        "        print('Downloading from {} to {}'.format(self.url, self.data_filename))\n",
        "        try:\n",
        "            urllib.request.urlretrieve(self.url, self.data_filename)\n",
        "            print('Done!')\n",
        "        except:\n",
        "            raise Exception('''Download failed! Make sure you have stable Internet connection and enter the right name''')\n",
        "\n",
        "    def download_pubmed(self, name):\n",
        "        url = 'https://raw.githubusercontent.com/tkipf/gcn/master/gcn/data/'\n",
        "        try:\n",
        "            print('Downloading', url)\n",
        "            urllib.request.urlretrieve(url + name, osp.join(self.root, name))\n",
        "            print('Done!')\n",
        "        except:\n",
        "            raise Exception('''Download failed! Make sure you have stable Internet connection and enter the right name''')\n",
        "\n",
        "    def download_zip(self, name):\n",
        "        url = 'https://raw.githubusercontent.com/ChandlerBang/Pro-GNN/master/other_datasets/{}.zip'.\\\n",
        "                format(name)\n",
        "        try:\n",
        "            print('Downlading', url)\n",
        "            urllib.request.urlretrieve(url, osp.join(self.root, name+'.zip'))\n",
        "            print('Done!')\n",
        "        except:\n",
        "            raise Exception('''Download failed! Make sure you have stable Internet connection and enter the right name''')\n",
        "\n",
        "    def load_zip(self):\n",
        "        data_filename = self.data_folder + '.zip'\n",
        "        name = self.name\n",
        "        if not osp.exists(data_filename):\n",
        "            self.download_zip(name)\n",
        "            with zipfile.ZipFile(data_filename, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.root)\n",
        "\n",
        "        feature_path = osp.join(self.data_folder, '{0}.feature'.format(name))\n",
        "        label_path = osp.join(self.data_folder, '{0}.label'.format(name))\n",
        "        graph_path = osp.join(self.data_folder, '{0}.edge'.format(name))\n",
        "\n",
        "        f = np.loadtxt(feature_path, dtype = float)\n",
        "        l = np.loadtxt(label_path, dtype = int)\n",
        "        features = sp.csr_matrix(f, dtype=np.float32)\n",
        "        # features = torch.FloatTensor(np.array(features.todense()))\n",
        "        struct_edges = np.genfromtxt(graph_path, dtype=int)\n",
        "        sedges = np.array(list(struct_edges), dtype=int).reshape(struct_edges.shape)\n",
        "        n = features.shape[0]\n",
        "        sadj = sp.coo_matrix((np.ones(sedges.shape[0]), (sedges[:, 0], sedges[:, 1])), shape=(n, n), dtype=np.float32)\n",
        "        sadj = sadj + sadj.T.multiply(sadj.T > sadj) - sadj.multiply(sadj.T > sadj)\n",
        "        label = np.array(l)\n",
        "\n",
        "        return sadj, features, label\n",
        "\n",
        "    def load_pubmed(self):\n",
        "        dataset = 'pubmed'\n",
        "        names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "        objects = []\n",
        "        for i in range(len(names)):\n",
        "            name = \"ind.{}.{}\".format(dataset, names[i])\n",
        "            data_filename = osp.join(self.root, name)\n",
        "\n",
        "            if not osp.exists(data_filename):\n",
        "                self.download_pubmed(name)\n",
        "\n",
        "            with open(data_filename, 'rb') as f:\n",
        "                if sys.version_info > (3, 0):\n",
        "                    objects.append(pkl.load(f, encoding='latin1'))\n",
        "                else:\n",
        "                    objects.append(pkl.load(f))\n",
        "\n",
        "        x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "\n",
        "\n",
        "        test_idx_file = \"ind.{}.test.index\".format(dataset)\n",
        "        if not osp.exists(osp.join(self.root, test_idx_file)):\n",
        "            self.download_pubmed(test_idx_file)\n",
        "\n",
        "        test_idx_reorder = parse_index_file(osp.join(self.root, test_idx_file))\n",
        "        test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "        features = sp.vstack((allx, tx)).tolil()\n",
        "        features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "        adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "        labels = np.vstack((ally, ty))\n",
        "        labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "        labels = np.where(labels)[1]\n",
        "        return adj, features, labels\n",
        "\n",
        "    def get_adj(self):\n",
        "        adj, features, labels = self.load_npz(self.data_filename)\n",
        "        adj = adj + adj.T\n",
        "        adj = adj.tolil()\n",
        "        adj[adj > 1] = 1\n",
        "\n",
        "        if self.require_lcc:\n",
        "            lcc = self.largest_connected_components(adj)\n",
        "            adj = adj[lcc][:, lcc]\n",
        "            features = features[lcc]\n",
        "            labels = labels[lcc]\n",
        "            assert adj.sum(0).A1.min() > 0, \"Graph contains singleton nodes\"\n",
        "\n",
        "        # whether to set diag=0?\n",
        "        adj.setdiag(0)\n",
        "        adj = adj.astype(\"float32\").tocsr()\n",
        "        adj.eliminate_zeros()\n",
        "\n",
        "        assert np.abs(adj - adj.T).sum() == 0, \"Input graph is not symmetric\"\n",
        "        assert adj.max() == 1 and len(np.unique(adj[adj.nonzero()].A1)) == 1, \"Graph must be unweighted\"\n",
        "\n",
        "        return adj, features, labels\n",
        "\n",
        "    def load_npz(self, file_name, is_sparse=True):\n",
        "        with np.load(file_name) as loader:\n",
        "            # loader = dict(loader)\n",
        "            if is_sparse:\n",
        "                adj = sp.csr_matrix((loader['adj_data'], loader['adj_indices'],\n",
        "                                            loader['adj_indptr']), shape=loader['adj_shape'])\n",
        "                if 'attr_data' in loader:\n",
        "                    features = sp.csr_matrix((loader['attr_data'], loader['attr_indices'],\n",
        "                                                 loader['attr_indptr']), shape=loader['attr_shape'])\n",
        "                else:\n",
        "                    features = None\n",
        "                labels = loader.get('labels')\n",
        "            else:\n",
        "                adj = loader['adj_data']\n",
        "                if 'attr_data' in loader:\n",
        "                    features = loader['attr_data']\n",
        "                else:\n",
        "                    features = None\n",
        "                labels = loader.get('labels')\n",
        "        if features is None:\n",
        "            features = np.eye(adj.shape[0])\n",
        "        features = sp.csr_matrix(features, dtype=np.float32)\n",
        "        return adj, features, labels\n",
        "\n",
        "    def largest_connected_components(self, adj, n_components=1):\n",
        "        \"\"\"Select k largest connected components.\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tadj : scipy.sparse.csr_matrix\n",
        "\t\t\tinput adjacency matrix\n",
        "\t\tn_components : int\n",
        "\t\t\tn largest connected components we want to select\n",
        "\t\t\"\"\"\n",
        "\n",
        "        _, component_indices = sp.csgraph.connected_components(adj)\n",
        "        component_sizes = np.bincount(component_indices)\n",
        "        components_to_keep = np.argsort(component_sizes)[::-1][:n_components]  # reverse order to sort descending\n",
        "        nodes_to_keep = [\n",
        "            idx for (idx, component) in enumerate(component_indices) if component in components_to_keep]\n",
        "        print(\"Selecting {0} largest connected components\".format(n_components))\n",
        "        return nodes_to_keep\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{0}(adj_shape={1}, feature_shape={2})'.format(self.name, self.adj.shape, self.features.shape)\n",
        "\n",
        "    def get_mask(self):\n",
        "        idx_train, idx_val, idx_test = self.idx_train, self.idx_val, self.idx_test\n",
        "        labels = self.onehot(self.labels)\n",
        "\n",
        "        def get_mask(idx):\n",
        "            mask = np.zeros(labels.shape[0], dtype=np.bool)\n",
        "            mask[idx] = 1\n",
        "            return mask\n",
        "\n",
        "        def get_y(idx):\n",
        "            mx = np.zeros(labels.shape)\n",
        "            mx[idx] = labels[idx]\n",
        "            return mx\n",
        "\n",
        "        self.train_mask = get_mask(self.idx_train)\n",
        "        self.val_mask = get_mask(self.idx_val)\n",
        "        self.test_mask = get_mask(self.idx_test)\n",
        "        self.y_train, self.y_val, self.y_test = get_y(idx_train), get_y(idx_val), get_y(idx_test)\n",
        "\n",
        "    def onehot(self, labels):\n",
        "        eye = np.identity(labels.max() + 1)\n",
        "        onehot_mx = eye[labels]\n",
        "        return onehot_mx\n",
        "\n",
        "def parse_index_file(filename):\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "def get_train_val_test(nnodes, val_size=0.1, test_size=0.8, stratify=None, seed=None):\n",
        "    \"\"\"This setting follows nettack/mettack, where we split the nodes\n",
        "    into 10% training, 10% validation and 80% testing data\n",
        "    Parameters\n",
        "    ----------\n",
        "    nnodes : int\n",
        "        number of nodes in total\n",
        "    val_size : float\n",
        "        size of validation set\n",
        "    test_size : float\n",
        "        size of test set\n",
        "    stratify :\n",
        "        data is expected to split in a stratified fashion. So stratify should be labels.\n",
        "    seed : int or None\n",
        "        random seed\n",
        "    Returns\n",
        "    -------\n",
        "    idx_train :\n",
        "        node training indices\n",
        "    idx_val :\n",
        "        node validation indices\n",
        "    idx_test :\n",
        "        node test indices\n",
        "    \"\"\"\n",
        "\n",
        "    assert stratify is not None, 'stratify cannot be None!'\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    idx = np.arange(nnodes)\n",
        "    train_size = 1 - val_size - test_size\n",
        "    idx_train_and_val, idx_test = train_test_split(idx,\n",
        "                                                   random_state=None,\n",
        "                                                   train_size=train_size + val_size,\n",
        "                                                   test_size=test_size,\n",
        "                                                   stratify=stratify)\n",
        "\n",
        "    if stratify is not None:\n",
        "        stratify = stratify[idx_train_and_val]\n",
        "\n",
        "    idx_train, idx_val = train_test_split(idx_train_and_val,\n",
        "                                          random_state=None,\n",
        "                                          train_size=(train_size / (train_size + val_size)),\n",
        "                                          test_size=(val_size / (train_size + val_size)),\n",
        "                                          stratify=stratify)\n",
        "\n",
        "\n",
        "def get_train_val_test_gcn(labels, seed=None):\n",
        "    \"\"\"This setting follows gcn, where we randomly sample 20 instances for each class\n",
        "    as training data, 500 instances as validation data, 1000 instances as test data.\n",
        "    Note here we are not using fixed splits. When random seed changes, the splits\n",
        "    will also change.\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : numpy.array\n",
        "        node labels\n",
        "    seed : int or None\n",
        "        random seed\n",
        "    Returns\n",
        "    -------\n",
        "    idx_train :\n",
        "        node training indices\n",
        "    idx_val :\n",
        "        node validation indices\n",
        "    idx_test :\n",
        "        node test indices\n",
        "    \"\"\"\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    idx = np.arange(len(labels))\n",
        "    nclass = labels.max() + 1\n",
        "    idx_train = []\n",
        "    idx_unlabeled = []\n",
        "    for i in range(nclass):\n",
        "        labels_i = idx[labels==i]\n",
        "        labels_i = np.random.permutation(labels_i)\n",
        "        idx_train = np.hstack((idx_train, labels_i[: 20])).astype(int)\n",
        "        idx_unlabeled = np.hstack((idx_unlabeled, labels_i[20: ])).astype(int)\n",
        "\n",
        "    idx_unlabeled = np.random.permutation(idx_unlabeled)\n",
        "    idx_val = idx_unlabeled[: 500]\n",
        "    idx_test = idx_unlabeled[500: 1500]\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def get_train_val_test_trad(labels, seed=None):\n",
        "    \"\"\"Splits data using a traditional fixed split of 0.7/0.2/0.1 for train/val/test\n",
        "    sets respectively. Each class is distributed so that each set contains\n",
        "    an approximately equal ratio of nodes per class.\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : numpy.array\n",
        "        node labels\n",
        "    seed : int or None\n",
        "        random seed\n",
        "    Returns\n",
        "    -------\n",
        "    idx_train :\n",
        "        node training indices\n",
        "    idx_val :\n",
        "        node validation indices\n",
        "    idx_test :\n",
        "        node test indices\n",
        "    \"\"\"\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    nclass = labels.max() + 1\n",
        "    num_nodes = len(labels)\n",
        "    train_split = math.floor((0.7 * num_nodes) / (nclass))\n",
        "    val_split = math.floor((0.2 * num_nodes) / (nclass))\n",
        "    test_split = math.floor((0.1 * num_nodes) / (nclass))\n",
        "\n",
        "    idx = np.arange(len(labels))\n",
        "    idx_train = []\n",
        "    idx_val = []\n",
        "    idx_test = []\n",
        "    idx_unlabeled = []\n",
        "    for i in range(nclass):\n",
        "        labels_i = idx[labels==i]\n",
        "        labels_i = np.random.permutation(labels_i)\n",
        "        idx_train = np.hstack((idx_train, labels_i[: train_split])).astype(int)\n",
        "        idx_val = np.hstack((idx_val, labels_i[train_split : train_split + val_split])).astype(int)\n",
        "        idx_test = np.hstack((idx_test, labels_i[train_split + val_split : train_split + val_split + test_split])).astype(int)\n",
        "\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def get_train_val_test_real(labels, seed=None):\n",
        "    \"\"\"Splits data using a fixed split of 0.7/0.2/0.1 for train/val/test\n",
        "    sets respectively. Each class is distributed so that each set contains\n",
        "    a ratio of classes ~equal to class ratio in the entire dataset.\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : numpy.array\n",
        "        node labels\n",
        "    seed : int or None\n",
        "        random seed\n",
        "    Returns\n",
        "    -------\n",
        "    idx_train :\n",
        "        node training indices\n",
        "    idx_val :\n",
        "        node validation indices\n",
        "    idx_test :\n",
        "        node test indices\n",
        "    \"\"\"\n",
        "\n",
        "    nclass = labels.max() + 1\n",
        "    num_nodes = len(labels)\n",
        "    train_split = math.floor((0.7 * num_nodes))\n",
        "    val_split = math.floor((0.2 * num_nodes))\n",
        "    test_split = math.floor((0.1 * num_nodes))\n",
        "\n",
        "    idx = np.arange(len(labels))\n",
        "    idx_train = []\n",
        "    idx_val = []\n",
        "    idx_test = []\n",
        "    idx_unlabeled = []\n",
        "    for i in range(nclass):\n",
        "        ratio = sum(labels==i) / len(labels)\n",
        "        r_train = math.floor(train_split * ratio)\n",
        "        r_val = math.floor(val_split * ratio)\n",
        "        r_test = math.floor(test_split * ratio)\n",
        "        labels_i = idx[labels==i]\n",
        "        labels_i = np.random.permutation(labels_i)\n",
        "        idx_train = np.hstack((idx_train, labels_i[: r_train])).astype(int)\n",
        "        idx_val = np.hstack((idx_val, labels_i[r_train : r_train + r_val])).astype(int)\n",
        "        idx_test = np.hstack((idx_test, labels_i[r_train + r_val : r_train + r_val + r_test])).astype(int)\n",
        "\n",
        "    return idx_train, idx_val, idx_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMSJdIP8Ct9g"
      },
      "outputs": [],
      "source": [
        "class Graph:\n",
        "    def __init__(self, adj, labels, features, idx_train, idx_val, idx_test, split_seed, device):\n",
        "        self.adj = adj.to(device)\n",
        "        self.features = features.to(device)\n",
        "        self.labels = labels.to(device)\n",
        "        self.idx_train = idx_train.to(device)\n",
        "        self.idx_val = idx_val.to(device)\n",
        "        self.idx_test = idx_test.to(device)\n",
        "        self.split_seed = split_seed\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<Graph {self.adj.shape[0]}x{self.adj.shape[1]}>\"\n",
        "\n",
        "    def summarize(self, name=\"\"):\n",
        "        print()\n",
        "        print(f'[i] Dataset Summary: {name}')\n",
        "        print(f'\\tadj shape: {list(self.adj.shape)}')\n",
        "        print(f'\\tfeature shape: {list(self.features.shape)}')\n",
        "        print(f'\\tnum labels: {self.labels.max().item()+1}')\n",
        "        print(f'\\tsplit seed: {self.split_seed}')\n",
        "        print(\n",
        "            f'\\ttrain|val|test: {self.idx_train.sum()}|{self.idx_val.sum()}|{self.idx_test.sum()}')\n",
        "\n",
        "    def split(self, nsplits):\n",
        "        indices = torch.zeros(10, dtype=torch.bool)\n",
        "        return None\n",
        "\n",
        "    def numEdges(self):\n",
        "        return self.adj.sum() / 2\n",
        "\n",
        "    def adjacency(self):\n",
        "        return self.adj\n",
        "\n",
        "    def numNodes(self):\n",
        "        return self.adj.shape[0]\n",
        "\n",
        "    def getSample(self, size):\n",
        "        indices = (torch.bernoulli(torch.empty(1, size)[0].uniform_(0,1))) > 0.5\n",
        "        maskA = indices.nonzero().t()[0]\n",
        "        maskB = (~indices).nonzero().t()[0]\n",
        "\n",
        "        return maskA, maskB\n",
        "\n",
        "    def getSubgraph(self, indices):\n",
        "        return Graph(\n",
        "            adj=self.adj[indices].t()[indices].t(),\n",
        "            features=self.features[indices],\n",
        "            labels=self.labels[indices],\n",
        "            idx_train=self.idx_train[indices],\n",
        "            idx_val=self.idx_val[indices],\n",
        "            idx_test=self.idx_test[indices],\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    def getEntropies(self):\n",
        "        # TODO\n",
        "        return 0\n",
        "\n",
        "def getGraph(root, name, setting, seed, device, verbose=True):\n",
        "    data = Dataset(root, name, setting, seed)\n",
        "\n",
        "    adj = torch.LongTensor(data.adj.todense())\n",
        "    adj = Utils.make_symmetric(adj)\n",
        "    labels = torch.LongTensor(data.labels)\n",
        "    features = torch.FloatTensor(np.array(data.features.todense()))\n",
        "\n",
        "    def indices_to_bool(indices, length):\n",
        "        arr = torch.zeros(length)\n",
        "        arr[indices] = 1\n",
        "        return arr > 0\n",
        "\n",
        "    idx_train = Utils.idx_to_bool(data.idx_train, features.shape[0])\n",
        "    idx_val = Utils.idx_to_bool(data.idx_val, features.shape[0])\n",
        "    idx_test = Utils.idx_to_bool(data.idx_test, features.shape[0])\n",
        "\n",
        "    return Graph(adj, labels, features, idx_train, idx_val, idx_test, seed, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sBQZ5ViIyCf"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woHfxdGC4jGd",
        "outputId": "42e12f7d-fb50-401b-b375-bfd650bb0c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Dataset: cora ====\n",
            "Loading cora dataset...\n",
            "Downloading from https://raw.githubusercontent.com/danielzuegner/gnn-meta-attack/master/data/cora.npz to ./cora.npz\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "args.dataset = 'cora'\n",
        "\n",
        "print(f'==== Dataset: {args.dataset} ====')\n",
        "graph = getGraph(root='.', name=args.dataset, setting='gcn', seed=args.seed, device=device)\n",
        "\n",
        "# CiteSeer: worse initial accuracy, C&S hyperparameters had to be lowered in order to run\n",
        "# flickr: initial acc ~50%\n",
        "# BlogCatalog\n",
        "# cora: initial acc ~79%\n",
        "# PolBlogs: initial acc ~86%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUwaSq43fBFt"
      },
      "outputs": [],
      "source": [
        "graph.num_nodes = graph.numNodes() #graph.features.shape[0]\n",
        "graph.num_features = graph.features.shape[1]\n",
        "graph.num_edges = graph.numEdges() #graph.edge_index.shape[1]\n",
        "graph.device = device\n",
        "graph.edge_index = graph.adj.nonzero().t().contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhVNg7hqlCHA"
      },
      "outputs": [],
      "source": [
        "## NetworkX Graph functionality ##\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "data = Data(x=graph.features, y=graph.labels, edge_index=graph.edge_index)\n",
        "\n",
        "nxg = nx.Graph()\n",
        "nxg = to_networkx(data, to_undirected=True)\n",
        "graph.nx_g = nxg\n",
        "\n",
        "graph.clustering = nx.clustering(nxg)\n",
        "graph.centrality = nx.degree_centrality(nxg)\n",
        "graph.degree = nxg.degree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.is_connected(graph.nx_g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPX9Yu5izZeh",
        "outputId": "6423fa01-2f7d-4547-d04c-79fd302a88a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUd1xOVNI7Jk"
      },
      "source": [
        "## Adjacency matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3na5mvgfvxc"
      },
      "outputs": [],
      "source": [
        "def to_sym_adj(edge_index, num_nodes):\n",
        "  '''\n",
        "  input: edge_index is tensor of shape [2, num_edges] and num_nodes is integer\n",
        "  output: tensors of shape [num_nodes, num_nodes] that represents the symmetric adjaceny matrix of edge_index\n",
        "  '''\n",
        "  num_edges = graph.edge_index.shape[1]\n",
        "  adj = torch.zeros(num_nodes, num_nodes )\n",
        "  for i in range(num_edges):\n",
        "    u, v = graph.edge_index[0][i],  graph.edge_index[1][i]\n",
        "    adj[u][v] = 1\n",
        "    adj[v][u] = 1\n",
        "  return adj\n",
        "\n",
        "def to_edge_index(adj_matrix):\n",
        "  '''\n",
        "  input: adj matrix tensor of shape [num_nodes, num_nodes]\n",
        "  output: tensor of shape [2, num_edges]\n",
        "  '''\n",
        "  edge_index = graph.adj.nonzero().t().contiguous()\n",
        "  return edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEzMEN6kcfeu"
      },
      "outputs": [],
      "source": [
        "graph.adj = to_sym_adj(graph.edge_index, graph.num_nodes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "adj_edge_index = to_dense_adj(graph.edge_index).squeeze()\n",
        "(adj_edge_index == graph.adj).all()"
      ],
      "metadata": {
        "id": "gmisOcsBw18R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b4cf56-3136-4135-c052-81ff6430f6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Designate protected nodes g0\n",
        "g0 = torch.rand(graph.numNodes()) <= args.protect_size\n",
        "g0 = g0.to(device)\n",
        "gX = ~g0.to(device)\n",
        "\n",
        "print(f\"Number of protected nodes: {g0.sum():.0f}\")\n",
        "print(f\"Ratio of protected nodes: {g0.sum() / graph.features.shape[0]:.2%}\")\n",
        "\n",
        "# Sampling matrix\n",
        "samplingMatrix = SamplingMatrix(g0, gX, graph.adj, args.sample_size)\n",
        "print((samplingMatrix.sampling_matrix))"
      ],
      "metadata": {
        "id": "PqflHjh4Dsi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9f34e3-7afd-48f6-b7e5-242628359582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of protected nodes: 285\n",
            "Ratio of protected nodes: 10.52%\n",
            "first run\n",
            "tensor([[0.0000e+00, 5.6800e-05, 5.6800e-05,  ..., 5.6800e-05, 5.6800e-05,\n",
            "         5.6800e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 5.6800e-05,  ..., 5.6800e-05, 5.6800e-05,\n",
            "         5.6800e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6800e-05, 5.6800e-05,\n",
            "         5.6800e-05],\n",
            "        ...,\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.6800e-05,\n",
            "         5.6800e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         5.6800e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (samplingMatrix.sampling_matrix>=0).all() and (samplingMatrix.sampling_matrix<=1).all()"
      ],
      "metadata": {
        "id": "7EhKzNCWIaku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.bernoulli?"
      ],
      "metadata": {
        "id": "q5NNJjH7HXE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#samplingMatrix.get_sample()"
      ],
      "metadata": {
        "id": "9QgKdROUDsq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#samplingMatrix.getRatio()"
      ],
      "metadata": {
        "id": "OQEaXq9zDsq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-RL3WcGINfJ"
      },
      "source": [
        "# Metrics Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvTSQmSHyjjQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Metrics:\n",
        "  def acc(predictions, labels):\n",
        "    \"\"\"\n",
        "    input: float tensor of predicted values of nodes [num_nodes, num_classes], int tensor of ground truth labels of nodes [num_nodes]\n",
        "\n",
        "    output: accuracy value of predictions calcualted by dividing the number of correct predictions by total number of predictions\n",
        "    \"\"\"\n",
        "    correct = (predictions.argmax(1) == labels).sum()\n",
        "    acc = correct / predictions.size(dim=0)\n",
        "    return acc.item()\n",
        "\n",
        "  def calc_acc(model, features, adj, labels, idx=False):\n",
        "    \"\"\"\n",
        "    input: GNN model, tensor of graph features[num_nodes, num_features], adj matrix[num_nodes, num_nodes],\n",
        "              tensor of graph labels[num_nodes], desired indices to compare truth values[num_nodes]\n",
        "\n",
        "    output: calculated acc of correctly predicted nodes (float)\n",
        "    \"\"\"\n",
        "    if not idx:\n",
        "        idx = torch.ones_like(labels) > 0\n",
        "\n",
        "    pred = model(features, adj)\n",
        "\n",
        "    correct = (pred.argmax(1)[idx] == labels[idx]).sum()\n",
        "    acc = correct / idx.sum()\n",
        "    return acc.item()\n",
        "\n",
        "  def partial_acc(predictions, labels, g0, g_g0, verbose=True):\n",
        "    \"\"\"\n",
        "    input: float tensor predicted values of nodes [num_nodes, num_classes], int tensor of ground truth label of nodes [num_nodes],\n",
        "       boolean tensor of denoting authorized set of nodes [num_nodes], boolean tensor denoting protected set of nodes [number of nodes]\n",
        "\n",
        "    output: dict with calculated accuracy values of both the authorized and protected set of nodes\n",
        "    \"\"\"\n",
        "    g0_acc = Metrics.acc(predictions[g0], labels[g0])\n",
        "    gX_acc = Metrics.acc(predictions[g_g0], labels[g_g0])\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"G0: {g0_acc:.2%}\")\n",
        "        print(f\"GX: {gX_acc:.2%}\")\n",
        "\n",
        "    return {\n",
        "        \"g0\": g0_acc,\n",
        "        \"gX\": gX_acc\n",
        "    }\n",
        "\n",
        "  def mask_adj(adj, bool_list, device):\n",
        "    \"\"\"\n",
        "    input: adj matrix [num_nodes, num_nodes], list of bools where desired indices are true [num_nodes], device architecture [string]\n",
        "\n",
        "    output: modified adj matrix [num_nodes, num_nodes] of only true values of bool_list\n",
        "    \"\"\"\n",
        "    idx = Utils.bool_to_idx(bool_list).squeeze().to(device)\n",
        "\n",
        "    temp_adj = adj.clone().to(device)\n",
        "    temp_adj.index_fill_(dim=0, index=idx, value=0)\n",
        "    diff = adj - temp_adj\n",
        "\n",
        "    temp_adj = diff.clone().to(device)\n",
        "    temp_adj.index_fill_(dim=1, index=idx, value=0)\n",
        "    diff = diff - temp_adj\n",
        "\n",
        "    # add = int(diff.clamp(0,1).sum() / 2)\n",
        "    # remove = int(diff.clamp(-1,0).abs().sum() / 2)\n",
        "\n",
        "    return diff\n",
        "\n",
        "  def show_metrics(changes, labels, g0, device, verbose=True):\n",
        "    \"\"\"\n",
        "    Prints the changes in edges with respect to g0 and labels of a diff\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    par_name : par_type\n",
        "        par_description\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "    out : ret_type\n",
        "        ret_description\n",
        "\n",
        "    Examples\n",
        "    ---\n",
        "    >>>example\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def print_same_diff(type, adj):\n",
        "        edges = Utils.to_edges(adj)\n",
        "        same = 0\n",
        "        for edge in edges.t():\n",
        "            same += int(labels[edge[0]].item() == labels[edge[1]].item())\n",
        "\n",
        "        diff = edges.shape[1] - same\n",
        "\n",
        "        if verbose: print(f\"     {type}   {int(same)}  \\t{int(diff)}  \\t{int(same+diff)}\")\n",
        "        return { \"same\": int(same), \"diff\": int(diff), \"total\": int(same + diff)}\n",
        "\n",
        "    def print_add_remove(adj):\n",
        "        \"\"\"prints change in edges\"\"\"\n",
        "        add = adj.clamp(0,1)\n",
        "        remove = adj.clamp(-1,0).abs()\n",
        "        if verbose: print(\"                A-A\\tA-B\\tTOTAL\")\n",
        "        numAdd = print_same_diff(\"     (+)\", add)\n",
        "        numRemove = print_same_diff(\"     (-)\", remove)\n",
        "\n",
        "        return {\"add\": numAdd, \"remove\": numRemove, \"total\": numAdd[\"total\"] + numRemove[\"total\"]}\n",
        "    # print_add_remove(changes)\n",
        "\n",
        "    r = {}\n",
        "\n",
        "    if verbose: print(\"     Within G0 ====\")\n",
        "    g0_adj = Metrics.mask_adj(changes, g0, device)\n",
        "    r[\"g0\"] = print_add_remove(g0_adj)\n",
        "\n",
        "    if verbose: print(\"     Within GX ====\")\n",
        "    gX_adj = Metrics.mask_adj(changes, ~g0, device)\n",
        "    r[\"gX\"] = print_add_remove(gX_adj)\n",
        "\n",
        "    if verbose: print(\"     Between G0-GX ====\")\n",
        "    g0gX_adj = (changes - g0_adj - gX_adj)\n",
        "    r[\"g0gX\"] = print_add_remove(g0gX_adj)\n",
        "\n",
        "    if verbose: print()\n",
        "    print_same_diff(\"   TOTAL\", changes)\n",
        "\n",
        "    return r\n",
        "\n",
        "def calc_entropy(data: torch.tensor, numbins=50):\n",
        "    bins = torch.histc(data, bins=numbins)\n",
        "    bins /= bins.sum()\n",
        "    return ((bins * torch.log2(bins)).nan_to_num().sum() * -1).item()\n",
        "\n",
        "def calc_correlation(tensor1: torch.tensor, tensor2: torch.tensor):\n",
        "    \"\"\"calculates correlation between two tensor objects\"\"\"\n",
        "    cat = torch.cat((tensor1.unsqueeze(0).cpu(), tensor2.unsqueeze(0).cpu())).numpy()\n",
        "    return np.corrcoef(cat)[0][1]\n",
        "\n",
        "def get_ent_cor(features: torch.tensor, labels: torch.tensor, num: int=10, rand=False, offset=0):\n",
        "    \"\"\"\n",
        "    Return the features with most entropy and/or correlation\n",
        "\n",
        "    Parameters\n",
        "    ---\n",
        "    par_name : par_type\n",
        "        par_description\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "    entropy, correlation, index\n",
        "\n",
        "    Examples\n",
        "    ---\n",
        "    >>>example\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ent_cor = torch.zeros(3, features.shape[1])\n",
        "\n",
        "    for r in range(features.shape[1]):\n",
        "        feat = features.t()[r]\n",
        "        entropy = calc_entropy(feat)\n",
        "        correlation = abs(calc_correlation(feat, labels))\n",
        "        ent_cor[0][r] = entropy\n",
        "        ent_cor[1][r] = correlation\n",
        "        ent_cor[2][r] = entropy + correlation\n",
        "\n",
        "    ent_cor.nan_to_num_()\n",
        "\n",
        "    if rand:\n",
        "        idx = torch.tensor(random.sample(range(features.shape[1]), num))\n",
        "    else:\n",
        "        idx = torch.topk(ent_cor[2], num + offset, sorted=True).indices[offset:]\n",
        "\n",
        "    data = ent_cor[:,idx]\n",
        "\n",
        "    return data[0], data[1], idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_zW17gsJsno"
      },
      "source": [
        "# Correct and Smooth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Change the implementation of Correct_and_Smooth, variables in init should be parameters (num_correction_layers=50, correction_alpha=1.0,\n",
        "num_smoothing_layers=50, smoothing_alpha=0.8,\n",
        "autoscale=False, scale=20.)"
      ],
      "metadata": {
        "id": "mvalZg6y1zua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF0RWWFMJ37b"
      },
      "outputs": [],
      "source": [
        "from numpy import linspace\n",
        "\n",
        "class Correct_and_Smooth():\n",
        "    def __init__(self, adj, labels, idx_train, idx_val, idx_test):\n",
        "\n",
        "      self.A = adj\n",
        "      self.labels = labels\n",
        "      self.idx_train = idx_train\n",
        "      self.idx_val = idx_val\n",
        "      self.idx_test = idx_test\n",
        "      self.D = torch.diag(self.A.sum(-1))\n",
        "      self.D_inv_sqrt = self.D.pow(-0.5)\n",
        "\n",
        "      self.D_inv_sqrt[self.D_inv_sqrt == float('inf')] = 0 #\n",
        "      self.S = (self.D_inv_sqrt @ self.A @ self.D_inv_sqrt).to(device) # Normalized adj matrix\n",
        "\n",
        "      self.y = self.labels.squeeze()\n",
        "      self.Y = F.one_hot(self.y, int(max(self.y) + 1)).to(device)\n",
        "\n",
        "    def residual_error(self, Z):\n",
        "      \"\"\"\n",
        "      Form residual error matrix E = Z - Y for training data\n",
        "      \"\"\"\n",
        "      E = Z - self.Y\n",
        "      E[self.idx_test + self.idx_val] = 0\n",
        "      return E\n",
        "\n",
        "    def correct(self, E, alpha1 = 0.8, eps = 1e-5, show_progress=True):\n",
        "      \"\"\"\n",
        "      Step 1: Smooth error across the graph\n",
        "      E^(t+1) = (1-alpha1)E + alpha * S @ E^(t) -> Ehat  # alpha1 is hyperparameter\n",
        "      \"\"\"\n",
        "      itr = 0\n",
        "      Ehat = E\n",
        "      diff = eps\n",
        "      while diff >= eps and itr < 500:\n",
        "        Et = (1 - alpha1) * E + alpha1 * (self.S @ Ehat) # (S @ Ehat) sets new error for each node as weighted average of neighbors error\n",
        "        diff = float(torch.norm(Ehat - Et))\n",
        "        Ehat = Et\n",
        "        if show_progress:\n",
        "            print(f\"Iteration: [{itr}] - Diff: [{diff}]\")\n",
        "        itr += 1\n",
        "      return Ehat\n",
        "\n",
        "    def autoscale(self, E, Ehat, Z): # Size of new errors now in the same scale as original errors\n",
        "      \"\"\"\n",
        "      Step 2:  Adding residual errors back to predictions gives new prediction vector Zr, normalize and re-scale new predictions\n",
        "      sigma = sum of absolute value of E for each training sample / num training samples\n",
        "      \"\"\"\n",
        "      sigma = float(sum(torch.norm(E[self.idx_train], p=1, dim=-1))) / len(self.idx_train)\n",
        "      Zr = Z + sigma * Ehat / sum(abs(Ehat))\n",
        "      Zr[self.idx_train] = Z[self.idx_train]\n",
        "      return Zr\n",
        "\n",
        "    def smooth(self, G, alpha2=0.8, eps=1e-5, show_progress=True): # G is current best guess matrix, alpha2 is hyperparameter\n",
        "      \"\"\"\n",
        "      Step 3: Smooth final predictions over our best guess matrix G, initialized to our scale prediction vector\n",
        "      G^(t+1) = (1 - alpha)G + alpha2 SG^(t) -> Yhat\n",
        "      \"\"\"\n",
        "      itr = 0\n",
        "      yhat = G\n",
        "      diff = eps\n",
        "      while diff >= eps and itr < 500:\n",
        "        Gt = (1 - alpha2) * G + alpha2 * (self.S @ yhat)\n",
        "        diff = float(torch.norm(yhat - Gt))\n",
        "        yhat = Gt\n",
        "        if show_progress:\n",
        "            print(f\"Iteration: [{itr}] - Diff: [{diff}]\")\n",
        "        itr += 1\n",
        "      return yhat\n",
        "\n",
        "    def correct_and_smooth(self, Z, a1=0.85, a2=0.85, eps=1e-6, show_p= False):\n",
        "      \"\"\"\n",
        "      Full pipeline for C&S\n",
        "      \"\"\"\n",
        "      #\n",
        "      Z = torch.softmax(Z, -1).to(device)\n",
        "\n",
        "      E = self.residual_error(Z)\n",
        "      Ehat = self.correct(E, alpha1=a1, eps=eps, show_progress= show_p)\n",
        "      G = self.autoscale(E, Ehat, Z)\n",
        "      G[self.idx_train] = self.Y[self.idx_train].type(torch.float32)\n",
        "      yhat = self.smooth(G, alpha2=a2, eps=eps, show_progress= show_p)\n",
        "      return yhat\n",
        "\n",
        "    def hp_sweep(self, Z, hyperparams1, hyperparams2, show_p= False):\n",
        "      val_idx = Utils.bool_to_idx(self.idx_val)\n",
        "      E = self.residual_error(Z)\n",
        "      values = []\n",
        "      for hp1 in hyperparams1:\n",
        "        for hp2 in hyperparams2:\n",
        "          yhat = self.correct_and_smooth(Z, a1=hp1, a2=hp2, show_p= show_p)\n",
        "          #yhat = torch.argmax(yhat, -1)\n",
        "          val_acc = Metrics.acc(yhat[self.idx_val], self.y[self.idx_val])\n",
        "          values.append([val_acc, (hp1, hp2)])\n",
        "      return sorted(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcUtPEMvIYEx"
      },
      "source": [
        "# GCN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: idx_val is not used?!"
      ],
      "metadata": {
        "id": "8BDLroTxoa0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKw5j3sdMi0O"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, dim_input_features, dim_output_classes, num_hidden_layers,\n",
        "        device, lr=0.01, dropout=0.5, weight_decay=5e-4, name=\"\"):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(dim_input_features, num_hidden_layers)\n",
        "        self.conv2 = GCNConv(num_hidden_layers, dim_output_classes)\n",
        "\n",
        "        self.lr = lr\n",
        "        self.dropout = dropout\n",
        "        self.weight_decay = weight_decay\n",
        "        self.name = name\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1).squeeze()\n",
        "\n",
        "    def predict(self, x, edge_index, *_):\n",
        "        if edge_index.shape[0] == graph.numNodes():\n",
        "          edge_index = to_edge_index(edge_index)\n",
        "\n",
        "        return self.forward(x, edge_index)\n",
        "\n",
        "\n",
        "    def fit(self, x, edge_index, labels, idx_train, idx_test, epochs, verbose=True):\n",
        "\n",
        "        if epochs == 0:\n",
        "            return None\n",
        "\n",
        "        self.train()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}', disable=not verbose)\n",
        "        t.set_description(f\"Training {self.name}\")\n",
        "\n",
        "        for epoch in t:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = self(x, edge_index)\n",
        "\n",
        "            loss = F.cross_entropy(predictions[idx_train], labels[idx_train])\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_postfix({\"loss\": round(loss.item(), 2)})\n",
        "        return predictions\n",
        "\n",
        "    def train1epoch(self, x, edge_index, labels, idx_train, idx_test):\n",
        "\n",
        "        self.train()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = self(x, edge_index)\n",
        "\n",
        "        loss = F.cross_entropy(predictions[idx_train], labels[idx_train])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg7l7g3TBmtc"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"gcn\"\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = gcn.predict(graph.features, graph.adj)\n",
        "Metrics.partial_acc(pred, graph.labels, g0, gX)"
      ],
      "metadata": {
        "id": "ACSzMnmcxYM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8815aefb-4859-4726-f998-e6c80397c1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G0: 7.72%\n",
            "GX: 9.66%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'g0': 0.07719298452138901, 'gX': 0.09657449275255203}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#post_pred = gcn.post(graph.features, graph.adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "#Metrics.partial_acc(post_pred, graph.labels, g0, gX)"
      ],
      "metadata": {
        "id": "pW9Bwe1MxYGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN_ADJ"
      ],
      "metadata": {
        "id": "v3GeEB_9zR0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN_ADJ(torch.nn.Module):\n",
        "    def __init__(self, dim_input_features, dim_output_classes, num_hidden_layers,\n",
        "        device, lr=0.01, dropout=0.5, weight_decay=5e-4, name=\"\"):\n",
        "\n",
        "        super(GCN_ADJ, self).__init__()\n",
        "\n",
        "        self.conv1 = DenseGCNConv(dim_input_features, num_hidden_layers)\n",
        "        self.conv2 = DenseGCNConv(num_hidden_layers, dim_output_classes)\n",
        "\n",
        "        self.lr = lr\n",
        "        self.dropout = dropout\n",
        "        self.weight_decay = weight_decay\n",
        "        self.name = name\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, adj)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.conv2(x, adj)\n",
        "\n",
        "        return F.log_softmax(x, dim=1).squeeze()\n",
        "\n",
        "    def predict(self, x, edge_index, *_):\n",
        "        return self.forward(x, edge_index)\n",
        "\n",
        "    def fit(self, graph, epochs, verbose=True):\n",
        "\n",
        "        if epochs == 0:\n",
        "            return None\n",
        "\n",
        "        g_features = graph.features\n",
        "        adj = graph.adj\n",
        "        g_labels = graph.labels\n",
        "\n",
        "        self.train()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}', disable=not verbose)\n",
        "        t.set_description(f\"Training {self.name}\")\n",
        "\n",
        "        for epoch in t:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = self(g_features, adj)\n",
        "\n",
        "            loss = F.cross_entropy(predictions[graph.idx_train], g_labels[graph.idx_train])\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_postfix({\"loss\": round(loss.item(), 2)})\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def fitManual(self, features, adj, labels, idx_train, idx_test, epochs, verbose=True):\n",
        "\n",
        "        if epochs == 0:\n",
        "            return None\n",
        "\n",
        "        self.train()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}', disable=not verbose)\n",
        "        t.set_description(f\"Training {self.name}\")\n",
        "\n",
        "        for epoch in t:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = self(features, adj)\n",
        "\n",
        "            loss = F.cross_entropy(predictions[idx_train], labels[idx_train])\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_postfix({\"loss\": round(loss.item(), 2)})\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def train1epoch(self, features, adj, labels, idx_train, idx_test):\n",
        "\n",
        "        g_features = graph.features\n",
        "        adj = adj\n",
        "        g_labels = graph.labels\n",
        "\n",
        "        self.train()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = self(g_features, adj)\n",
        "\n",
        "        loss = F.cross_entropy(predictions[idx_train], g_labels[idx_train])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return loss.item()"
      ],
      "metadata": {
        "id": "MV9bbW1_zSYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GCN_CS"
      ],
      "metadata": {
        "id": "Og-ZR9iB6jud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN_CS(torch.nn.Module):\n",
        "    def __init__(self, dim_input_features, dim_output_classes, num_hidden_layers,\n",
        "        device, lr=0.01, dropout=0.5, weight_decay=5e-4, name=\"\"):\n",
        "      super(GCN_CS, self).__init__()\n",
        "      self.gcn = GCN_ADJ(dim_input_features, dim_output_classes, num_hidden_layers, device, lr, dropout, weight_decay, name)\n",
        "\n",
        "    def post(self,x, adj, labels, idx_train, idx_val, idx_test):\n",
        "        cs = Correct_and_Smooth(adj, labels, idx_train, idx_val, idx_test)\n",
        "        pred = self.forward(x, adj)\n",
        "        pred = cs.correct_and_smooth(pred, a1=0.9, a2=0.9, eps=1e-6)\n",
        "        return pred\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        return self.gcn.forward(x, adj)\n",
        "\n",
        "    def predict(self, x, adj, labels, idx_train, idx_val, idx_test):\n",
        "        return self.post(x, adj, labels, idx_train, idx_val, idx_test)\n",
        "\n",
        "    def fit(self, x, adj, labels, idx_train, idx_test, epochs, verbose=True):\n",
        "        return self.gcn.fit(x, adj, labels, idx_train, idx_test, epochs)\n",
        "\n",
        "    def train1epoch(self, x, adj, labels, idx_train, idx_test):\n",
        "        return self.gcn.train1epoch(x, adj, labels, idx_train, idx_test)"
      ],
      "metadata": {
        "id": "Sf8NnEg_6jJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "gcn_cs = GCN_CS(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"gcn_cs\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "l8l_VilVUn-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gcn_cs.predict(graph.features, graph.adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)"
      ],
      "metadata": {
        "id": "kEnSXzxVxTlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pred = gcn_cs.predict(graph.features, graph.adj,graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "Metrics.partial_acc(pred, graph.labels, g0, gX)"
      ],
      "metadata": {
        "id": "VFo5xYI-zOO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a8cf35-92ed-4e30-df72-cd85e697c31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G0: 7.72%\n",
            "GX: 9.66%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'g0': 0.07719298452138901, 'gX': 0.09657449275255203}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57n7bfprFJeg"
      },
      "source": [
        "# Explainer Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVUkcTISFpqJ"
      },
      "outputs": [],
      "source": [
        "'''train_utils.py\n",
        "    Some training utilities.\n",
        "'''\n",
        "import torch.optim as optim\n",
        "class train_utils:\n",
        "    def build_optimizer(args, params, weight_decay=0.0):\n",
        "        filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "        if args.opt == 'adam':\n",
        "            optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "        elif args.opt == 'sgd':\n",
        "            optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "        elif args.opt == 'rmsprop':\n",
        "            optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "        elif args.opt == 'adagrad':\n",
        "            optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "        if args.opt_scheduler == 'none':\n",
        "            return None, optimizer\n",
        "        elif args.opt_scheduler == 'step':\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "        elif args.opt_scheduler == 'cos':\n",
        "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "        return scheduler, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "-56lETqNK31M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e65d99-a8b6-4c48-edd3-b13a50971ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay-oPPhSEm7V"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    io_utils.py\n",
        "    Utilities for reading and writing logs.\n",
        "\"\"\"\n",
        "import statistics\n",
        "import re\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sc\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import networkx as nx\n",
        "import tensorboardX\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "class io_utils:\n",
        "  def gen_prefix(args):\n",
        "      '''Generate label prefix for a graph model.\n",
        "      '''\n",
        "      if args.bmname is not None:\n",
        "          name = args.bmname\n",
        "      else:\n",
        "          name = args.dataset\n",
        "      name += \"_\" + args.method\n",
        "\n",
        "      name += \"_h\" + str(args.hidden_dim) + \"_o\" + str(args.output_dim)\n",
        "      if not args.bias:\n",
        "          name += \"_nobias\"\n",
        "      if len(args.name_suffix) > 0:\n",
        "          name += \"_\" + args.name_suffix\n",
        "      return name\n",
        "\n",
        "\n",
        "  def gen_explainer_prefix(args):\n",
        "      '''Generate label prefix for a graph explainer model.\n",
        "      '''\n",
        "      name = gen_prefix(args) + \"_explain\"\n",
        "      if len(args.explainer_suffix) > 0:\n",
        "          name += \"_\" + args.explainer_suffix\n",
        "      return name\n",
        "\n",
        "\n",
        "  def create_filename(save_dir, args, isbest=False, num_epochs=-1):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          args        :  the arguments parsed in the parser\n",
        "          isbest      :  whether the saved model is the best-performing one\n",
        "          num_epochs  :  epoch number of the model (when isbest=False)\n",
        "      \"\"\"\n",
        "      filename = os.path.join(save_dir, gen_prefix(args))\n",
        "      os.makedirs(filename, exist_ok=True)\n",
        "\n",
        "      if isbest:\n",
        "          filename = os.path.join(filename, \"best\")\n",
        "      elif num_epochs > 0:\n",
        "          filename = os.path.join(filename, str(num_epochs))\n",
        "\n",
        "      return filename + \".pth.tar\"\n",
        "\n",
        "\n",
        "  def save_checkpoint(model, optimizer, args, num_epochs=-1, isbest=False, cg_dict=None):\n",
        "      \"\"\"Save pytorch model checkpoint.\n",
        "      Args:\n",
        "          - model         : The PyTorch model to save.\n",
        "          - optimizer     : The optimizer used to train the model.\n",
        "          - args          : A dict of meta-data about the model.\n",
        "          - num_epochs    : Number of training epochs.\n",
        "          - isbest        : True if the model has the highest accuracy so far.\n",
        "          - cg_dict       : A dictionary of the sampled computation graphs.\n",
        "      \"\"\"\n",
        "      filename = create_filename(args.ckptdir, args, isbest, num_epochs=num_epochs)\n",
        "      torch.save(\n",
        "          {\n",
        "              \"epoch\": num_epochs,\n",
        "              \"model_type\": args.method,\n",
        "              \"optimizer\": optimizer,\n",
        "              \"model_state\": model.state_dict(),\n",
        "              \"optimizer_state\": optimizer.state_dict(),\n",
        "              \"cg\": cg_dict,\n",
        "          },\n",
        "          filename,\n",
        "      )\n",
        "\n",
        "\n",
        "  def load_ckpt(args, isbest=False):\n",
        "      '''Load a pre-trained pytorch model from checkpoint.\n",
        "      '''\n",
        "      print(\"loading model\")\n",
        "      filename = create_filename(args.ckptdir, args, isbest)\n",
        "      print(filename)\n",
        "      if os.path.isfile(filename):\n",
        "          print(\"=> loading checkpoint '{}'\".format(filename))\n",
        "          ckpt = torch.load(filename)\n",
        "      else:\n",
        "          print(\"Checkpoint does not exist!\")\n",
        "          print(\"Checked path -- {}\".format(filename))\n",
        "          print(\"Make sure you have provided the correct path!\")\n",
        "          print(\"You may have forgotten to train a model for this dataset.\")\n",
        "          print()\n",
        "          print(\"To train one of the paper's models, run the following\")\n",
        "          print(\">> python train.py --dataset=DATASET_NAME\")\n",
        "          print()\n",
        "          raise Exception(\"File not found.\")\n",
        "      return ckpt\n",
        "\n",
        "  def preprocess_cg(cg):\n",
        "      \"\"\"Pre-process computation graph.\"\"\"\n",
        "      if use_cuda:\n",
        "          preprocessed_cg_tensor = torch.from_numpy(cg).cuda()\n",
        "      else:\n",
        "          preprocessed_cg_tensor = torch.from_numpy(cg)\n",
        "\n",
        "      preprocessed_cg_tensor.unsqueeze_(0)\n",
        "      return Variable(preprocessed_cg_tensor, requires_grad=False)\n",
        "\n",
        "  def load_model(path):\n",
        "      \"\"\"Load a pytorch model.\"\"\"\n",
        "      model = torch.load(path)\n",
        "      model.eval()\n",
        "      if use_cuda:\n",
        "          model.cuda()\n",
        "\n",
        "      for p in model.features.parameters():\n",
        "          p.requires_grad = False\n",
        "      for p in model.classifier.parameters():\n",
        "          p.requires_grad = False\n",
        "\n",
        "      return model\n",
        "\n",
        "\n",
        "  def load_cg(path):\n",
        "      \"\"\"Load a computation graph.\"\"\"\n",
        "      cg = pickle.load(open(path))\n",
        "      return cg\n",
        "\n",
        "\n",
        "  def save(mask_cg):\n",
        "      \"\"\"Save a rendering of the computation graph mask.\"\"\"\n",
        "      mask = mask_cg.cpu().data.numpy()[0]\n",
        "      mask = np.transpose(mask, (1, 2, 0))\n",
        "\n",
        "      mask = (mask - np.min(mask)) / np.max(mask)\n",
        "      mask = 1 - mask\n",
        "\n",
        "      cv2.imwrite(\"mask.png\", np.uint8(255 * mask))\n",
        "\n",
        "  def log_matrix(writer, mat, name, epoch, fig_size=(8, 6), dpi=200):\n",
        "      \"\"\"Save an image of a matrix to disk.\n",
        "      Args:\n",
        "          - writer    :  A file writer.\n",
        "          - mat       :  The matrix to write.\n",
        "          - name      :  Name of the file to save.\n",
        "          - epoch     :  Epoch number.\n",
        "          - fig_size  :  Size to of the figure to save.\n",
        "          - dpi       :  Resolution.\n",
        "      \"\"\"\n",
        "      plt.switch_backend(\"agg\")\n",
        "      fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
        "      mat = mat.cpu().detach().numpy()\n",
        "      if mat.ndim == 1:\n",
        "          mat = mat[:, np.newaxis]\n",
        "      plt.imshow(mat, cmap=plt.get_cmap(\"BuPu\"))\n",
        "      cbar = plt.colorbar()\n",
        "      cbar.solids.set_edgecolor(\"face\")\n",
        "\n",
        "      plt.tight_layout()\n",
        "      fig.canvas.draw()\n",
        "      writer.add_image(name, tensorboardX.utils.figure_to_image(fig), epoch)\n",
        "\n",
        "\n",
        "  def denoise_graph(adj, node_idx, feat=None, label=None, threshold=None, threshold_num=None, max_component=True):\n",
        "      \"\"\"Cleaning a graph by thresholding its node values.\n",
        "      Args:\n",
        "          - adj               :  Adjacency matrix.\n",
        "          - node_idx          :  Index of node to highlight (TODO ?)\n",
        "          - feat              :  An array of node features.\n",
        "          - label             :  A list of node labels.\n",
        "          - threshold         :  The weight threshold.\n",
        "          - theshold_num      :  The maximum number of nodes to threshold.\n",
        "          - max_component     :  TODO\n",
        "      \"\"\"\n",
        "      num_nodes = adj.shape[-1]\n",
        "      G = nx.Graph()\n",
        "      G.add_nodes_from(range(num_nodes))\n",
        "      G.nodes[node_idx][\"self\"] = 1\n",
        "      if feat is not None:\n",
        "          for node in G.nodes():\n",
        "              G.nodes[node][\"feat\"] = feat[node]\n",
        "      if label is not None:\n",
        "          for node in G.nodes():\n",
        "              G.nodes[node][\"label\"] = label[node]\n",
        "\n",
        "      if threshold_num is not None:\n",
        "          # this is for symmetric graphs: edges are repeated twice in adj\n",
        "          adj_threshold_num = threshold_num * 2\n",
        "          #adj += np.random.rand(adj.shape[0], adj.shape[1]) * 1e-4\n",
        "          neigh_size = len(adj[adj > 0])\n",
        "          threshold_num = min(neigh_size, adj_threshold_num)\n",
        "          threshold = np.sort(adj[adj > 0])[-threshold_num]\n",
        "\n",
        "      if threshold is not None:\n",
        "          weighted_edge_list = [\n",
        "              (i, j, adj[i, j])\n",
        "              for i in range(num_nodes)\n",
        "              for j in range(num_nodes)\n",
        "              if adj[i, j] >= threshold\n",
        "          ]\n",
        "      else:\n",
        "          weighted_edge_list = [\n",
        "              (i, j, adj[i, j])\n",
        "              for i in range(num_nodes)\n",
        "              for j in range(num_nodes)\n",
        "              if adj[i, j] > 1e-6\n",
        "          ]\n",
        "      G.add_weighted_edges_from(weighted_edge_list)\n",
        "      if max_component:\n",
        "          largest_cc = max(nx.connected_components(G), key=len)\n",
        "          G = G.subgraph(largest_cc).copy()\n",
        "      else:\n",
        "          # remove zero degree nodes\n",
        "          G.remove_nodes_from(list(nx.isolates(G)))\n",
        "      return G\n",
        "\n",
        "  # TODO: unify log_graph and log_graph2\n",
        "  def log_graph(\n",
        "      writer,\n",
        "      Gc,\n",
        "      name,\n",
        "      identify_self=True,\n",
        "      nodecolor=\"label\",\n",
        "      epoch=0,\n",
        "      fig_size=(4, 3),\n",
        "      dpi=300,\n",
        "      label_node_feat=False,\n",
        "      edge_vmax=None,\n",
        "      args=None,\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          nodecolor: the color of node, can be determined by 'label', or 'feat'. For feat, it needs to\n",
        "              be one-hot'\n",
        "      \"\"\"\n",
        "      cmap = plt.get_cmap(\"Set1\")\n",
        "      plt.switch_backend(\"agg\")\n",
        "      fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
        "\n",
        "      node_colors = []\n",
        "      # edge_colors = [min(max(w, 0.0), 1.0) for (u,v,w) in Gc.edges.data('weight', default=1)]\n",
        "      edge_colors = [w for (u, v, w) in Gc.edges.data(\"weight\", default=1)]\n",
        "\n",
        "      # maximum value for node color\n",
        "      vmax = 8\n",
        "      for i in Gc.nodes():\n",
        "          if nodecolor == \"feat\" and \"feat\" in Gc.nodes[i]:\n",
        "              num_classes = Gc.nodes[i][\"feat\"].size()[0]\n",
        "              if num_classes >= 10:\n",
        "                  cmap = plt.get_cmap(\"tab20\")\n",
        "                  vmax = 19\n",
        "              elif num_classes >= 8:\n",
        "                  cmap = plt.get_cmap(\"tab10\")\n",
        "                  vmax = 9\n",
        "              break\n",
        "\n",
        "      feat_labels = {}\n",
        "      for i in Gc.nodes():\n",
        "          if identify_self and \"self\" in Gc.nodes[i]:\n",
        "              node_colors.append(0)\n",
        "          elif nodecolor == \"label\" and \"label\" in Gc.nodes[i]:\n",
        "              node_colors.append(Gc.nodes[i][\"label\"] + 1)\n",
        "          elif nodecolor == \"feat\" and \"feat\" in Gc.nodes[i]:\n",
        "              # print(Gc.nodes[i]['feat'])\n",
        "              feat = Gc.nodes[i][\"feat\"].detach().numpy()\n",
        "              # idx with pos val in 1D array\n",
        "              feat_class = 0\n",
        "              for j in range(len(feat)):\n",
        "                  if feat[j] == 1:\n",
        "                      feat_class = j\n",
        "                      break\n",
        "              node_colors.append(feat_class)\n",
        "              feat_labels[i] = feat_class\n",
        "          else:\n",
        "              node_colors.append(1)\n",
        "      if not label_node_feat:\n",
        "          feat_labels = None\n",
        "\n",
        "      plt.switch_backend(\"agg\")\n",
        "      fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
        "\n",
        "      if Gc.number_of_nodes() == 0:\n",
        "          raise Exception(\"empty graph\")\n",
        "      if Gc.number_of_edges() == 0:\n",
        "          raise Exception(\"empty edge\")\n",
        "      # remove_nodes = []\n",
        "      # for u in Gc.nodes():\n",
        "      #    if Gc\n",
        "      pos_layout = nx.kamada_kawai_layout(Gc, weight=None)\n",
        "      # pos_layout = nx.spring_layout(Gc, weight=None)\n",
        "\n",
        "      weights = [d for (u, v, d) in Gc.edges(data=\"weight\", default=1)]\n",
        "      if edge_vmax is None:\n",
        "          edge_vmax = statistics.median_high(\n",
        "              [d for (u, v, d) in Gc.edges(data=\"weight\", default=1)]\n",
        "          )\n",
        "      min_color = min([d for (u, v, d) in Gc.edges(data=\"weight\", default=1)])\n",
        "      # color range: gray to black\n",
        "      edge_vmin = 2 * min_color - edge_vmax\n",
        "      nx.draw(\n",
        "          Gc,\n",
        "          pos=pos_layout,\n",
        "          with_labels=False,\n",
        "          font_size=4,\n",
        "          labels=feat_labels,\n",
        "          node_color=node_colors,\n",
        "          vmin=0,\n",
        "          vmax=vmax,\n",
        "          cmap=cmap,\n",
        "          edge_color=edge_colors,\n",
        "          edge_cmap=plt.get_cmap(\"Greys\"),\n",
        "          edge_vmin=edge_vmin,\n",
        "          edge_vmax=edge_vmax,\n",
        "          width=1.0,\n",
        "          node_size=50,\n",
        "          alpha=0.8,\n",
        "      )\n",
        "      fig.axes[0].xaxis.set_visible(False)\n",
        "      fig.canvas.draw()\n",
        "\n",
        "      logdir = \"log\" if not hasattr(args, \"logdir\") or not args.logdir else str(args.logdir)\n",
        "      if nodecolor != \"feat\":\n",
        "          name += gen_explainer_prefix(args)\n",
        "      save_path = os.path.join(logdir, name  + \"_\" + str(epoch) + \".pdf\")\n",
        "      print(logdir + \"/\" + name + gen_explainer_prefix(args) + \"_\" + str(epoch) + \".pdf\")\n",
        "      os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "      plt.savefig(save_path, format=\"pdf\")\n",
        "\n",
        "      img = tensorboardX.utils.figure_to_image(fig)\n",
        "      writer.add_image(name, img, epoch)\n",
        "\n",
        "\n",
        "  def plot_cmap(cmap, ncolor):\n",
        "      \"\"\"\n",
        "      A convenient function to plot colors of a matplotlib cmap\n",
        "      Credit goes to http://gvallver.perso.univ-pau.fr/?p=712\n",
        "      Args:\n",
        "          ncolor (int): number of color to show\n",
        "          cmap: a cmap object or a matplotlib color name\n",
        "      \"\"\"\n",
        "\n",
        "      if isinstance(cmap, str):\n",
        "          name = cmap\n",
        "          try:\n",
        "              cm = plt.get_cmap(cmap)\n",
        "          except ValueError:\n",
        "              print(\"WARNINGS :\", cmap, \" is not a known colormap\")\n",
        "              cm = plt.cm.gray\n",
        "      else:\n",
        "          cm = cmap\n",
        "          name = cm.name\n",
        "\n",
        "      with matplotlib.rc_context(matplotlib.rcParamsDefault):\n",
        "          fig = plt.figure(figsize=(12, 1), frameon=False)\n",
        "          ax = fig.add_subplot(111)\n",
        "          ax.pcolor(np.linspace(1, ncolor, ncolor).reshape(1, ncolor), cmap=cm)\n",
        "          ax.set_title(name)\n",
        "          xt = ax.set_xticks([])\n",
        "          yt = ax.set_yticks([])\n",
        "      return fig\n",
        "\n",
        "\n",
        "  def plot_cmap_tb(writer, cmap, ncolor, name):\n",
        "      \"\"\"Plot the color map used for plot.\"\"\"\n",
        "      fig = plot_cmap(cmap, ncolor)\n",
        "      img = tensorboardX.utils.figure_to_image(fig)\n",
        "      writer.add_image(name, img, 0)\n",
        "\n",
        "\n",
        "  def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "      \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "      sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "      indices = torch.from_numpy(\n",
        "          np.vstack((sparse_mx.row, sparse_mx.col)).astype(int64)\n",
        "      )\n",
        "      values = torch.from_numpy(sparse_mx.data)\n",
        "      shape = torch.Size(sparse_mx.shape)\n",
        "      return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "  def numpy_to_torch(img, requires_grad=True):\n",
        "      if len(img.shape) < 3:\n",
        "          output = np.float32([img])\n",
        "      else:\n",
        "          output = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "      output = torch.from_numpy(output)\n",
        "      if use_cuda:\n",
        "          output = output.cuda()\n",
        "\n",
        "      output.unsqueeze_(0)\n",
        "      v = Variable(output, requires_grad=requires_grad)\n",
        "      return v\n",
        "\n",
        "\n",
        "  def read_graphfile(datadir, dataname, max_nodes=None, edge_labels=False):\n",
        "      \"\"\" Read data from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n",
        "          graph index starts with 1 in file\n",
        "      Returns:\n",
        "          List of networkx objects with graph and node labels\n",
        "      \"\"\"\n",
        "      prefix = os.path.join(datadir, dataname, dataname)\n",
        "      filename_graph_indic = prefix + \"_graph_indicator.txt\"\n",
        "      # index of graphs that a given node belongs to\n",
        "      graph_indic = {}\n",
        "      with open(filename_graph_indic) as f:\n",
        "          i = 1\n",
        "          for line in f:\n",
        "              line = line.strip(\"\\n\")\n",
        "              graph_indic[i] = int(line)\n",
        "              i += 1\n",
        "\n",
        "      filename_nodes = prefix + \"_node_labels.txt\"\n",
        "      node_labels = []\n",
        "      min_label_val = None\n",
        "      try:\n",
        "          with open(filename_nodes) as f:\n",
        "              for line in f:\n",
        "                  line = line.strip(\"\\n\")\n",
        "                  l = int(line)\n",
        "                  node_labels += [l]\n",
        "                  if min_label_val is None or min_label_val > l:\n",
        "                      min_label_val = l\n",
        "          # assume that node labels are consecutive\n",
        "          num_unique_node_labels = max(node_labels) - min_label_val + 1\n",
        "          node_labels = [l - min_label_val for l in node_labels]\n",
        "      except IOError:\n",
        "          print(\"No node labels\")\n",
        "\n",
        "      filename_node_attrs = prefix + \"_node_attributes.txt\"\n",
        "      node_attrs = []\n",
        "      try:\n",
        "          with open(filename_node_attrs) as f:\n",
        "              for line in f:\n",
        "                  line = line.strip(\"\\s\\n\")\n",
        "                  attrs = [\n",
        "                      float(attr) for attr in re.split(\"[,\\s]+\", line) if not attr == \"\"\n",
        "                  ]\n",
        "                  node_attrs.append(np.array(attrs))\n",
        "      except IOError:\n",
        "          print(\"No node attributes\")\n",
        "\n",
        "      label_has_zero = False\n",
        "      filename_graphs = prefix + \"_graph_labels.txt\"\n",
        "      graph_labels = []\n",
        "\n",
        "      label_vals = []\n",
        "      with open(filename_graphs) as f:\n",
        "          for line in f:\n",
        "              line = line.strip(\"\\n\")\n",
        "              val = int(line)\n",
        "              if val not in label_vals:\n",
        "                  label_vals.append(val)\n",
        "              graph_labels.append(val)\n",
        "\n",
        "      label_map_to_int = {val: i for i, val in enumerate(label_vals)}\n",
        "      graph_labels = np.array([label_map_to_int[l] for l in graph_labels])\n",
        "\n",
        "      if edge_labels:\n",
        "          # For Tox21_AHR we want to know edge labels\n",
        "          filename_edges = prefix + \"_edge_labels.txt\"\n",
        "          edge_labels = []\n",
        "\n",
        "          edge_label_vals = []\n",
        "          with open(filename_edges) as f:\n",
        "              for line in f:\n",
        "                  line = line.strip(\"\\n\")\n",
        "                  val = int(line)\n",
        "                  if val not in edge_label_vals:\n",
        "                      edge_label_vals.append(val)\n",
        "                  edge_labels.append(val)\n",
        "\n",
        "          edge_label_map_to_int = {val: i for i, val in enumerate(edge_label_vals)}\n",
        "\n",
        "      filename_adj = prefix + \"_A.txt\"\n",
        "      adj_list = {i: [] for i in range(1, len(graph_labels) + 1)}\n",
        "      # edge_label_list={i:[] for i in range(1,len(graph_labels)+1)}\n",
        "      index_graph = {i: [] for i in range(1, len(graph_labels) + 1)}\n",
        "      num_edges = 0\n",
        "      with open(filename_adj) as f:\n",
        "          for line in f:\n",
        "              line = line.strip(\"\\n\").split(\",\")\n",
        "              e0, e1 = (int(line[0].strip(\" \")), int(line[1].strip(\" \")))\n",
        "              adj_list[graph_indic[e0]].append((e0, e1))\n",
        "              index_graph[graph_indic[e0]] += [e0, e1]\n",
        "              # edge_label_list[graph_indic[e0]].append(edge_labels[num_edges])\n",
        "              num_edges += 1\n",
        "      for k in index_graph.keys():\n",
        "          index_graph[k] = [u - 1 for u in set(index_graph[k])]\n",
        "\n",
        "      graphs = []\n",
        "      for i in range(1, 1 + len(adj_list)):\n",
        "          # indexed from 1 here\n",
        "          G = nx.from_edgelist(adj_list[i])\n",
        "\n",
        "          if max_nodes is not None and G.number_of_nodes() > max_nodes:\n",
        "              continue\n",
        "\n",
        "          # add features and labels\n",
        "          G.graph[\"label\"] = graph_labels[i - 1]\n",
        "\n",
        "          # Special label for aromaticity experiment\n",
        "          # aromatic_edge = 2\n",
        "          # G.graph['aromatic'] = aromatic_edge in edge_label_list[i]\n",
        "\n",
        "          for u in G.nodes():\n",
        "              if len(node_labels) > 0:\n",
        "                  node_label_one_hot = [0] * num_unique_node_labels\n",
        "                  node_label = node_labels[u - 1]\n",
        "                  node_label_one_hot[node_label] = 1\n",
        "                  G.nodes[u][\"label\"] = node_label_one_hot\n",
        "              if len(node_attrs) > 0:\n",
        "                  G.nodes[u][\"feat\"] = node_attrs[u - 1]\n",
        "          if len(node_attrs) > 0:\n",
        "              G.graph[\"feat_dim\"] = node_attrs[0].shape[0]\n",
        "\n",
        "          # relabeling\n",
        "          mapping = {}\n",
        "          it = 0\n",
        "          if float(nx.__version__) < 2.0:\n",
        "              for n in G.nodes():\n",
        "                  mapping[n] = it\n",
        "                  it += 1\n",
        "          else:\n",
        "              for n in G.nodes:\n",
        "                  mapping[n] = it\n",
        "                  it += 1\n",
        "\n",
        "          # indexed from 0\n",
        "          graphs.append(nx.relabel_nodes(G, mapping))\n",
        "      return graphs\n",
        "\n",
        "\n",
        "  def read_biosnap(datadir, edgelist_file, label_file, feat_file=None, concat=True):\n",
        "      \"\"\" Read data from BioSnap\n",
        "      Returns:\n",
        "          List of networkx objects with graph and node labels\n",
        "      \"\"\"\n",
        "      G = nx.Graph()\n",
        "      delimiter = \"\\t\" if \"tsv\" in edgelist_file else \",\"\n",
        "      print(delimiter)\n",
        "      df = pd.read_csv(\n",
        "          os.path.join(datadir, edgelist_file), delimiter=delimiter, header=None\n",
        "      )\n",
        "      data = list(map(tuple, df.values.tolist()))\n",
        "      G.add_edges_from(data)\n",
        "      print(\"Total nodes: \", G.number_of_nodes())\n",
        "\n",
        "      G = max(nx.connected_component_subgraphs(G), key=len)\n",
        "      print(\"Total nodes in largest connected component: \", G.number_of_nodes())\n",
        "\n",
        "      df = pd.read_csv(os.path.join(datadir, label_file), delimiter=\"\\t\", usecols=[0, 1])\n",
        "      data = list(map(tuple, df.values.tolist()))\n",
        "\n",
        "      missing_node = 0\n",
        "      for line in data:\n",
        "          if int(line[0]) not in G:\n",
        "              missing_node += 1\n",
        "          else:\n",
        "              G.nodes[int(line[0])][\"label\"] = int(line[1] == \"Essential\")\n",
        "\n",
        "      print(\"missing node: \", missing_node)\n",
        "\n",
        "      missing_label = 0\n",
        "      remove_nodes = []\n",
        "      for u in G.nodes():\n",
        "          if \"label\" not in G.nodes[u]:\n",
        "              missing_label += 1\n",
        "              remove_nodes.append(u)\n",
        "      G.remove_nodes_from(remove_nodes)\n",
        "      print(\"missing_label: \", missing_label)\n",
        "\n",
        "      if feat_file is None:\n",
        "          feature_generator = featgen.ConstFeatureGen(np.ones(10, dtype=float))\n",
        "          feature_generator.gen_node_features(G)\n",
        "      else:\n",
        "          df = pd.read_csv(os.path.join(datadir, feat_file), delimiter=\",\")\n",
        "          data = np.array(df.values)\n",
        "          print(\"Feat shape: \", data.shape)\n",
        "\n",
        "          for row in data:\n",
        "              if int(row[0]) in G:\n",
        "                  if concat:\n",
        "                      node = int(row[0])\n",
        "                      onehot = np.zeros(10)\n",
        "                      onehot[min(G.degree[node], 10) - 1] = 1.0\n",
        "                      G.nodes[node][\"feat\"] = np.hstack(\n",
        "                          (np.log(row[1:] + 0.1), [1.0], onehot)\n",
        "                      )\n",
        "                  else:\n",
        "                      G.nodes[int(row[0])][\"feat\"] = np.log(row[1:] + 0.1)\n",
        "\n",
        "          missing_feat = 0\n",
        "          remove_nodes = []\n",
        "          for u in G.nodes():\n",
        "              if \"feat\" not in G.nodes[u]:\n",
        "                  missing_feat += 1\n",
        "                  remove_nodes.append(u)\n",
        "          G.remove_nodes_from(remove_nodes)\n",
        "          print(\"missing feat: \", missing_feat)\n",
        "\n",
        "      return G\n",
        "\n",
        "\n",
        "  def build_aromaticity_dataset():\n",
        "      filename = \"data/tox21_10k_data_all.sdf\"\n",
        "      basename = filename.split(\".\")[0]\n",
        "      collector = []\n",
        "      sdprovider = Chem.SDMolSupplier(filename)\n",
        "      for i,mol in enumerate(sdprovider):\n",
        "          try:\n",
        "              moldict = {}\n",
        "              moldict['smiles'] = Chem.MolToSmiles(mol)\n",
        "              #Parse Data\n",
        "              for propname in mol.GetPropNames():\n",
        "                  moldict[propname] = mol.GetProp(propname)\n",
        "              nb_bonds = len(mol.GetBonds())\n",
        "              is_aromatic = False; aromatic_bonds = []\n",
        "              for j in range(nb_bonds):\n",
        "                  if mol.GetBondWithIdx(j).GetIsAromatic():\n",
        "                      aromatic_bonds.append(j)\n",
        "                      is_aromatic = True\n",
        "              moldict['aromaticity'] = is_aromatic\n",
        "              moldict['aromatic_bonds'] = aromatic_bonds\n",
        "              collector.append(moldict)\n",
        "          except:\n",
        "              print(\"Molecule %s failed\"%i)\n",
        "      data = pd.DataFrame(collector)\n",
        "      data.to_csv(basename + '_pandas.csv')\n",
        "\n",
        "\n",
        "  def gen_train_plt_name(args):\n",
        "      return \"results/\" + gen_prefix(args) + \".png\"\n",
        "\n",
        "\n",
        "  def log_assignment(assign_tensor, writer, epoch, batch_idx):\n",
        "      plt.switch_backend(\"agg\")\n",
        "      fig = plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "      # has to be smaller than args.batch_size\n",
        "      for i in range(len(batch_idx)):\n",
        "          plt.subplot(2, 2, i + 1)\n",
        "          plt.imshow(\n",
        "              assign_tensor.cpu().data.numpy()[batch_idx[i]], cmap=plt.get_cmap(\"BuPu\")\n",
        "          )\n",
        "          cbar = plt.colorbar()\n",
        "          cbar.solids.set_edgecolor(\"face\")\n",
        "      plt.tight_layout()\n",
        "      fig.canvas.draw()\n",
        "\n",
        "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n",
        "      data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "      writer.add_image(\"assignment\", data, epoch)\n",
        "\n",
        "  # TODO: unify log_graph and log_graph2\n",
        "  def log_graph2(adj, batch_num_nodes, writer, epoch, batch_idx, assign_tensor=None):\n",
        "      plt.switch_backend(\"agg\")\n",
        "      fig = plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "      for i in range(len(batch_idx)):\n",
        "          ax = plt.subplot(2, 2, i + 1)\n",
        "          num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "          adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "          G = nx.from_numpy_matrix(adj_matrix)\n",
        "          nx.draw(\n",
        "              G,\n",
        "              pos=nx.spring_layout(G),\n",
        "              with_labels=True,\n",
        "              node_color=\"#336699\",\n",
        "              edge_color=\"grey\",\n",
        "              width=0.5,\n",
        "              node_size=300,\n",
        "              alpha=0.7,\n",
        "          )\n",
        "          ax.xaxis.set_visible(False)\n",
        "\n",
        "      plt.tight_layout()\n",
        "      fig.canvas.draw()\n",
        "\n",
        "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n",
        "      data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "      writer.add_image(\"graphs\", data, epoch)\n",
        "\n",
        "      # log a label-less version\n",
        "      # fig = plt.figure(figsize=(8,6), dpi=300)\n",
        "      # for i in range(len(batch_idx)):\n",
        "      #    ax = plt.subplot(2, 2, i+1)\n",
        "      #    num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "      #    adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "      #    G = nx.from_numpy_matrix(adj_matrix)\n",
        "      #    nx.draw(G, pos=nx.spring_layout(G), with_labels=False, node_color='#336699',\n",
        "      #            edge_color='grey', width=0.5, node_size=25,\n",
        "      #            alpha=0.8)\n",
        "\n",
        "      # plt.tight_layout()\n",
        "      # fig.canvas.draw()\n",
        "\n",
        "      # data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "      # data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "      # writer.add_image('graphs_no_label', data, epoch)\n",
        "\n",
        "      # colored according to assignment\n",
        "      assignment = assign_tensor.cpu().data.numpy()\n",
        "      fig = plt.figure(figsize=(8, 6), dpi=300)\n",
        "\n",
        "      num_clusters = assignment.shape[2]\n",
        "      all_colors = np.array(range(num_clusters))\n",
        "\n",
        "      for i in range(len(batch_idx)):\n",
        "          ax = plt.subplot(2, 2, i + 1)\n",
        "          num_nodes = batch_num_nodes[batch_idx[i]]\n",
        "          adj_matrix = adj[batch_idx[i], :num_nodes, :num_nodes].cpu().data.numpy()\n",
        "\n",
        "          label = np.argmax(assignment[batch_idx[i]], axis=1).astype(int)\n",
        "          label = label[: batch_num_nodes[batch_idx[i]]]\n",
        "          node_colors = all_colors[label]\n",
        "\n",
        "          G = nx.from_numpy_matrix(adj_matrix)\n",
        "          nx.draw(\n",
        "              G,\n",
        "              pos=nx.spring_layout(G),\n",
        "              with_labels=False,\n",
        "              node_color=node_colors,\n",
        "              edge_color=\"grey\",\n",
        "              width=0.4,\n",
        "              node_size=50,\n",
        "              cmap=plt.get_cmap(\"Set1\"),\n",
        "              vmin=0,\n",
        "              vmax=num_clusters - 1,\n",
        "              alpha=0.8,\n",
        "          )\n",
        "\n",
        "      plt.tight_layout()\n",
        "      fig.canvas.draw()\n",
        "\n",
        "      data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n",
        "      data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "      writer.add_image(\"graphs_colored\", data, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvg7nf9LELmo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  graph_utils.py\n",
        "  Utility for sampling graphs from a dataset.\n",
        "\"\"\"\n",
        "import torch.utils.data\n",
        "\n",
        "class graph_utils:\n",
        "    class GraphSampler(torch.utils.data.Dataset):\n",
        "        \"\"\" Sample graphs and nodes in graph\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(\n",
        "            self,\n",
        "            G_list,\n",
        "            features=\"default\",\n",
        "            normalize=True,\n",
        "            assign_feat=\"default\",\n",
        "            max_num_nodes=0,\n",
        "        ):\n",
        "            self.adj_all = []\n",
        "            self.len_all = []\n",
        "            self.feature_all = []\n",
        "            self.label_all = []\n",
        "\n",
        "            self.assign_feat_all = []\n",
        "\n",
        "            if max_num_nodes == 0:\n",
        "                self.max_num_nodes = max([G.number_of_nodes() for G in G_list])\n",
        "            else:\n",
        "                self.max_num_nodes = max_num_nodes\n",
        "\n",
        "            existing_node = list(G_list[0].nodes())[-1]\n",
        "            self.feat_dim = G_list[0].nodes[existing_node][\"feat\"].shape[0]\n",
        "\n",
        "            for G in G_list:\n",
        "                adj = np.array(nx.to_numpy_matrix(G))\n",
        "                if normalize:\n",
        "                    sqrt_deg = np.diag(\n",
        "                        1.0 / np.sqrt(np.sum(adj, axis=0, dtype=float).squeeze())\n",
        "                    )\n",
        "                    adj = np.matmul(np.matmul(sqrt_deg, adj), sqrt_deg)\n",
        "                self.adj_all.append(adj)\n",
        "                self.len_all.append(G.number_of_nodes())\n",
        "                self.label_all.append(G.graph[\"label\"])\n",
        "                # feat matrix: max_num_nodes x feat_dim\n",
        "                if features == \"default\":\n",
        "                    f = np.zeros((self.max_num_nodes, self.feat_dim), dtype=float)\n",
        "                    for i, u in enumerate(G.nodes()):\n",
        "                        f[i, :] = G.nodes[u][\"feat\"]\n",
        "                    self.feature_all.append(f)\n",
        "                elif features == \"id\":\n",
        "                    self.feature_all.append(np.identity(self.max_num_nodes))\n",
        "                elif features == \"deg-num\":\n",
        "                    degs = np.sum(np.array(adj), 1)\n",
        "                    degs = np.expand_dims(\n",
        "                        np.pad(degs, [0, self.max_num_nodes - G.number_of_nodes()], 0),\n",
        "                        axis=1,\n",
        "                    )\n",
        "                    self.feature_all.append(degs)\n",
        "                elif features == \"deg\":\n",
        "                    self.max_deg = 10\n",
        "                    degs = np.sum(np.array(adj), 1).astype(int)\n",
        "                    degs[degs > self.max_deg] = self.max_deg\n",
        "                    feat = np.zeros((len(degs), self.max_deg + 1))\n",
        "                    feat[np.arange(len(degs)), degs] = 1\n",
        "                    feat = np.pad(\n",
        "                        feat,\n",
        "                        ((0, self.max_num_nodes - G.number_of_nodes()), (0, 0)),\n",
        "                        \"constant\",\n",
        "                        constant_values=0,\n",
        "                    )\n",
        "\n",
        "                    f = np.zeros((self.max_num_nodes, self.feat_dim), dtype=float)\n",
        "                    for i, u in enumerate(G.nodes()):\n",
        "                        f[i, :] = G.nodes[u][\"feat\"]\n",
        "\n",
        "                    feat = np.concatenate((feat, f), axis=1)\n",
        "\n",
        "                    self.feature_all.append(feat)\n",
        "                elif features == \"struct\":\n",
        "                    self.max_deg = 10\n",
        "                    degs = np.sum(np.array(adj), 1).astype(int)\n",
        "                    degs[degs > 10] = 10\n",
        "                    feat = np.zeros((len(degs), self.max_deg + 1))\n",
        "                    feat[np.arange(len(degs)), degs] = 1\n",
        "                    degs = np.pad(\n",
        "                        feat,\n",
        "                        ((0, self.max_num_nodes - G.number_of_nodes()), (0, 0)),\n",
        "                        \"constant\",\n",
        "                        constant_values=0,\n",
        "                    )\n",
        "\n",
        "                    clusterings = np.array(list(nx.clustering(G).values()))\n",
        "                    clusterings = np.expand_dims(\n",
        "                        np.pad(\n",
        "                            clusterings,\n",
        "                            [0, self.max_num_nodes - G.number_of_nodes()],\n",
        "                            \"constant\",\n",
        "                        ),\n",
        "                        axis=1,\n",
        "                    )\n",
        "                    g_feat = np.hstack([degs, clusterings])\n",
        "                    if \"feat\" in G.nodes[0]:\n",
        "                        node_feats = np.array(\n",
        "                            [G.nodes[i][\"feat\"] for i in range(G.number_of_nodes())]\n",
        "                        )\n",
        "                        node_feats = np.pad(\n",
        "                            node_feats,\n",
        "                            ((0, self.max_num_nodes - G.number_of_nodes()), (0, 0)),\n",
        "                            \"constant\",\n",
        "                        )\n",
        "                        g_feat = np.hstack([g_feat, node_feats])\n",
        "\n",
        "                    self.feature_all.append(g_feat)\n",
        "\n",
        "                if assign_feat == \"id\":\n",
        "                    self.assign_feat_all.append(\n",
        "                        np.hstack((np.identity(self.max_num_nodes), self.feature_all[-1]))\n",
        "                    )\n",
        "                else:\n",
        "                    self.assign_feat_all.append(self.feature_all[-1])\n",
        "\n",
        "            self.feat_dim = self.feature_all[0].shape[1]\n",
        "            self.assign_feat_dim = self.assign_feat_all[0].shape[1]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.adj_all)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            adj = self.adj_all[idx]\n",
        "            num_nodes = adj.shape[0]\n",
        "            adj_padded = np.zeros((self.max_num_nodes, self.max_num_nodes))\n",
        "            adj_padded[:num_nodes, :num_nodes] = adj\n",
        "\n",
        "            # use all nodes for aggregation (baseline)\n",
        "            return {\n",
        "                \"adj\": adj_padded,\n",
        "                \"feats\": self.feature_all[idx].copy(),\n",
        "                \"label\": self.label_all[idx],\n",
        "                \"num_nodes\": num_nodes,\n",
        "                \"assign_feats\": self.assign_feat_all[idx].copy(),\n",
        "            }\n",
        "\n",
        "    def neighborhoods(adj, n_hops, use_cuda):\n",
        "        \"\"\"Returns the n_hops degree adjacency matrix adj.\"\"\"\n",
        "        adj = torch.tensor(adj, dtype=torch.float)\n",
        "        if use_cuda:\n",
        "            adj = adj.cuda()\n",
        "        hop_adj = power_adj = adj\n",
        "        for i in range(n_hops - 1):\n",
        "            power_adj = power_adj @ adj\n",
        "            prev_hop_adj = hop_adj\n",
        "            hop_adj = hop_adj + power_adj\n",
        "            hop_adj = (hop_adj > 0).float()\n",
        "        return hop_adj.cpu().numpy().astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkKV4kAY3gSZ"
      },
      "source": [
        "# GNN Explainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.explain import Explainer, GNNExplainer"
      ],
      "metadata": {
        "id": "zdHdZzxdQ9ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5iqwJ5N3hTi"
      },
      "outputs": [],
      "source": [
        "class Explainer1:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        adj,\n",
        "        feat,\n",
        "        label,\n",
        "        pred,\n",
        "        train_idx,\n",
        "        args,\n",
        "        writer=None,\n",
        "        print_training=True,\n",
        "        graph_mode=False,\n",
        "        graph_idx=False,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.adj = adj.detach().cpu().numpy()\n",
        "        self.feat = feat.detach().cpu().numpy()\n",
        "        self.label = label.detach().cpu().numpy()\n",
        "        self.pred = pred.detach().cpu().numpy()\n",
        "        self.train_idx = train_idx.detach().cpu().numpy()\n",
        "        self.n_hops = args.num_gc_layers\n",
        "        self.graph_mode = graph_mode\n",
        "        self.graph_idx = graph_idx\n",
        "        self.neighborhoods = None if self.graph_mode else graph_utils.neighborhoods(adj=self.adj, n_hops=self.n_hops, use_cuda=use_cuda)\n",
        "        self.args = args\n",
        "        self.writer = writer\n",
        "        self.print_training = print_training\n",
        "\n",
        "\n",
        "    # Main method\n",
        "    def explain(\n",
        "        self, node_idx, graph_idx=0, graph_mode=False, unconstrained=False, model=\"exp\"\n",
        "    ):\n",
        "        \"\"\"Explain a single node prediction\n",
        "        \"\"\"\n",
        "        # index of the query node in the new adj\n",
        "        if graph_mode:\n",
        "            node_idx_new = node_idx\n",
        "            sub_adj = self.adj[graph_idx]\n",
        "            sub_feat = self.feat[graph_idx, :]\n",
        "            sub_label = self.label[graph_idx].detach().cpu().numpy()\n",
        "            neighbors = np.asarray(range(self.adj.shape[0]))\n",
        "        else:\n",
        "            #print(\"node label: \", self.label[graph_idx][node_idx])\n",
        "            print(\"node label: \", self.label[node_idx])\n",
        "\n",
        "            node_idx_new, sub_adj, sub_feat, sub_label, neighbors = self.extract_neighborhood(\n",
        "                node_idx, graph_idx\n",
        "            )\n",
        "            print(\"neigh graph idx: \", node_idx, node_idx_new)\n",
        "            sub_label = np.expand_dims(sub_label, axis=0)\n",
        "\n",
        "        sub_adj = np.expand_dims(sub_adj, axis=0)\n",
        "        sub_feat = np.expand_dims(sub_feat, axis=0)\n",
        "\n",
        "        adj   = torch.tensor(sub_adj, dtype=torch.float)\n",
        "        x     = torch.tensor(sub_feat, requires_grad=True, dtype=torch.float)\n",
        "        label = torch.tensor(sub_label, dtype=torch.long)\n",
        "\n",
        "        if self.graph_mode:\n",
        "            pred_label = np.argmax(self.pred[0][graph_idx], axis=0)\n",
        "            print(\"Graph predicted label: \", pred_label)\n",
        "        else:\n",
        "            #pred_label = np.argmax(self.pred[graph_idx][neighbors], axis=1)\n",
        "            pred_label = np.argmax(self.pred[neighbors], axis=1)\n",
        "\n",
        "            print(\"Node predicted label: \", pred_label[node_idx_new])\n",
        "\n",
        "        explainer = ExplainModule(\n",
        "            adj=adj,\n",
        "            x=x,\n",
        "            model=self.model,\n",
        "            label=label,\n",
        "            args=self.args,\n",
        "            writer=self.writer,\n",
        "            graph_idx=self.graph_idx,\n",
        "            graph_mode=self.graph_mode,\n",
        "        )\n",
        "        if self.args.gpu:\n",
        "            explainer = explainer.cuda()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "        # gradient baseline\n",
        "        if model == \"grad\":\n",
        "            explainer.zero_grad()\n",
        "            # pdb.set_trace()\n",
        "            adj_grad = torch.abs(\n",
        "                explainer.adj_feat_grad(node_idx_new, pred_label[node_idx_new])[0]\n",
        "            )[graph_idx]\n",
        "            masked_adj = adj_grad + adj_grad.t()\n",
        "            masked_adj = nn.functional.sigmoid(masked_adj)\n",
        "            masked_adj = masked_adj.cpu().detach().numpy() * sub_adj.squeeze()\n",
        "        else:\n",
        "            explainer.train()\n",
        "            begin_time = time.time()\n",
        "            for epoch in range(self.args.num_epochs):\n",
        "                explainer.zero_grad()\n",
        "                explainer.optimizer.zero_grad()\n",
        "                ypred, adj_atts = explainer(node_idx_new, unconstrained=unconstrained)\n",
        "                loss = explainer.loss(ypred, pred_label, node_idx_new, epoch)\n",
        "                loss.backward()\n",
        "\n",
        "                explainer.optimizer.step()\n",
        "                if explainer.scheduler is not None:\n",
        "                    explainer.scheduler.step()\n",
        "\n",
        "                mask_density = explainer.mask_density()\n",
        "                if self.print_training:\n",
        "                    print(\n",
        "                        \"epoch: \",\n",
        "                        epoch,\n",
        "                        \"; loss: \",\n",
        "                        loss.item(),\n",
        "                        \"; mask density: \",\n",
        "                        mask_density.item(),\n",
        "                        \"; pred: \",\n",
        "                        ypred,\n",
        "                    )\n",
        "                single_subgraph_label = sub_label.squeeze()\n",
        "\n",
        "                if self.writer is not None:\n",
        "                    self.writer.add_scalar(\"mask/density\", mask_density, epoch)\n",
        "                    self.writer.add_scalar(\n",
        "                        \"optimization/lr\",\n",
        "                        explainer.optimizer.param_groups[0][\"lr\"],\n",
        "                        epoch,\n",
        "                    )\n",
        "                    if epoch % 25 == 0:\n",
        "                        explainer.log_mask(epoch)\n",
        "                        explainer.log_masked_adj(\n",
        "                            node_idx_new, epoch, label=single_subgraph_label\n",
        "                        )\n",
        "                        explainer.log_adj_grad(\n",
        "                            node_idx_new, pred_label, epoch, label=single_subgraph_label\n",
        "                        )\n",
        "\n",
        "                    if epoch == 0:\n",
        "                        if self.model.att:\n",
        "                            # explain node\n",
        "                            print(\"adj att size: \", adj_atts.size())\n",
        "                            adj_att = torch.sum(adj_atts[0], dim=2)\n",
        "                            # adj_att = adj_att[neighbors][:, neighbors]\n",
        "                            node_adj_att = adj_att * adj.float().cuda()\n",
        "                            io_utils.log_matrix(\n",
        "                                self.writer, node_adj_att[0], \"att/matrix\", epoch\n",
        "                            )\n",
        "                            node_adj_att = node_adj_att[0].cpu().detach().numpy()\n",
        "                            G = io_utils.denoise_graph(\n",
        "                                node_adj_att,\n",
        "                                node_idx_new,\n",
        "                                threshold=3.8,  # threshold_num=20,\n",
        "                                max_component=True,\n",
        "                            )\n",
        "                            io_utils.log_graph(\n",
        "                                self.writer,\n",
        "                                G,\n",
        "                                name=\"att/graph\",\n",
        "                                identify_self=not self.graph_mode,\n",
        "                                nodecolor=\"label\",\n",
        "                                edge_vmax=None,\n",
        "                                args=self.args,\n",
        "                            )\n",
        "                if model != \"exp\":\n",
        "                    break\n",
        "\n",
        "            print(\"finished training in \", time.time() - begin_time)\n",
        "            if model == \"exp\":\n",
        "                masked_adj = (\n",
        "                    explainer.masked_adj[0].cpu().detach().numpy() * sub_adj.squeeze()\n",
        "                )\n",
        "            else:\n",
        "                adj_atts = nn.functional.sigmoid(adj_atts).squeeze()\n",
        "                masked_adj = adj_atts.cpu().detach().numpy() * sub_adj.squeeze()\n",
        "\n",
        "        fname = 'masked_adj_' + io_utils.gen_explainer_prefix(self.args) + (\n",
        "                'node_idx_'+str(node_idx)+'graph_idx_'+str(self.graph_idx)+'.npy')\n",
        "        with open(os.path.join(self.args.logdir, fname), 'wb') as outfile:\n",
        "            np.save(outfile, np.asarray(masked_adj.copy()))\n",
        "            print(\"Saved adjacency matrix to \", fname)\n",
        "        return masked_adj\n",
        "\n",
        "\n",
        "    # NODE EXPLAINER\n",
        "    def explain_nodes(self, node_indices, args, graph_idx=0):\n",
        "        \"\"\"\n",
        "        Explain nodes\n",
        "        Args:\n",
        "            - node_indices  :  Indices of the nodes to be explained\n",
        "            - args          :  Program arguments (mainly for logging paths)\n",
        "            - graph_idx     :  Index of the graph to explain the nodes from (if multiple).\n",
        "        \"\"\"\n",
        "        masked_adjs = [\n",
        "            self.explain(node_idx, graph_idx=graph_idx) for node_idx in node_indices\n",
        "        ]\n",
        "        ref_idx = node_indices[0]\n",
        "        ref_adj = masked_adjs[0]\n",
        "        curr_idx = node_indices[1]\n",
        "        curr_adj = masked_adjs[1]\n",
        "        new_ref_idx, _, ref_feat, _, _ = self.extract_neighborhood(ref_idx)\n",
        "        new_curr_idx, _, curr_feat, _, _ = self.extract_neighborhood(curr_idx)\n",
        "\n",
        "        G_ref = io_utils.denoise_graph(ref_adj, new_ref_idx, ref_feat, threshold=0.1)\n",
        "        denoised_ref_feat = np.array(\n",
        "            [G_ref.nodes[node][\"feat\"] for node in G_ref.nodes()]\n",
        "        )\n",
        "        denoised_ref_adj = nx.to_numpy_matrix(G_ref)\n",
        "        # ref center node\n",
        "        ref_node_idx = list(G_ref.nodes()).index(new_ref_idx)\n",
        "\n",
        "        G_curr = io_utils.denoise_graph(\n",
        "            curr_adj, new_curr_idx, curr_feat, threshold=0.1\n",
        "        )\n",
        "        denoised_curr_feat = np.array(\n",
        "            [G_curr.nodes[node][\"feat\"] for node in G_curr.nodes()]\n",
        "        )\n",
        "        denoised_curr_adj = nx.to_numpy_matrix(G_curr)\n",
        "        # curr center node\n",
        "        curr_node_idx = list(G_curr.nodes()).index(new_curr_idx)\n",
        "\n",
        "        P, aligned_adj, aligned_feat = self.align(\n",
        "            denoised_ref_feat,\n",
        "            denoised_ref_adj,\n",
        "            ref_node_idx,\n",
        "            denoised_curr_feat,\n",
        "            denoised_curr_adj,\n",
        "            curr_node_idx,\n",
        "            args=args,\n",
        "        )\n",
        "        io_utils.log_matrix(self.writer, P, \"align/P\", 0)\n",
        "\n",
        "        G_ref = nx.convert_node_labels_to_integers(G_ref)\n",
        "        io_utils.log_graph(self.writer, G_ref, \"align/ref\")\n",
        "        G_curr = nx.convert_node_labels_to_integers(G_curr)\n",
        "        io_utils.log_graph(self.writer, G_curr, \"align/before\")\n",
        "\n",
        "        P = P.cpu().detach().numpy()\n",
        "        aligned_adj = aligned_adj.cpu().detach().numpy()\n",
        "        aligned_feat = aligned_feat.cpu().detach().numpy()\n",
        "\n",
        "        aligned_idx = np.argmax(P[:, curr_node_idx])\n",
        "        print(\"aligned self: \", aligned_idx)\n",
        "        G_aligned = io_utils.denoise_graph(\n",
        "            aligned_adj, aligned_idx, aligned_feat, threshold=0.5\n",
        "        )\n",
        "        io_utils.log_graph(self.writer, G_aligned, \"mask/aligned\")\n",
        "\n",
        "        # io_utils.log_graph(self.writer, aligned_adj.cpu().detach().numpy(), new_curr_idx,\n",
        "        #        'align/aligned', epoch=1)\n",
        "\n",
        "        return masked_adjs\n",
        "\n",
        "\n",
        "    def explain_nodes_gnn_stats(self, node_indices, args, graph_idx=0, model=\"exp\"):\n",
        "        masked_adjs = [\n",
        "            self.explain(node_idx, graph_idx=graph_idx, model=model)\n",
        "            for node_idx in node_indices\n",
        "        ]\n",
        "        # pdb.set_trace()\n",
        "        graphs = []\n",
        "        feats = []\n",
        "        adjs = []\n",
        "        pred_all = []\n",
        "        real_all = []\n",
        "        for i, idx in enumerate(node_indices):\n",
        "            new_idx, _, feat, _, _ = self.extract_neighborhood(idx)\n",
        "            G = io_utils.denoise_graph(masked_adjs[i], new_idx, feat, threshold_num=20)\n",
        "            pred, real = self.make_pred_real(masked_adjs[i], new_idx)\n",
        "            pred_all.append(pred)\n",
        "            real_all.append(real)\n",
        "            denoised_feat = np.array([G.nodes[node][\"feat\"] for node in G.nodes()])\n",
        "            denoised_adj = nx.to_numpy_matrix(G)\n",
        "            graphs.append(G)\n",
        "            feats.append(denoised_feat)\n",
        "            adjs.append(denoised_adj)\n",
        "            io_utils.log_graph(\n",
        "                self.writer,\n",
        "                G,\n",
        "                \"graph/{}_{}_{}\".format(self.args.dataset, model, i),\n",
        "                identify_self=True,\n",
        "                args=self.args\n",
        "            )\n",
        "\n",
        "        pred_all = np.concatenate((pred_all), axis=0)\n",
        "        real_all = np.concatenate((real_all), axis=0)\n",
        "\n",
        "        auc_all = roc_auc_score(real_all, pred_all)\n",
        "        precision, recall, thresholds = precision_recall_curve(real_all, pred_all)\n",
        "\n",
        "        plt.switch_backend(\"agg\")\n",
        "        plt.plot(recall, precision)\n",
        "        plt.savefig(\"log/pr/pr_\" + self.args.dataset + \"_\" + model + \".png\")\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        auc_all = roc_auc_score(real_all, pred_all)\n",
        "        precision, recall, thresholds = precision_recall_curve(real_all, pred_all)\n",
        "\n",
        "        plt.switch_backend(\"agg\")\n",
        "        plt.plot(recall, precision)\n",
        "        plt.savefig(\"log/pr/pr_\" + self.args.dataset + \"_\" + model + \".png\")\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        with open(\"log/pr/auc_\" + self.args.dataset + \"_\" + model + \".txt\", \"w\") as f:\n",
        "            f.write(\n",
        "                \"dataset: {}, model: {}, auc: {}\\n\".format(\n",
        "                    self.args.dataset, \"exp\", str(auc_all)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return masked_adjs\n",
        "\n",
        "    # GRAPH EXPLAINER\n",
        "    def explain_graphs(self, graph_indices):\n",
        "        \"\"\"\n",
        "        Explain graphs.\n",
        "        \"\"\"\n",
        "        masked_adjs = []\n",
        "\n",
        "        for graph_idx in graph_indices:\n",
        "            masked_adj = self.explain(node_idx=0, graph_idx=graph_idx, graph_mode=True)\n",
        "            G_denoised = io_utils.denoise_graph(\n",
        "                masked_adj,\n",
        "                0,\n",
        "                threshold_num=20,\n",
        "                feat=self.feat[graph_idx],\n",
        "                max_component=False,\n",
        "            )\n",
        "            label = self.label[graph_idx]\n",
        "            io_utils.log_graph(\n",
        "                self.writer,\n",
        "                G_denoised,\n",
        "                \"graph/graphidx_{}_label={}\".format(graph_idx, label),\n",
        "                identify_self=False,\n",
        "                nodecolor=\"feat\",\n",
        "                args=self.args\n",
        "            )\n",
        "            masked_adjs.append(masked_adj)\n",
        "\n",
        "            G_orig = io_utils.denoise_graph(\n",
        "                self.adj[graph_idx],\n",
        "                0,\n",
        "                feat=self.feat[graph_idx],\n",
        "                threshold=None,\n",
        "                max_component=False,\n",
        "            )\n",
        "\n",
        "            io_utils.log_graph(\n",
        "                self.writer,\n",
        "                G_orig,\n",
        "                \"graph/graphidx_{}\".format(graph_idx),\n",
        "                identify_self=False,\n",
        "                nodecolor=\"feat\",\n",
        "                args=self.args\n",
        "            )\n",
        "\n",
        "        # plot cmap for graphs' node features\n",
        "        io_utils.plot_cmap_tb(self.writer, \"tab20\", 20, \"tab20_cmap\")\n",
        "\n",
        "        return masked_adjs\n",
        "\n",
        "    def log_representer(self, rep_val, sim_val, alpha, graph_idx=0):\n",
        "        \"\"\" visualize output of representer instances. \"\"\"\n",
        "        rep_val = rep_val.cpu().detach().numpy()\n",
        "        sim_val = sim_val.cpu().detach().numpy()\n",
        "        alpha = alpha.cpu().detach().numpy()\n",
        "        sorted_rep = sorted(range(len(rep_val)), key=lambda k: rep_val[k])\n",
        "        print(sorted_rep)\n",
        "        topk = 5\n",
        "        most_neg_idx = [sorted_rep[i] for i in range(topk)]\n",
        "        most_pos_idx = [sorted_rep[-i - 1] for i in range(topk)]\n",
        "        rep_idx = [most_pos_idx, most_neg_idx]\n",
        "\n",
        "        if self.graph_mode:\n",
        "            pred = np.argmax(self.pred[0][graph_idx], axis=0)\n",
        "        else:\n",
        "            pred = np.argmax(self.pred[graph_idx][self.train_idx], axis=1)\n",
        "        print(metrics.confusion_matrix(self.label[graph_idx][self.train_idx], pred))\n",
        "        plt.switch_backend(\"agg\")\n",
        "        fig = plt.figure(figsize=(5, 3), dpi=600)\n",
        "        for i in range(2):\n",
        "            for j in range(topk):\n",
        "                idx = self.train_idx[rep_idx[i][j]]\n",
        "                print(\n",
        "                    \"node idx: \",\n",
        "                    idx,\n",
        "                    \"; node label: \",\n",
        "                    self.label[graph_idx][idx],\n",
        "                    \"; pred: \",\n",
        "                    pred,\n",
        "                )\n",
        "\n",
        "                idx_new, sub_adj, sub_feat, sub_label, neighbors = self.extract_neighborhood(\n",
        "                    idx, graph_idx\n",
        "                )\n",
        "                G = nx.from_numpy_matrix(sub_adj)\n",
        "                node_colors = [1 for i in range(G.number_of_nodes())]\n",
        "                node_colors[idx_new] = 0\n",
        "                # node_color='#336699',\n",
        "\n",
        "                ax = plt.subplot(2, topk, i * topk + j + 1)\n",
        "                nx.draw(\n",
        "                    G,\n",
        "                    pos=nx.spring_layout(G),\n",
        "                    with_labels=True,\n",
        "                    font_size=4,\n",
        "                    node_color=node_colors,\n",
        "                    cmap=plt.get_cmap(\"Set1\"),\n",
        "                    vmin=0,\n",
        "                    vmax=8,\n",
        "                    edge_vmin=0.0,\n",
        "                    edge_vmax=1.0,\n",
        "                    width=0.5,\n",
        "                    node_size=25,\n",
        "                    alpha=0.7,\n",
        "                )\n",
        "                ax.xaxis.set_visible(False)\n",
        "        fig.canvas.draw()\n",
        "        self.writer.add_image(\n",
        "            \"local/representer_neigh\", tensorboardX.utils.figure_to_image(fig), 0\n",
        "        )\n",
        "\n",
        "    def representer(self):\n",
        "        \"\"\"\n",
        "        experiment using representer theorem for finding supporting instances.\n",
        "        https://papers.nips.cc/paper/8141-representer-point-selection-for-explaining-deep-neural-networks.pdf\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        self.model.zero_grad()\n",
        "        adj = torch.tensor(self.adj, dtype=torch.float)\n",
        "        x = torch.tensor(self.feat, requires_grad=True, dtype=torch.float)\n",
        "        label = torch.tensor(self.label, dtype=torch.long)\n",
        "        if self.args.gpu:\n",
        "            adj, x, label = adj.cuda(), x.cuda(), label.cuda()\n",
        "\n",
        "        preds, _ = self.model(x, adj)\n",
        "        preds.retain_grad()\n",
        "        self.embedding = self.model.embedding_tensor\n",
        "        loss = self.model.loss(preds, label)\n",
        "        loss.backward()\n",
        "        self.preds_grad = preds.grad\n",
        "        pred_idx = np.expand_dims(np.argmax(self.pred, axis=2), axis=2)\n",
        "        pred_idx = torch.LongTensor(pred_idx)\n",
        "        if self.args.gpu:\n",
        "            pred_idx = pred_idx.cuda()\n",
        "        self.alpha = self.preds_grad\n",
        "\n",
        "\n",
        "    # Utilities\n",
        "    def extract_neighborhood(self, node_idx, graph_idx=0):\n",
        "        \"\"\"Returns the neighborhood of a given ndoe.\"\"\"\n",
        "        #neighbors_adj_row = self.neighborhoods[graph_idx][node_idx, :]\n",
        "        neighbors_adj_row = self.neighborhoods[node_idx, :]\n",
        "\n",
        "        # index of the query node in the new adj\n",
        "        node_idx_new = sum(neighbors_adj_row[:node_idx])\n",
        "        neighbors = np.nonzero(neighbors_adj_row)[0]\n",
        "\n",
        "        #sub_adj = self.adj[graph_idx][neighbors][:, neighbors]\n",
        "        sub_adj = self.adj[neighbors][:, neighbors]\n",
        "\n",
        "        sub_feat = self.feat[graph_idx, neighbors]\n",
        "\n",
        "        #sub_label = self.label[graph_idx][neighbors]\n",
        "        sub_label = self.label[neighbors]\n",
        "\n",
        "        return node_idx_new, sub_adj, sub_feat, sub_label, neighbors\n",
        "\n",
        "    def align(\n",
        "        self, ref_feat, ref_adj, ref_node_idx, curr_feat, curr_adj, curr_node_idx, args\n",
        "    ):\n",
        "        \"\"\" Tries to find an alignment between two graphs.\n",
        "        \"\"\"\n",
        "        ref_adj = torch.FloatTensor(ref_adj)\n",
        "        curr_adj = torch.FloatTensor(curr_adj)\n",
        "\n",
        "        ref_feat = torch.FloatTensor(ref_feat)\n",
        "        curr_feat = torch.FloatTensor(curr_feat)\n",
        "\n",
        "        P = nn.Parameter(torch.FloatTensor(ref_adj.shape[0], curr_adj.shape[0]))\n",
        "        with torch.no_grad():\n",
        "            nn.init.constant_(P, 1.0 / ref_adj.shape[0])\n",
        "            P[ref_node_idx, :] = 0.0\n",
        "            P[:, curr_node_idx] = 0.0\n",
        "            P[ref_node_idx, curr_node_idx] = 1.0\n",
        "        opt = torch.optim.Adam([P], lr=0.01, betas=(0.5, 0.999))\n",
        "        for i in range(args.align_steps):\n",
        "            opt.zero_grad()\n",
        "            feat_loss = torch.norm(P @ curr_feat - ref_feat)\n",
        "\n",
        "            aligned_adj = P @ curr_adj @ torch.transpose(P, 0, 1)\n",
        "            align_loss = torch.norm(aligned_adj - ref_adj)\n",
        "            loss = feat_loss + align_loss\n",
        "            loss.backward()  # Calculate gradients\n",
        "            self.writer.add_scalar(\"optimization/align_loss\", loss, i)\n",
        "            print(\"iter: \", i, \"; loss: \", loss)\n",
        "            opt.step()\n",
        "\n",
        "        return P, aligned_adj, P @ curr_feat\n",
        "\n",
        "    def make_pred_real(self, adj, start):\n",
        "        # house graph\n",
        "        if self.args.dataset == \"syn1\" or self.args.dataset == \"syn2\":\n",
        "            # num_pred = max(G.number_of_edges(), 6)\n",
        "            pred = adj[np.triu(adj) > 0]\n",
        "            real = adj.copy()\n",
        "\n",
        "            if real[start][start + 1] > 0:\n",
        "                real[start][start + 1] = 10\n",
        "            if real[start + 1][start + 2] > 0:\n",
        "                real[start + 1][start + 2] = 10\n",
        "            if real[start + 2][start + 3] > 0:\n",
        "                real[start + 2][start + 3] = 10\n",
        "            if real[start][start + 3] > 0:\n",
        "                real[start][start + 3] = 10\n",
        "            if real[start][start + 4] > 0:\n",
        "                real[start][start + 4] = 10\n",
        "            if real[start + 1][start + 4]:\n",
        "                real[start + 1][start + 4] = 10\n",
        "            real = real[np.triu(real) > 0]\n",
        "            real[real != 10] = 0\n",
        "            real[real == 10] = 1\n",
        "\n",
        "        # cycle graph\n",
        "        elif self.args.dataset == \"syn4\":\n",
        "            pred = adj[np.triu(adj) > 0]\n",
        "            real = adj.copy()\n",
        "            # pdb.set_trace()\n",
        "            if real[start][start + 1] > 0:\n",
        "                real[start][start + 1] = 10\n",
        "            if real[start + 1][start + 2] > 0:\n",
        "                real[start + 1][start + 2] = 10\n",
        "            if real[start + 2][start + 3] > 0:\n",
        "                real[start + 2][start + 3] = 10\n",
        "            if real[start + 3][start + 4] > 0:\n",
        "                real[start + 3][start + 4] = 10\n",
        "            if real[start + 4][start + 5] > 0:\n",
        "                real[start + 4][start + 5] = 10\n",
        "            if real[start][start + 5]:\n",
        "                real[start][start + 5] = 10\n",
        "            real = real[np.triu(real) > 0]\n",
        "            real[real != 10] = 0\n",
        "            real[real == 10] = 1\n",
        "\n",
        "        return pred, real\n",
        "\n",
        "\n",
        "class ExplainModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        adj,\n",
        "        x,\n",
        "        model,\n",
        "        label,\n",
        "        args,\n",
        "        graph_idx=0,\n",
        "        writer=None,\n",
        "        use_sigmoid=True,\n",
        "        graph_mode=False,\n",
        "    ):\n",
        "        super(ExplainModule, self).__init__()\n",
        "        self.adj = adj\n",
        "        self.x = x\n",
        "        self.model = model\n",
        "        self.label = label\n",
        "        self.graph_idx = graph_idx\n",
        "        self.args = args\n",
        "        self.writer = writer\n",
        "        self.mask_act = args.mask_act\n",
        "        self.use_sigmoid = use_sigmoid\n",
        "        self.graph_mode = graph_mode\n",
        "\n",
        "        init_strategy = \"normal\"\n",
        "        num_nodes = adj.size()[1]\n",
        "        self.mask, self.mask_bias = self.construct_edge_mask(\n",
        "            num_nodes, init_strategy=init_strategy\n",
        "        )\n",
        "\n",
        "        self.feat_mask = self.construct_feat_mask(x.size(-1), init_strategy=\"constant\")\n",
        "        params = [self.mask, self.feat_mask]\n",
        "        if self.mask_bias is not None:\n",
        "            params.append(self.mask_bias)\n",
        "        # For masking diagonal entries\n",
        "        self.diag_mask = torch.ones(num_nodes, num_nodes) - torch.eye(num_nodes)\n",
        "        if args.gpu:\n",
        "            self.diag_mask = self.diag_mask.cuda()\n",
        "\n",
        "        self.scheduler, self.optimizer = train_utils.build_optimizer(args, params)\n",
        "\n",
        "        self.coeffs = {\n",
        "            \"size\": 0.005,\n",
        "            \"feat_size\": 1.0,\n",
        "            \"ent\": 1.0,\n",
        "            \"feat_ent\": 0.1,\n",
        "            \"grad\": 0,\n",
        "            \"lap\": 1.0,\n",
        "        }\n",
        "\n",
        "    def construct_feat_mask(self, feat_dim, init_strategy=\"normal\"):\n",
        "        mask = nn.Parameter(torch.FloatTensor(feat_dim))\n",
        "        if init_strategy == \"normal\":\n",
        "            std = 0.1\n",
        "            with torch.no_grad():\n",
        "                mask.normal_(1.0, std)\n",
        "        elif init_strategy == \"constant\":\n",
        "            with torch.no_grad():\n",
        "                nn.init.constant_(mask, 0.0)\n",
        "                # mask[0] = 2\n",
        "        return mask\n",
        "\n",
        "    def construct_edge_mask(self, num_nodes, init_strategy=\"normal\", const_val=1.0):\n",
        "        mask = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))\n",
        "        if init_strategy == \"normal\":\n",
        "            std = nn.init.calculate_gain(\"relu\") * math.sqrt(\n",
        "                2.0 / (num_nodes + num_nodes)\n",
        "            )\n",
        "            with torch.no_grad():\n",
        "                mask.normal_(1.0, std)\n",
        "                # mask.clamp_(0.0, 1.0)\n",
        "        elif init_strategy == \"const\":\n",
        "            nn.init.constant_(mask, const_val)\n",
        "\n",
        "        if self.args.mask_bias:\n",
        "            mask_bias = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))\n",
        "            nn.init.constant_(mask_bias, 0.0)\n",
        "        else:\n",
        "            mask_bias = None\n",
        "\n",
        "        return mask, mask_bias\n",
        "\n",
        "    def _masked_adj(self):\n",
        "        sym_mask = self.mask\n",
        "        if self.mask_act == \"sigmoid\":\n",
        "            sym_mask = torch.sigmoid(self.mask)\n",
        "        elif self.mask_act == \"ReLU\":\n",
        "            sym_mask = nn.ReLU()(self.mask)\n",
        "        sym_mask = (sym_mask + sym_mask.t()) / 2\n",
        "        adj = self.adj.cuda() if self.args.gpu else self.adj\n",
        "        masked_adj = adj * sym_mask\n",
        "        if self.args.mask_bias:\n",
        "            bias = (self.mask_bias + self.mask_bias.t()) / 2\n",
        "            bias = nn.ReLU6()(bias * 6) / 6\n",
        "            masked_adj += (bias + bias.t()) / 2\n",
        "        return masked_adj * self.diag_mask\n",
        "\n",
        "    def mask_density(self):\n",
        "        mask_sum = torch.sum(self._masked_adj()).cpu()\n",
        "        adj_sum = torch.sum(self.adj)\n",
        "        return mask_sum / adj_sum\n",
        "\n",
        "    def forward(self, node_idx, unconstrained=False, mask_features=True, marginalize=False):\n",
        "        x = self.x.cuda() if self.args.gpu else self.x\n",
        "\n",
        "        if unconstrained:\n",
        "            sym_mask = torch.sigmoid(self.mask) if self.use_sigmoid else self.mask\n",
        "            self.masked_adj = (\n",
        "                torch.unsqueeze((sym_mask + sym_mask.t()) / 2, 0) * self.diag_mask\n",
        "            )\n",
        "        else:\n",
        "            self.masked_adj = self._masked_adj()\n",
        "            if mask_features:\n",
        "                feat_mask = (\n",
        "                    torch.sigmoid(self.feat_mask)\n",
        "                    if self.use_sigmoid\n",
        "                    else self.feat_mask\n",
        "                )\n",
        "                if marginalize:\n",
        "                    std_tensor = torch.ones_like(x, dtype=torch.float) / 2\n",
        "                    mean_tensor = torch.zeros_like(x, dtype=torch.float) - x\n",
        "                    z = torch.normal(mean=mean_tensor, std=std_tensor)\n",
        "                    x = x + z * (1 - feat_mask)\n",
        "                else:\n",
        "                    x = x * feat_mask\n",
        "\n",
        "        ypred, adj_att = self.model(x, self.masked_adj)\n",
        "        if self.graph_mode:\n",
        "            res = nn.Softmax(dim=0)(ypred[0])\n",
        "        else:\n",
        "            node_pred = ypred[self.graph_idx, node_idx, :]\n",
        "            res = nn.Softmax(dim=0)(node_pred)\n",
        "        return res, adj_att\n",
        "\n",
        "    def adj_feat_grad(self, node_idx, pred_label_node):\n",
        "        self.model.zero_grad()\n",
        "        self.adj.requires_grad = True\n",
        "        self.x.requires_grad = True\n",
        "        if self.adj.grad is not None:\n",
        "            self.adj.grad.zero_()\n",
        "            self.x.grad.zero_()\n",
        "        if self.args.gpu:\n",
        "            adj = self.adj.cuda()\n",
        "            x = self.x.cuda()\n",
        "            label = self.label.cuda()\n",
        "        else:\n",
        "            x, adj = self.x, self.adj\n",
        "        ypred, _ = self.model(x, adj)\n",
        "        if self.graph_mode:\n",
        "            logit = nn.Softmax(dim=0)(ypred[0])\n",
        "        else:\n",
        "            logit = nn.Softmax(dim=0)(ypred[self.graph_idx, node_idx, :])\n",
        "        logit = logit[pred_label_node]\n",
        "        loss = -torch.log(logit)\n",
        "        loss.backward()\n",
        "        return self.adj.grad, self.x.grad\n",
        "\n",
        "    def loss(self, pred, pred_label, node_idx, epoch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred: prediction made by current model\n",
        "            pred_label: the label predicted by the original model.\n",
        "        \"\"\"\n",
        "        mi_obj = False\n",
        "        if mi_obj:\n",
        "            pred_loss = -torch.sum(pred * torch.log(pred))\n",
        "        else:\n",
        "            pred_label_node = pred_label if self.graph_mode else pred_label[node_idx]\n",
        "            gt_label_node = self.label if self.graph_mode else self.label[0][node_idx]\n",
        "            logit = pred[gt_label_node]\n",
        "            pred_loss = -torch.log(logit)\n",
        "        # size\n",
        "        mask = self.mask\n",
        "        if self.mask_act == \"sigmoid\":\n",
        "            mask = torch.sigmoid(self.mask)\n",
        "        elif self.mask_act == \"ReLU\":\n",
        "            mask = nn.ReLU()(self.mask)\n",
        "        size_loss = self.coeffs[\"size\"] * torch.sum(mask)\n",
        "\n",
        "        # pre_mask_sum = torch.sum(self.feat_mask)\n",
        "        feat_mask = (\n",
        "            torch.sigmoid(self.feat_mask) if self.use_sigmoid else self.feat_mask\n",
        "        )\n",
        "        feat_size_loss = self.coeffs[\"feat_size\"] * torch.mean(feat_mask)\n",
        "\n",
        "        # entropy\n",
        "        mask_ent = -mask * torch.log(mask) - (1 - mask) * torch.log(1 - mask)\n",
        "        mask_ent_loss = self.coeffs[\"ent\"] * torch.mean(mask_ent)\n",
        "\n",
        "        feat_mask_ent = - feat_mask             \\\n",
        "                        * torch.log(feat_mask)  \\\n",
        "                        - (1 - feat_mask)       \\\n",
        "                        * torch.log(1 - feat_mask)\n",
        "\n",
        "        feat_mask_ent_loss = self.coeffs[\"feat_ent\"] * torch.mean(feat_mask_ent)\n",
        "\n",
        "        # laplacian\n",
        "        D = torch.diag(torch.sum(self.masked_adj[0], 0))\n",
        "        m_adj = self.masked_adj if self.graph_mode else self.masked_adj[self.graph_idx]\n",
        "        L = D - m_adj\n",
        "        pred_label_t = torch.tensor(pred_label, dtype=torch.float)\n",
        "        if self.args.gpu:\n",
        "            pred_label_t = pred_label_t.cuda()\n",
        "            L = L.cuda()\n",
        "        if self.graph_mode:\n",
        "            lap_loss = 0\n",
        "        else:\n",
        "            lap_loss = (self.coeffs[\"lap\"]\n",
        "                * (pred_label_t @ L @ pred_label_t)\n",
        "                / self.adj.numel()\n",
        "            )\n",
        "\n",
        "        # grad\n",
        "        # adj\n",
        "        # adj_grad, x_grad = self.adj_feat_grad(node_idx, pred_label_node)\n",
        "        # adj_grad = adj_grad[self.graph_idx]\n",
        "        # x_grad = x_grad[self.graph_idx]\n",
        "        # if self.args.gpu:\n",
        "        #    adj_grad = adj_grad.cuda()\n",
        "        # grad_loss = self.coeffs['grad'] * -torch.mean(torch.abs(adj_grad) * mask)\n",
        "\n",
        "        # feat\n",
        "        # x_grad_sum = torch.sum(x_grad, 1)\n",
        "        # grad_feat_loss = self.coeffs['featgrad'] * -torch.mean(x_grad_sum * mask)\n",
        "\n",
        "        loss = pred_loss + size_loss + lap_loss + mask_ent_loss + feat_size_loss\n",
        "        if self.writer is not None:\n",
        "            self.writer.add_scalar(\"optimization/size_loss\", size_loss, epoch)\n",
        "            self.writer.add_scalar(\"optimization/feat_size_loss\", feat_size_loss, epoch)\n",
        "            self.writer.add_scalar(\"optimization/mask_ent_loss\", mask_ent_loss, epoch)\n",
        "            self.writer.add_scalar(\n",
        "                \"optimization/feat_mask_ent_loss\", mask_ent_loss, epoch\n",
        "            )\n",
        "            # self.writer.add_scalar('optimization/grad_loss', grad_loss, epoch)\n",
        "            self.writer.add_scalar(\"optimization/pred_loss\", pred_loss, epoch)\n",
        "            self.writer.add_scalar(\"optimization/lap_loss\", lap_loss, epoch)\n",
        "            self.writer.add_scalar(\"optimization/overall_loss\", loss, epoch)\n",
        "        return loss\n",
        "\n",
        "    def log_mask(self, epoch):\n",
        "        plt.switch_backend(\"agg\")\n",
        "        fig = plt.figure(figsize=(4, 3), dpi=400)\n",
        "        plt.imshow(self.mask.cpu().detach().numpy(), cmap=plt.get_cmap(\"BuPu\"))\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.solids.set_edgecolor(\"face\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        fig.canvas.draw()\n",
        "        self.writer.add_image(\n",
        "            \"mask/mask\", tensorboardX.utils.figure_to_image(fig), epoch\n",
        "        )\n",
        "\n",
        "        # fig = plt.figure(figsize=(4,3), dpi=400)\n",
        "        # plt.imshow(self.feat_mask.cpu().detach().numpy()[:,np.newaxis], cmap=plt.get_cmap('BuPu'))\n",
        "        # cbar = plt.colorbar()\n",
        "        # cbar.solids.set_edgecolor(\"face\")\n",
        "\n",
        "        # plt.tight_layout()\n",
        "        # fig.canvas.draw()\n",
        "        # self.writer.add_image('mask/feat_mask', tensorboardX.utils.figure_to_image(fig), epoch)\n",
        "        io_utils.log_matrix(\n",
        "            self.writer, torch.sigmoid(self.feat_mask), \"mask/feat_mask\", epoch\n",
        "        )\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 3), dpi=400)\n",
        "        # use [0] to remove the batch dim\n",
        "        plt.imshow(self.masked_adj[0].cpu().detach().numpy(), cmap=plt.get_cmap(\"BuPu\"))\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.solids.set_edgecolor(\"face\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        fig.canvas.draw()\n",
        "        self.writer.add_image(\n",
        "            \"mask/adj\", tensorboardX.utils.figure_to_image(fig), epoch\n",
        "        )\n",
        "\n",
        "        if self.args.mask_bias:\n",
        "            fig = plt.figure(figsize=(4, 3), dpi=400)\n",
        "            # use [0] to remove the batch dim\n",
        "            plt.imshow(self.mask_bias.cpu().detach().numpy(), cmap=plt.get_cmap(\"BuPu\"))\n",
        "            cbar = plt.colorbar()\n",
        "            cbar.solids.set_edgecolor(\"face\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            fig.canvas.draw()\n",
        "            self.writer.add_image(\n",
        "                \"mask/bias\", tensorboardX.utils.figure_to_image(fig), epoch\n",
        "            )\n",
        "\n",
        "    def log_adj_grad(self, node_idx, pred_label, epoch, label=None):\n",
        "        log_adj = False\n",
        "\n",
        "        if self.graph_mode:\n",
        "            predicted_label = pred_label\n",
        "            # adj_grad, x_grad = torch.abs(self.adj_feat_grad(node_idx, predicted_label)[0])[0]\n",
        "            adj_grad, x_grad = self.adj_feat_grad(node_idx, predicted_label)\n",
        "            adj_grad = torch.abs(adj_grad)[0]\n",
        "            x_grad = torch.sum(x_grad[0], 0, keepdim=True).t()\n",
        "        else:\n",
        "            predicted_label = pred_label[node_idx]\n",
        "            # adj_grad = torch.abs(self.adj_feat_grad(node_idx, predicted_label)[0])[self.graph_idx]\n",
        "            adj_grad, x_grad = self.adj_feat_grad(node_idx, predicted_label)\n",
        "            adj_grad = torch.abs(adj_grad)[self.graph_idx]\n",
        "            x_grad = x_grad[self.graph_idx][node_idx][:, np.newaxis]\n",
        "            # x_grad = torch.sum(x_grad[self.graph_idx], 0, keepdim=True).t()\n",
        "        adj_grad = (adj_grad + adj_grad.t()) / 2\n",
        "        adj_grad = (adj_grad * self.adj).squeeze()\n",
        "        if log_adj:\n",
        "            io_utils.log_matrix(self.writer, adj_grad, \"grad/adj_masked\", epoch)\n",
        "            self.adj.requires_grad = False\n",
        "            io_utils.log_matrix(self.writer, self.adj.squeeze(), \"grad/adj_orig\", epoch)\n",
        "\n",
        "        masked_adj = self.masked_adj[0].cpu().detach().numpy()\n",
        "\n",
        "        # only for graph mode since many node neighborhoods for syn tasks are relatively large for\n",
        "        # visualization\n",
        "        if self.graph_mode:\n",
        "            G = io_utils.denoise_graph(\n",
        "                masked_adj, node_idx, feat=self.x[0], threshold=None, max_component=False\n",
        "            )\n",
        "            io_utils.log_graph(\n",
        "                self.writer,\n",
        "                G,\n",
        "                name=\"grad/graph_orig\",\n",
        "                epoch=epoch,\n",
        "                identify_self=False,\n",
        "                label_node_feat=True,\n",
        "                nodecolor=\"feat\",\n",
        "                edge_vmax=None,\n",
        "                args=self.args,\n",
        "            )\n",
        "        io_utils.log_matrix(self.writer, x_grad, \"grad/feat\", epoch)\n",
        "\n",
        "        adj_grad = adj_grad.detach().numpy()\n",
        "        if self.graph_mode:\n",
        "            print(\"GRAPH model\")\n",
        "            G = io_utils.denoise_graph(\n",
        "                adj_grad,\n",
        "                node_idx,\n",
        "                feat=self.x[0],\n",
        "                threshold=0.0003,  # threshold_num=20,\n",
        "                max_component=True,\n",
        "            )\n",
        "            io_utils.log_graph(\n",
        "                self.writer,\n",
        "                G,\n",
        "                name=\"grad/graph\",\n",
        "                epoch=epoch,\n",
        "                identify_self=False,\n",
        "                label_node_feat=True,\n",
        "                nodecolor=\"feat\",\n",
        "                edge_vmax=None,\n",
        "                args=self.args,\n",
        "            )\n",
        "        else:\n",
        "            # G = io_utils.denoise_graph(adj_grad, node_idx, label=label, threshold=0.5)\n",
        "            G = io_utils.denoise_graph(adj_grad, node_idx, threshold_num=12)\n",
        "            io_utils.log_graph(\n",
        "                self.writer, G, name=\"grad/graph\", epoch=epoch, args=self.args\n",
        "            )\n",
        "\n",
        "        # if graph attention, also visualize att\n",
        "\n",
        "    def log_masked_adj(self, node_idx, epoch, name=\"mask/graph\", label=None):\n",
        "        # use [0] to remove the batch dim\n",
        "        masked_adj = self.masked_adj[0].cpu().detach().numpy()\n",
        "        if self.graph_mode:\n",
        "            G = io_utils.denoise_graph(\n",
        "                masked_adj,\n",
        "                node_idx,\n",
        "                feat=self.x[0],\n",
        "                threshold=0.2,  # threshold_num=20,\n",
        "                max_component=True,\n",
        "            )\n",
        "            io_utils.log_graph(\n",
        "                self.writer,\n",
        "                G,\n",
        "                name=name,\n",
        "                identify_self=False,\n",
        "                nodecolor=\"feat\",\n",
        "                epoch=epoch,\n",
        "                label_node_feat=True,\n",
        "                edge_vmax=None,\n",
        "                args=self.args,\n",
        "            )\n",
        "        else:\n",
        "            G = io_utils.denoise_graph(\n",
        "                masked_adj, node_idx, threshold_num=12, max_component=True\n",
        "            )\n",
        "            io_utils.log_graph(\n",
        "                self.writer,\n",
        "                G,\n",
        "                name=name,\n",
        "                identify_self=True,\n",
        "                nodecolor=\"label\",\n",
        "                epoch=epoch,\n",
        "                edge_vmax=None,\n",
        "                args=self.args,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdqgQ2648Vlv"
      },
      "source": [
        "# NetworkX Plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arQB3YdG8bCz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import csrgraph as cg\n",
        "import numpy as np\n",
        "\n",
        "class NXG:\n",
        "    def get_correct_idx(predictions, labels, get_correct=True):\n",
        "      \"\"\"\n",
        "      input: tensor of predicted values of shape [num_nodes, num_labels], tensor of ground truth labels of shape [num_nodes],\n",
        "              bool determining whether to get indices of correctly or incorrectly predicted nodes\n",
        "      output: tensor of indices (correct or incorrect)\n",
        "      \"\"\"\n",
        "      if get_correct:\n",
        "        idx = (predictions.argmax(1) == labels)\n",
        "      else:\n",
        "        idx = (predictions.argmax(1) != labels)\n",
        "\n",
        "      idx = Utils.bool_to_idx(idx).flatten()\n",
        "      return idx\n",
        "\n",
        "    def protected_node_ratio(nodes, protected_set):\n",
        "      \"\"\"\n",
        "      input: tensor of node indices, tensor of protected indices\n",
        "      output: percentage of protected nodes in nodes\n",
        "      \"\"\"\n",
        "      protected = 0\n",
        "      for i in nodes:\n",
        "        if protected_set[i] == True:\n",
        "          protected += 1\n",
        "      return (float(protected) / len(nodes))\n",
        "\n",
        "\n",
        "    def plot_wrong_pred(nx_graph, pred, labels):\n",
        "      \"\"\"\n",
        "      input: networkx graph, tensor of predicted values of shape [num_nodes, num_labels], tensor of ground truth labels of shape [num_nodes]\n",
        "      output: diagram with incorrectly predicted nodes colored\n",
        "      \"\"\"\n",
        "      pos = nx.spring_layout(nx_graph, seed=42)\n",
        "\n",
        "      wrong_idx = NXG.get_correct_idx(pred, labels, False).cpu()\n",
        "\n",
        "      wrong_bin = np.where(wrong_idx == True, 1, 0.25)\n",
        "      node_size = list(map(lambda x: x*100, wrong_bin))\n",
        "\n",
        "      plt.figure(figsize=(13, 13))\n",
        "      nodes = nx.draw_networkx_nodes(nx_graph, pos,\n",
        "                                    cmap=plt.cm.plasma,\n",
        "                                    node_color=wrong_bin,\n",
        "                                    node_size=0.5,\n",
        "                                    alpha=wrong_bin\n",
        "                                    )\n",
        "\n",
        "      edges = nx.draw_networkx_edges(nx_graph, pos, width=0.25, alpha=0.3)\n",
        "      plt.show()\n",
        "\n",
        "    def plot_sets(G, protected_set):\n",
        "      \"\"\"\n",
        "      input: networkx graph and a 1-dim tensor of indices of protected set\n",
        "      output: diagram with nodes in the protected set colored red and all other nodes colored blue /\n",
        "                edges between two nodes of the same type will be the same color as their specified node types /\n",
        "                else they will be greeen\n",
        "      \"\"\"\n",
        "      community_map = {}\n",
        "      protected_set = Utils.bool_to_idx(protected_set)\n",
        "      for node in G.nodes():\n",
        "        if node in protected_set:\n",
        "          community_map[node] = 1\n",
        "        else:\n",
        "          community_map[node] = 0\n",
        "\n",
        "      node_color = []\n",
        "      color_map = {0: 0, 1: 1}\n",
        "      node_color = [color_map[community_map[node]] for node in G.nodes()]\n",
        "\n",
        "      node_type_map = {0:'gX', 1:'g0'}\n",
        "      node_types = {node:node_type_map[community_map[node]] for node in G.nodes()}\n",
        "      nx.set_node_attributes(G, node_types, 'node_type')\n",
        "\n",
        "      node_label_map = {0:0, 1:1}\n",
        "      node_labels = {node:node_label_map[community_map[node]] for node in G.nodes()}\n",
        "      nx.set_node_attributes(G, node_labels, 'node_label')\n",
        "\n",
        "      edge_type_map = {'00':'e0', '11':'e1', '01':'e2', '10':'e2'}\n",
        "      edge_types = {}\n",
        "\n",
        "      for edge in G.edges:\n",
        "        class_node_0 = str(community_map[edge[0]])\n",
        "        class_node_1 = str(community_map[edge[1]])\n",
        "        edge_type = class_node_0 + class_node_1\n",
        "        edge_types[edge] = edge_type_map[edge_type]\n",
        "\n",
        "      nx.set_edge_attributes(G, edge_types, 'edge_type')\n",
        "\n",
        "      edge_color = {}\n",
        "      for edge in G.edges():\n",
        "        n1, n2 = edge\n",
        "        edge_color[edge] = community_map[n1] if community_map[n1] == community_map[n2] else 2\n",
        "        if community_map[n1] == community_map[n2] and community_map[n1] == 0:\n",
        "          edge_color[edge] = 'blue'\n",
        "        elif community_map[n1] == community_map[n2] and community_map[n1] == 1:\n",
        "          edge_color[edge] = 'red'\n",
        "        else:\n",
        "          edge_color[edge] = 'green'\n",
        "\n",
        "      pos = nx.spring_layout(G)\n",
        "      nx.classes.function.set_edge_attributes(G, edge_color, name='color')\n",
        "      colors = nx.get_edge_attributes(G,'color').values()\n",
        "      labels = nx.get_node_attributes(G, 'node_type')\n",
        "\n",
        "      plt.figure(figsize=(18, 18))\n",
        "      nodes = nx.draw_networkx_nodes(G, pos=pos, cmap=plt.get_cmap('coolwarm'), node_color=node_color, node_size=0.5)\n",
        "      edges = nx.draw_networkx_edges(G, pos=pos, edge_color=colors, width=0.35, alpha=0.3)\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    def plot_edge_changes(nx_graph, locked_adj, base_adj):\n",
        "      \"\"\"\n",
        "\n",
        "      \"\"\"\n",
        "      diff = locked_adj - base_adj\n",
        "      edge_color = {}\n",
        "      for edge in  nx_graph.edges:\n",
        "        e1, e2 = edge\n",
        "        if diff[e1][e2] == 1:\n",
        "          edge_color[edge] = 'red'\n",
        "        else:\n",
        "          edge_color[edge] = 'blue'\n",
        "\n",
        "      print(edge_color)\n",
        "      pos = nx.spring_layout(nx_graph)\n",
        "      nx.classes.function.set_edge_attributes(nx_graph, edge_color, name='color')\n",
        "      colors = nx.get_edge_attributes(nx_graph,'color').values()\n",
        "\n",
        "      plt.figure(figsize=(15, 15))\n",
        "      nodes = nx.draw_networkx_nodes(nx_graph, pos=pos, cmap=plt.get_cmap('coolwarm'), node_size=0.5)\n",
        "      edges = nx.draw_networkx_edges(nx_graph, pos=pos, edge_color=colors, width=0.3, alpha=0.3)\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "    def avg_degree(nx_graph, ind):\n",
        "      \"\"\"\n",
        "      input: networkx graph, tensor of node indices to be averaged over\n",
        "      output: float value average degree of nodes in the set of indices\n",
        "      \"\"\"\n",
        "      avg = 0\n",
        "      for i in ind:\n",
        "        avg += nx_graph.degree[int(i)]\n",
        "      return avg / float(len(ind))\n",
        "\n",
        "\n",
        "    def protected_neighbor_ratio(nx_graph, protected_set):\n",
        "      \"\"\"\n",
        "      input: networkx graph, 1-dim tensor of protected nodes in the graph\n",
        "      output: 1-dim tensor of size [num_nodes] where each node index is equal to avg protected neighbor ratio degree\n",
        "      \"\"\"\n",
        "      ratio = torch.zeros(graph.num_nodes)\n",
        "      for i in range(nx_graph.number_of_nodes()):\n",
        "        protected_count = 0\n",
        "        for j in nx_graph.neighbors(i):\n",
        "          if j in protected_set:\n",
        "            protected_count += 1\n",
        "        if graph.degree[i] != 0:\n",
        "          ratio[i] = protected_count / graph.degree[i]\n",
        "        else:\n",
        "          ratio[i] = 0\n",
        "      return ratio\n",
        "\n",
        "\n",
        "    def avg_clustering(nx_graph, ind):\n",
        "      \"\"\"\n",
        "      input: networkx graph, tensor of node indices to be averaged over\n",
        "      output: float value average clustering of nodes in the set of indices\n",
        "      \"\"\"\n",
        "      sum = 0\n",
        "      for i in ind:\n",
        "        sum += nx.clustering(nx_graph, int(i))\n",
        "      return sum / len(ind)\n",
        "\n",
        "\n",
        "    def manual_clustering(nx_graph):\n",
        "      \"\"\"\n",
        "      self-implementation of the networkx clustering method\n",
        "      input: networkx graph\n",
        "      output: dictionary where each key corresponds to a node and the value is the clustering coefficient of the key\n",
        "      \"\"\"\n",
        "      result = {}\n",
        "\n",
        "      for node in range(nx_graph.number_of_nodes()):\n",
        "      # Iterate over nodes of g\n",
        "        neighbors = list(nx_graph.neighbors(node))\n",
        "        n_neighbors = len(neighbors)\n",
        "        n_links = 0\n",
        "\n",
        "        if n_neighbors > 1:\n",
        "          for node1 in neighbors:\n",
        "            for node2 in neighbors:\n",
        "              if nx_graph.has_edge(node1,node2):\n",
        "                n_links += 1\n",
        "\n",
        "          n_links /= 2 # n_links is calculated twice\n",
        "          result[node] = 2*n_links/(n_neighbors*(n_neighbors-1))\n",
        "\n",
        "        else:\n",
        "          result[node] = 0\n",
        "\n",
        "      return result\n",
        "\n",
        "    def protected_clustering_ratio(nx_graph, protected_set):\n",
        "      \"\"\"\n",
        "      input: networkx graph, set of protected nodes\n",
        "      output: tensor of size (num_nodes) where each value is the number of protected nodes per cluster at each node\n",
        "      \"\"\"\n",
        "      protected = torch.zeros(nx_graph.number_of_nodes())\n",
        "\n",
        "      for node in range(nx_graph.number_of_nodes()):\n",
        "      # Iterate over nodes of g\n",
        "        neighbors = list(nx_graph.neighbors(node))\n",
        "        n_neighbors = len(neighbors)\n",
        "        n_links = 0\n",
        "\n",
        "        if n_neighbors > 1:\n",
        "          for node1 in neighbors:\n",
        "            for node2 in neighbors:\n",
        "              if nx_graph.has_edge(node1,node2):\n",
        "                n_links += 1\n",
        "                protected[node] += sum(el in [node, node1, node2] for el in protected_set)\n",
        "\n",
        "          n_links /= 2 # n_links is calculated twice\n",
        "          protected[node] /= n_links\n",
        "\n",
        "        else:\n",
        "          protected[node] = 0\n",
        "\n",
        "      return protected\n",
        "\n",
        "    def avg_centrality(nx_graph, ind):\n",
        "      \"\"\"\n",
        "      input: networkx graph, tensor of node indices to be averaged over\n",
        "      output: float value average centrality of nodes in the set of indices\n",
        "      \"\"\"\n",
        "      sum = 0\n",
        "      for i in ind:\n",
        "        sum += nx.degree_centrality(nx_graph).get(int(i))\n",
        "      return (sum / len(ind))\n",
        "\n",
        "\n",
        "    def label_count(indices):\n",
        "      \"\"\"\n",
        "      input: 1-dim tensor of node indices\n",
        "      output: dictionary of ground truth labels and the number of occurences of each label in indices\n",
        "      \"\"\"\n",
        "      l = {}\n",
        "      for i in range(graph.labels.max().item()+1):\n",
        "        l[i] = 0\n",
        "      for i in indices:\n",
        "        l[int(graph.labels[i])] += 1\n",
        "      return l\n",
        "\n",
        "\n",
        "    def k_nearest_neighbor_ratio(nx_graph, protected_set, k=3):\n",
        "      \"\"\"\n",
        "      input: networkx graph, 1-dim tensor of protected nodes in the graph, number of k-hops to make for a nodes neighborhood\n",
        "      output: returns tensor of size [num nodes, k] with values equal to avg ratio of protected to all nodes in k-neighborhood for a given node at each value k\n",
        "      \"\"\"\n",
        "\n",
        "      k_neighborhood_ratio = torch.zeros(nx_graph.number_of_nodes(), k)\n",
        "\n",
        "      for node in range(nx_graph.number_of_nodes()):\n",
        "\n",
        "        for i in range(1, k+1):\n",
        "          k_neighbors = nx.single_source_dijkstra_path_length(nx_graph, node, cutoff=i)\n",
        "          protected = int(protected_set[list(k_neighbors.keys())].sum())\n",
        "          total = len(k_neighbors.keys())\n",
        "          k_neighborhood_ratio[node][i-1] = float(protected) / total\n",
        "      return k_neighborhood_ratio\n",
        "\n",
        "    def avg_k_hop_ratio(idx, k, k_neighborhood_ratio):\n",
        "      \"\"\"\n",
        "      input: set of indices, k-hop integer, tensor of size [num_nodes, k] containing k_nearest_neighbor_ratio value at each k-hop value at each node\n",
        "      output: float value of avg k_neighborhood_ratio of all nodes in idx set at specified k-hop\n",
        "      \"\"\"\n",
        "      sum = 0\n",
        "      for i in idx:\n",
        "        sum += k_neighborhood_ratio[i][k-1]\n",
        "      return sum / len(idx)\n",
        "\n",
        "    def avg_random_walk_protected_ratio(nx_graph, protected_set, walk_len=5, n_walks=5):\n",
        "      \"\"\"\n",
        "      input: networkx graph, set of protected nodes, walk_len=integer value length of each walk, n_walks=integer value how many walks to do on each node\n",
        "      output: tensor of shape [num_nodes] where each index is equal to the avg number of protected nodes found in all random walks per the node index\n",
        "      \"\"\"\n",
        "      G = cg.csrgraph(nx_graph, threads=12)\n",
        "      avg_random_walk_protected_ratio = torch.zeros(graph.nx_g.number_of_nodes())\n",
        "\n",
        "      for n in G.nodes():\n",
        "        walks = G.random_walks(walklen=walk_len, # length of the walks\n",
        "                epochs=n_walks, # how many times to start a walk from each node\n",
        "                start_nodes=[n], # the starting node. It is either a list (e.g., [2,3]) or None. If None it does it on all nodes and returns epochs*G.number_of_nodes() walks\n",
        "                return_weight=1., # probability of returning to previous node, higher val -> BFS\n",
        "                neighbor_weight=1.) # probability of visiting new neighbor node -> DFS\n",
        "        avg = []\n",
        "        for w in walks:\n",
        "          protected = protected_set[w.tolist()].sum()\n",
        "          total = len(w)\n",
        "          avg.append(float(protected) / total)\n",
        "          avg_random_walk_protected_ratio[n] = (sum(avg) / len(avg))\n",
        "      return avg_random_walk_protected_ratio\n",
        "\n",
        "\n",
        "    def plot_incorrect_subgraph_k_neighborhood(nx_graph, incorrect_idx, k_hops=3, with_labels=True, protected_set=None):\n",
        "      \"\"\"\n",
        "      input: networkx graph, set of incorrectly predicted node indices, integer value of k-hops in neighborhood, with_labels-[bool] will color nodes in \\\n",
        "        incorrect idx according to set {g0:red, gX:blue}, protected_set is the tensor of protected nodes g0\n",
        "      output: plot of incorrectly predicted nodes and their k-hop neighbors with edges in a networkx subgraph where incorrectly predicted nodes are highlighted\n",
        "      \"\"\"\n",
        "      nodes = []\n",
        "      labeled_nodes = incorrect_idx.tolist()\n",
        "\n",
        "      for node in incorrect_idx.tolist():\n",
        "        path = nx.single_source_dijkstra_path(nx_graph, source=node, cutoff=k_hops).keys()\n",
        "        nodes.append(list(path))\n",
        "\n",
        "      nodes = list(set(([item for sublist in nodes for item in sublist])))\n",
        "      idx = [0.2] * (len(nodes))\n",
        "\n",
        "      for i in range(len(nodes)):\n",
        "        if nodes[i] in incorrect_idx:\n",
        "          idx[i] = 1\n",
        "\n",
        "      sub = nx_graph.subgraph(nodes)\n",
        "\n",
        "      if with_labels == True:\n",
        "        community_map = {}\n",
        "        protected_set = Utils.bool_to_idx(protected_set)\n",
        "        for node in sub.nodes():\n",
        "          if node in labeled_nodes:\n",
        "            if node in protected_set:\n",
        "              community_map[node] = 0\n",
        "            else:\n",
        "              community_map[node] = 1\n",
        "          else:\n",
        "            community_map[node] = -1\n",
        "\n",
        "        node_color = []\n",
        "        color_map = {0: \"red\", 1: \"blue\", -1: \"black\"}\n",
        "        node_color = [color_map[community_map[node]] for node in sub.nodes()]\n",
        "\n",
        "        node_type_map = {0:'g0', 1:'gX', -1:''}\n",
        "        node_types = {node:node_type_map[community_map[node]] for node in sub.nodes()}\n",
        "        nx.set_node_attributes(nx_graph, node_types, 'node_type')\n",
        "\n",
        "        node_label_map = {0:0, 1:1, -1:-1}\n",
        "        node_labels = {node:node_label_map[community_map[node]] for node in sub.nodes()}\n",
        "        nx.set_node_attributes(nx_graph, node_labels, 'node_label')\n",
        "        labels = nx.get_node_attributes(nx_graph, 'node_type')\n",
        "\n",
        "      pos = nx.spring_layout(sub)\n",
        "      plt.figure(figsize=(13, 13))\n",
        "\n",
        "      nodes = nx.draw_networkx_nodes(sub,\n",
        "                                      pos,\n",
        "                                      cmap=plt.get_cmap('coolwarm'),\n",
        "                                      node_color=node_color,\n",
        "                                      node_size=0.5,\n",
        "                                      label=labels,\n",
        "                                      alpha=idx\n",
        "                                    )\n",
        "\n",
        "      edges = nx.draw_networkx_edges(sub, pos, width=0.25, alpha=0.3)\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MohyT9B2I-zo"
      },
      "source": [
        "# Designate protected nodes g0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKOHwv5OXazY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c530de52-2532-4ab2-f028-8974a9f911cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of protected nodes: 246\n",
            "Ratio of protected nodes: 9.08%\n"
          ]
        }
      ],
      "source": [
        "g0 = torch.rand(graph.features.shape[0]) <= args.protect_size\n",
        "g0 = g0.to(device)\n",
        "gX = ~g0.to(device)\n",
        "\n",
        "print(f\"Number of protected nodes: {g0.sum():.0f}\")\n",
        "print(f\"Ratio of protected nodes: {g0.sum() / graph.features.shape[0]:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9mPkQDODqGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19148b51-42a0-41fd-f9b4-5148acf8d787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cora : Nodes=2708 : g0=246 : gx=2462 : train=140 : val=500 : test=1000\n"
          ]
        }
      ],
      "source": [
        "print(f\"{args.dataset} : Nodes={graph.numNodes()} : g0={g0.sum()} : gx={gX.sum()} : train={graph.idx_train.sum()} : val={graph.idx_val.sum()} : test={graph.idx_test.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpYq-kQKJCUc"
      },
      "source": [
        "# Sampling matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eygin-UfYezo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3362ef6-c19d-4a94-a68b-cabdb95ababb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first run\n",
            "tensor([[0.0000e+00, 5.5015e-05, 5.5015e-05,  ..., 5.5015e-05, 5.5015e-05,\n",
            "         5.5015e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 5.5015e-05,  ..., 5.5015e-05, 5.5015e-05,\n",
            "         5.5015e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5015e-05, 5.5015e-05,\n",
            "         5.5015e-05],\n",
            "        ...,\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.5015e-05,\n",
            "         5.5015e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         5.5015e-05],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00]])\n"
          ]
        }
      ],
      "source": [
        "samplingMatrix = SamplingMatrix(g0, gX, graph.adj, args.sample_size)\n",
        "print((samplingMatrix.sampling_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samplingMatrix.get_sample()"
      ],
      "metadata": {
        "id": "tdTQW_dN3D33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556a5621-1c92-4526-cc6c-239d9c158cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2,    7,   14,  ..., 2475, 2519, 2555],\n",
              "        [2365, 1077, 2178,  ..., 2657, 2597, 2640]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samplingMatrix.getRatio()"
      ],
      "metadata": {
        "id": "mfPdGGhn3Qtt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b2e46d-2bca-433e-d31f-55b69977c1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G0:  0.33\n",
            "GX:  0.33\n",
            "G0GX:0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG_c0QTiJGiN"
      },
      "source": [
        "# Surrogate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_BPGMot9SL"
      },
      "outputs": [],
      "source": [
        "surrogate_gcn = GCN_ADJ(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"surrogate_gcn\"\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "surrogate_gcn_cs = GCN_CS(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"surrogate_gcn_cs\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "am7SEtY2U48-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2mlTb-kJL2k"
      },
      "source": [
        "# Generate Perturbations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbi97PEilJ9I"
      },
      "outputs": [],
      "source": [
        "def generate_perturbations(surrogate, graph):\n",
        "  perturbations = torch.zeros_like(graph.adj).float()\n",
        "  count = torch.zeros_like(graph.adj).float()\n",
        "  num_perturbations = args.ptb_rate * graph.adj.sum()\n",
        "\n",
        "  t = tqdm(range(args.ptb_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
        "  t.set_description(\"Perturbing\")\n",
        "\n",
        "  for epoch in t:\n",
        "\n",
        "      # Re-initialize adj_grad\n",
        "      adj_grad = torch.zeros_like(graph.adj).float()\n",
        "\n",
        "      # Get modified adj\n",
        "      modified_adj = Utils.get_modified_adj(graph.adj, perturbations).float().to(device)\n",
        "\n",
        "      if args.do_sampling == 'Y':\n",
        "\n",
        "          for sample_epoch in range(args.num_samples):\n",
        "              # Get sample indices\n",
        "              # sampled = torch.bernoulli(sampling_matrix)\n",
        "\n",
        "              idx = samplingMatrix.get_sample()\n",
        "\n",
        "              # Map sample to adj\n",
        "              sample = modified_adj[idx[0], idx[1]].clone().detach().requires_grad_(True)\n",
        "              modified_adj[idx[0], idx[1]] = sample\n",
        "\n",
        "              # Get grad\n",
        "              predictions = surrogate.predict(graph.features, modified_adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "\n",
        "              loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
        "                  - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
        "\n",
        "              grad = torch.autograd.grad(loss, sample)[0]\n",
        "\n",
        "              if sample_epoch == 0:\n",
        "                print(grad)\n",
        "              # Implement averaging\n",
        "              adj_grad[idx[0], idx[1]] += grad\n",
        "              count[idx[0], idx[1]] += 1\n",
        "\n",
        "              # Update the sampling matrix\n",
        "              samplingMatrix.updateByGrad(adj_grad, count)\n",
        "              #samplingMatrix.getRatio()\n",
        "\n",
        "              # Average the gradient\n",
        "              adj_grad = torch.div(adj_grad, count)\n",
        "              adj_grad[adj_grad != adj_grad] = 0\n",
        "\n",
        "\n",
        "      else:\n",
        "          # Get grad\n",
        "          modified_adj = modified_adj.clone().detach().requires_grad_(True).to(device)\n",
        "          predictions = surrogate.predict(graph.features, modified_adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "\n",
        "          loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
        "              - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
        "\n",
        "          adj_grad = torch.autograd.grad(loss, modified_adj)[0]\n",
        "\n",
        "      # Update perturbations\n",
        "      lr = (num_perturbations) / (epoch + 1)\n",
        "      pre_projection = int(perturbations.sum() / 2)\n",
        "      perturbations = perturbations + (lr * adj_grad)\n",
        "      perturbations = Utils.projection(perturbations, num_perturbations)\n",
        "\n",
        "\n",
        "      # Train the model\n",
        "      modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
        "      surrogate.train1epoch(graph.features, modified_adj, graph.labels, graph.idx_train, graph.idx_test)\n",
        "      t.set_postfix({\"adj_l\": loss.item(),\n",
        "                      \"adj_g\": int(adj_grad.sum()),\n",
        "                      \"pre-p\": pre_projection,\n",
        "                      \"target\": int(num_perturbations / 2),\n",
        "                      \"loss\": loss})\n",
        "\n",
        "\n",
        "  return perturbations\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perturbations_gcn = generate_perturbations(surrogate_gcn, graph)\n",
        "\n",
        "#perturbations_gcn_cs = generate_perturbations(surrogate_gcn_cs, graph)"
      ],
      "metadata": {
        "id": "3ZM_Jy5XWlmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reset parameters (gradient --)"
      ],
      "metadata": {
        "id": "d6AC28vGzGaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: generate_perturbations on (surrogate_gcn_cs, graph) is taking a significantly longer time. Investigation is required."
      ],
      "metadata": {
        "id": "cNB4Un4Nrzv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#perturbations_gcn_cs = generate_perturbations(surrogate_gcn_cs, graph)\n",
        "\n",
        "# runs when replacing nan with 0 but best sample output is off and very slow"
      ],
      "metadata": {
        "id": "UiYSlIksWr2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6gJc7v9JViX"
      },
      "source": [
        "# Get best perturbation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCou-onpf0JE"
      },
      "outputs": [],
      "source": [
        "def get_best_perturbation(perturbations, graph, surrogate):\n",
        "  with torch.no_grad():\n",
        "\n",
        "      max_loss = -1000\n",
        "\n",
        "      for k in range(0,3):\n",
        "          sample = torch.bernoulli(perturbations)\n",
        "          modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
        "          modified_adj = Utils.make_symmetric(modified_adj) # Removing this creates \"impossible\" adj, but works well\n",
        "\n",
        "          predictions = surrogate.predict(graph.features, modified_adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "\n",
        "          loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
        "              - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
        "\n",
        "          if loss > max_loss:\n",
        "              max_loss = loss\n",
        "              best = sample\n",
        "\n",
        "      print(f\"Best sample loss: {max_loss:.2f}\\t Edges: {best.abs().sum() / 2:.0f}\")\n",
        "  return best"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best_gcn_perturbation = get_best_perturbation(perturbations_gcn, graph, surrogate_gcn)"
      ],
      "metadata": {
        "id": "2c6zgEXIXmpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best_gcn_cs_perturbation = get_best_perturbation(perturbations_gcn_cs, graph, surrogate_gcn_cs)\n",
        "\n",
        "# Best sample loss: 0.02\t Edges: 232"
      ],
      "metadata": {
        "id": "hbBPAF3XXqm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Edge Changes"
      ],
      "metadata": {
        "id": "aYgaJsOzhegS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "locked_adj_gcn = Utils.get_modified_adj(graph.adj, best_gcn_perturbation)\n",
        "\n",
        "edge_changes = abs(graph.adj - locked_adj_gcn)\n",
        "num_edge_changes = edge_changes.sum()\n",
        "\n",
        "print(f\"       Dataset: {args.dataset}\")\n",
        "print(f\"         Nodes: {graph.numNodes()}\")\n",
        "print(f\"Original edges: {int(graph.numEdges())}\")\n",
        "print(f\"  Edge changes: {int(num_edge_changes)}\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G8EBllVVhfGI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a38c55cf-5021-4b4f-9a1c-bf456ba3120f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlocked_adj_gcn = Utils.get_modified_adj(graph.adj, best_gcn_perturbation)\\n\\nedge_changes = abs(graph.adj - locked_adj_gcn)\\nnum_edge_changes = edge_changes.sum()\\n\\nprint(f\"       Dataset: {args.dataset}\")\\nprint(f\"         Nodes: {graph.numNodes()}\")\\nprint(f\"Original edges: {int(graph.numEdges())}\")\\nprint(f\"  Edge changes: {int(num_edge_changes)}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNNExplainer Analysis"
      ],
      "metadata": {
        "id": "xzVbdSl6fcIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, features, adj, labels, idx_train, idx_val, idx_test, name=\"\"):\n",
        "  model.fit(features, adj, labels, idx_train, idx_test, args.reg_epochs)\n",
        "\n",
        "  model_pred = model.predict(features, adj, labels, idx_train, idx_val, idx_test)\n",
        "  model_acc = Metrics.partial_acc(model_pred, labels, g0, gX)\n",
        "\n",
        "  print('{} Acc:'.format(name))\n",
        "  print(model_acc)\n",
        "\n",
        "  return model_pred, model_acc"
      ],
      "metadata": {
        "id": "HtpjF4p4Lma8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"baseline\"\n",
        ").to(device)\n",
        "baseline_pred, baseline_acc = evaluation(baseline, graph.features, graph.edge_index, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"baseline\")"
      ],
      "metadata": {
        "id": "3lMq11RiLMZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4fbf7dd-2152-4804-a8fd-5fcf6dde69d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training baseline: 100%|██████████| 100/100 [00:02<00:00, 36.73it/s, loss=0.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G0: 79.27%\n",
            "GX: 78.68%\n",
            "baseline Acc:\n",
            "{'g0': 0.792682945728302, 'gX': 0.7867587208747864}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "locked_edge_index = (locked_adj_gcn.nonzero().t().contiguous())"
      ],
      "metadata": {
        "id": "-xpDJoS9o3eU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "595ff349-e208-40a7-82cd-f4f7ecca3270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'locked_adj_gcn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-05c14f0599a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlocked_edge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlocked_adj_gcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'locked_adj_gcn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "locked_adj_gcn = Utils.get_modified_adj(graph.adj, best_gcn_perturbation)\n",
        "locked_gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=\"locked_gcn\"\n",
        ").to(device)\n",
        "locked_gcn_pred, locked_gcn_acc = evaluation(locked_gcn, graph.features, locked_edge_index, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"locked_gcn\")"
      ],
      "metadata": {
        "id": "RU8kZY6ULZqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#diff = locked_adj_gcn - graph.adj\n",
        "#Metrics.show_metrics(diff, graph.labels, g0, graph.device)"
      ],
      "metadata": {
        "id": "ztb9ak6oM0Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = Explainer(\n",
        "    model=gcn,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='node',\n",
        "        return_type='log_probs',\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "QkJ7sLdzfckR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanation = explainer(graph.features, graph.edge_index)"
      ],
      "metadata": {
        "id": "Zmxl-0DXgsV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "bLD7q66eRPd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Edge Target Distance Analysis"
      ],
      "metadata": {
        "id": "P_Jw3VzjRWqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For each of the dataset\n",
        "  For each private node x, if an edge (x,y) is added that is incident to the private node, find out the shortest distance from x to y in the original graph.\n",
        "\n",
        "Tabulate the results.\n",
        "(Only for original SLL)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4LbuSHy8RcUc",
        "outputId": "7f88ec93-eb6b-4bfc-c7aa-81fed23dd1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFor each of the dataset\\n  For each private node x, if an edge (x,y) is added that is incident to the private node, find out the shortest distance from x to y in the original graph.\\n\\nTabulate the results.\\n(Only for original SLL)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_new_edge_distance_data():\n",
        "    d = {'dataset': [], 'source':[], 'target':[], 'distance':[]}\n",
        "    df = pd.DataFrame(data=d)\n",
        "\n",
        "    datasets = ['PolBlogs','cora','CiteSeer', 'BlogCatalog', 'flickr']\n",
        "\n",
        "    for dataset_name in datasets:\n",
        "        graph = getGraph(root='.', name=dataset_name, setting='gcn', seed=args.seed, device=device)\n",
        "\n",
        "        graph.edge_index = graph.adj.nonzero().t().contiguous()\n",
        "        graph.num_nodes = graph.numNodes() #graph.features.shape[0]\n",
        "        graph.num_features = graph.features.shape[1]\n",
        "        graph.num_edges = graph.numEdges() #graph.edge_index.shape[1]\n",
        "        graph.device = device\n",
        "        graph.adj = to_sym_adj(graph.edge_index, graph.num_nodes).to(device)\n",
        "\n",
        "        data = Data(x=graph.features, y=graph.labels, edge_index=graph.edge_index)\n",
        "\n",
        "        nxg = nx.Graph()\n",
        "        nxg = to_networkx(data, to_undirected=True)\n",
        "        graph.nx_g = nxg\n",
        "\n",
        "        graph.clustering = nx.clustering(nxg)\n",
        "        graph.centrality = nx.degree_centrality(nxg)\n",
        "        graph.degree = nxg.degree\n",
        "\n",
        "        surrogate_gcn = GCN_ADJ(\n",
        "            dim_input_features= graph.num_features,\n",
        "            dim_output_classes= graph.labels.max().item()+1,\n",
        "            num_hidden_layers=args.hidden_layers,\n",
        "            device=device,\n",
        "            lr=args.model_lr,\n",
        "            dropout=args.dropout,\n",
        "            weight_decay=args.weight_decay,\n",
        "            name=f\"surrogate_gcn\").to(device)\n",
        "\n",
        "        g0 = torch.rand(graph.features.shape[0]) <= args.protect_size\n",
        "        g0 = g0.to(device)\n",
        "        gX = ~g0.to(device)\n",
        "\n",
        "        samplingMatrix = SamplingMatrix(g0, gX, graph.adj, args.sample_size)\n",
        "\n",
        "        perturbations_gcn = generate_perturbations(surrogate_gcn, graph)\n",
        "        best_gcn_perturbation = get_best_perturbation(perturbations_gcn, graph, surrogate_gcn)\n",
        "\n",
        "        locked_adj_gcn = Utils.get_modified_adj(graph.adj, best_gcn_perturbation)\n",
        "\n",
        "        protected_indices = list(Utils.bool_to_idx(g0).flatten())\n",
        "\n",
        "        new_edges_adj = locked_adj_gcn - graph.adj\n",
        "        print(f\"Shortest Distance in Original Graph:\")\n",
        "        for node in protected_indices:\n",
        "            new_edges = list(Utils.bool_to_idx((locked_adj_gcn - graph.adj)[node] == torch.tensor(1)).flatten())\n",
        "            for idx in new_edges:\n",
        "                df_index = len(df)\n",
        "                try:\n",
        "                  shortest_dist = nx.shortest_path_length(graph.nx_g, source=int(node), target=int(idx))\n",
        "                  #print(f\"\\t{node} : {idx} - {shortest_dist}\")\n",
        "                  df.loc[df_index,'dataset'] = dataset_name\n",
        "                  df.loc[df_index,'source'] = int(node)\n",
        "                  df.loc[df_index,'target'] = int(idx)\n",
        "                  df.loc[df_index,'distance'] = int(shortest_dist)\n",
        "                except:\n",
        "                  #print(f\"\\t{node} : {idx} - {-1}\")\n",
        "                  df.loc[df_index,'dataset'] = dataset_name\n",
        "                  df.loc[df_index,'source'] = int(node)\n",
        "                  df.loc[df_index,'target'] = int(idx)\n",
        "                  df.loc[df_index,'distance'] = int(-1)\n",
        "\n",
        "\n",
        "    file_name = dataset_name + '_new_edge_target_distance.csv'\n",
        "    df.to_csv(file_name)\n",
        "    from google.colab import files\n",
        "    files.download(file_name)"
      ],
      "metadata": {
        "id": "0eEEQ6GjRqyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data analysis\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/all_datasets_new_edge_target_distance.csv\").drop(columns=['Unnamed: 0'])\n",
        "graph_analysis_df = pd.read_csv('/content/drive/MyDrive/graph_analysis.csv')"
      ],
      "metadata": {
        "id": "1DydVojJ6E2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['PolBlogs','cora','CiteSeer', 'BlogCatalog', 'flickr']"
      ],
      "metadata": {
        "id": "QyCVKVotB8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_analysis_df['2-hop_pct'] = 0\n",
        "graph_analysis_df['3-hop_pct'] = 0\n",
        "\n",
        "for dataset in datasets:\n",
        "    temp = df[df['dataset']==dataset]\n",
        "\n",
        "    two_hop   = temp['distance'] <= 2\n",
        "    three_hop = temp['distance'] <= 3\n",
        "\n",
        "    print(dataset)\n",
        "    print(f\"Percentage nodes:\")\n",
        "    print(f\"  2-hop neighborhood: {sum(two_hop) / len(temp)}\")\n",
        "    print(f\"  3-hop neighborhood: {sum(three_hop) / len(temp)}\\n\")\n",
        "\n",
        "    idx = graph_analysis_df[graph_analysis_df['dataset']==dataset].index\n",
        "\n",
        "    graph_analysis_df.loc[idx, '2-hop_pct'] = sum(two_hop) / len(temp)\n",
        "    graph_analysis_df.loc[idx, '3-hop_pct'] = sum(three_hop) / len(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyLqscSR4CPw",
        "outputId": "7cd38b96-1787-4fc7-9dc6-8ed5a8b81c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PolBlogs\n",
            "Percentage nodes:\n",
            "  2-hop neighborhood: 0.6120238984316654\n",
            "  3-hop neighborhood: 0.8592233009708737\n",
            "\n",
            "cora\n",
            "Percentage nodes:\n",
            "  2-hop neighborhood: 0.21621621621621623\n",
            "  3-hop neighborhood: 0.23216659282233051\n",
            "\n",
            "CiteSeer\n",
            "Percentage nodes:\n",
            "  2-hop neighborhood: 0.6795851528384279\n",
            "  3-hop neighborhood: 0.6812227074235808\n",
            "\n",
            "BlogCatalog\n",
            "Percentage nodes:\n",
            "  2-hop neighborhood: 0.3825284017222597\n",
            "  3-hop neighborhood: 0.9927374591482077\n",
            "\n",
            "flickr\n",
            "Percentage nodes:\n",
            "  2-hop neighborhood: 0.4034410311736983\n",
            "  3-hop neighborhood: 0.9967066038271535\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_analysis_df.to_csv('graph_analysis.csv')"
      ],
      "metadata": {
        "id": "IOWcuzqX6R0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "connected_nodes_df    = df[df['distance']!=-1.0]\n",
        "disconnected_nodes_df = df[df['distance']==-1.0]\n",
        "\n",
        "connected_node_ratio    = len(connected_nodes_df)    / len(df)\n",
        "disconnected_node_ratio = len(disconnected_nodes_df) / len(df)"
      ],
      "metadata": {
        "id": "iCDBhIOn5LEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shortest_path_by_dataset():\n",
        "    print(f\"Average Shortest Path Distance by Dataset:\\n\")\n",
        "\n",
        "    avg_node_distance_by_dataset = {}\n",
        "\n",
        "    for dataset in datasets:\n",
        "        if dataset != 'all':\n",
        "          dataset_df = connected_nodes_df[connected_nodes_df['dataset']==dataset]\n",
        "        else:\n",
        "          dataset_df = connected_nodes_df\n",
        "\n",
        "        avg_dist   = dataset_df['distance'].sum() / len(dataset_df)\n",
        "        avg_node_distance_by_dataset[dataset] = avg_dist\n",
        "        print(f\"{dataset}: {avg_dist}\")\n",
        "\n",
        "    return avg_node_distance_by_dataset"
      ],
      "metadata": {
        "id": "yWLrloYl6qG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_analysis():\n",
        "    d = {'dataset': [], 'avg_clustering_coef':[], 'avg_centrality':[], 'avg_degree':[]}\n",
        "    graph_analysis_df = pd.DataFrame(data=d)\n",
        "\n",
        "    for i in range(len(datasets)):\n",
        "        dataset = datasets[i]\n",
        "        if dataset != 'all':\n",
        "            graph = getGraph(root='.', name=dataset, setting='gcn', seed=args.seed, device=device)\n",
        "            graph.edge_index = graph.adj.nonzero().t().contiguous()\n",
        "\n",
        "            data = Data(x=graph.features, y=graph.labels, edge_index=graph.edge_index)\n",
        "\n",
        "            nxg = nx.Graph()\n",
        "            nxg = to_networkx(data, to_undirected=True)\n",
        "\n",
        "            indices = [node for node in nxg.nodes]\n",
        "\n",
        "            clustering = nx.clustering(nxg, nodes=list(int(i) for i in indices)).values()\n",
        "            centrality = nx.closeness_centrality(nxg)\n",
        "            close_cent = [centrality[i] for i in indices]\n",
        "            degree     = [val for (node, val) in nxg.degree(nbunch=indices)]\n",
        "\n",
        "            print(f\"Average Graph Attributes for {dataset}\")\n",
        "            print(f\"  clustering: {sum(clustering) / len(indices)}\")\n",
        "            print(f\"  centrality: {sum(close_cent) / len(indices)}\")\n",
        "            print(f\"      degree: {sum(degree)     / len(indices)}\\n\")\n",
        "\n",
        "            graph_analysis_df.loc[i,'dataset']             = dataset\n",
        "            graph_analysis_df.loc[i,'avg_clustering_coef'] = sum(clustering) / len(indices)\n",
        "            graph_analysis_df.loc[i,'avg_centrality']      = sum(close_cent) / len(indices)\n",
        "            graph_analysis_df.loc[i,'avg_degree']          = sum(degree)     / len(indices)\n",
        "\n",
        "    return graph_analysis_df"
      ],
      "metadata": {
        "id": "6AZWaI2uBn-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_node_distance_by_dataset = shortest_path_by_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CewDIZZyCc36",
        "outputId": "342e660b-6b3d-407c-b6d6-a6a97f62c8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Shortest Path Distance by Dataset:\n",
            "\n",
            "PolBlogs: 3.1930812550281575\n",
            "cora: 6.862380146644106\n",
            "CiteSeer: 10.256366723259763\n",
            "BlogCatalog: 2.6247341391295325\n",
            "flickr: 2.599852364999148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_indices = [int(i) for i in Utils.bool_to_idx(g0 + gX).flatten()]\n",
        "\n",
        "#graph_analysis_df = graph_analysis()"
      ],
      "metadata": {
        "id": "aF6T3CFxfzQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_analysis_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QFgEhMYu3yEj",
        "outputId": "3baa890a-b977-466c-8633-c53cc38a29ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       dataset  avg_clustering_coef  avg_centrality  avg_degree  \\\n",
              "0     PolBlogs             0.262652        0.250429   22.436242   \n",
              "1         cora             0.240673        0.137476    3.898080   \n",
              "2     CiteSeer             0.142555        0.045350    2.739130   \n",
              "3  BlogCatalog             0.122372        0.400284   66.105851   \n",
              "4       flickr             0.330117        0.418222   63.297162   \n",
              "\n",
              "   avg_shortest_path_to_new_edge  2-hop_pct  3-hop_pct  \n",
              "0                       3.193081   0.612024   0.859223  \n",
              "1                       6.862380   0.216216   0.232167  \n",
              "2                      10.256367   0.679585   0.681223  \n",
              "3                       2.624734   0.382528   0.992737  \n",
              "4                       2.599852   0.403441   0.996707  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d031212-5ad7-42aa-b20c-fa4935d18d2a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dataset</th>\n",
              "      <th>avg_clustering_coef</th>\n",
              "      <th>avg_centrality</th>\n",
              "      <th>avg_degree</th>\n",
              "      <th>avg_shortest_path_to_new_edge</th>\n",
              "      <th>2-hop_pct</th>\n",
              "      <th>3-hop_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PolBlogs</td>\n",
              "      <td>0.262652</td>\n",
              "      <td>0.250429</td>\n",
              "      <td>22.436242</td>\n",
              "      <td>3.193081</td>\n",
              "      <td>0.612024</td>\n",
              "      <td>0.859223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cora</td>\n",
              "      <td>0.240673</td>\n",
              "      <td>0.137476</td>\n",
              "      <td>3.898080</td>\n",
              "      <td>6.862380</td>\n",
              "      <td>0.216216</td>\n",
              "      <td>0.232167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CiteSeer</td>\n",
              "      <td>0.142555</td>\n",
              "      <td>0.045350</td>\n",
              "      <td>2.739130</td>\n",
              "      <td>10.256367</td>\n",
              "      <td>0.679585</td>\n",
              "      <td>0.681223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BlogCatalog</td>\n",
              "      <td>0.122372</td>\n",
              "      <td>0.400284</td>\n",
              "      <td>66.105851</td>\n",
              "      <td>2.624734</td>\n",
              "      <td>0.382528</td>\n",
              "      <td>0.992737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>flickr</td>\n",
              "      <td>0.330117</td>\n",
              "      <td>0.418222</td>\n",
              "      <td>63.297162</td>\n",
              "      <td>2.599852</td>\n",
              "      <td>0.403441</td>\n",
              "      <td>0.996707</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d031212-5ad7-42aa-b20c-fa4935d18d2a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d031212-5ad7-42aa-b20c-fa4935d18d2a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d031212-5ad7-42aa-b20c-fa4935d18d2a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72282627-89a7-45db-8209-0abae47fa263\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72282627-89a7-45db-8209-0abae47fa263')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72282627-89a7-45db-8209-0abae47fa263 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e08ddb4a-3b60-4478-8775-f001ddb8b18d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('graph_analysis_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e08ddb4a-3b60-4478-8775-f001ddb8b18d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('graph_analysis_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "graph_analysis_df",
              "summary": "{\n  \"name\": \"graph_analysis_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"dataset\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"cora\",\n          \"flickr\",\n          \"CiteSeer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_clustering_coef\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08645929251905404,\n        \"min\": 0.1223715862330581,\n        \"max\": 0.3301168019637329,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2406732985019374,\n          0.3301168019637329,\n          0.142554578784099\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_centrality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16234732412927722,\n        \"min\": 0.0453500579972466,\n        \"max\": 0.4182216502374666,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.13747602791603,\n          0.4182216502374666,\n          0.0453500579972466\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_degree\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31.143374220193774,\n        \"min\": 2.739130434782609,\n        \"max\": 66.1058506543495,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.89807976366322,\n          63.297161716171615,\n          2.739130434782609\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_shortest_path_to_new_edge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3803792220585316,\n        \"min\": 2.599852364999148,\n        \"max\": 10.256366723259765,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          6.862380146644106,\n          2.599852364999148,\n          10.256366723259765\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2-hop_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.187051131126698,\n        \"min\": 0.21621621621621623,\n        \"max\": 0.6795851528384279,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.21621621621621623,\n          0.4034410311736983,\n          0.6795851528384279\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3-hop_pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3180283135405234,\n        \"min\": 0.23216659282233051,\n        \"max\": 0.9967066038271535,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.23216659282233051,\n          0.9967066038271535,\n          0.6812227074235808\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "graph_analysis_df['avg_shortest_path_to_new_edge'] = 0\n",
        "\n",
        "for u,v in avg_node_distance_by_dataset.items():\n",
        "    idx = graph_analysis_df[graph_analysis_df['dataset']==u].index[0]\n",
        "    graph_analysis_df.loc[idx,'avg_shortest_path_to_new_edge'] = v\n",
        "\n",
        "\n",
        "graph_analysis_df = graph_analysis_df[:5]\n",
        "graph_analysis_df.to_csv('graph_analysis.csv', index=False)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V6JMt9PZ3RPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Locked Graph Analysis"
      ],
      "metadata": {
        "id": "wdZHpyhAg8Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For each dataset in the original SLL paper, for x in range(10, 50, 10)\n",
        "    Randomly select x% of the private nodes, and restore its 1 neighborhood edges\n",
        "    Run GCN and GCN-CS on the partially corrected graph\n",
        "    Tabulate the results to indicate the performance before and after the partial correction.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "L8B-Iw1FhCsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"baseline_gcn\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "rwwydJPO6XC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=\"locked_gcn\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "WtxRs7pG6cXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN\n",
        "locked_adj_gcn = Utils.get_modified_adj(graph.adj, best_gcn_perturbation)\n",
        "partial_locked_adj = torch.zeros_like(locked_adj_gcn).copy_(locked_adj_gcn)\n",
        "\n",
        "protected_indices = list(Utils.bool_to_idx(g0).flatten())\n",
        "\n",
        "for x in range(10, 51, 10):\n",
        "  split = round(int(g0.sum()) * (x/100))\n",
        "  partial_locked_adj = torch.zeros_like(locked_adj_gcn).copy_(locked_adj_gcn)\n",
        "\n",
        "  random.shuffle(protected_indices)\n",
        "  for idx in protected_indices[:split]:\n",
        "      neighbors = nxg.neighbors(int(idx))\n",
        "      partial_locked_adj[idx] = graph.adj[idx]\n",
        "      partial_locked_adj[list(neighbors), idx] = 1\n",
        "\n",
        "  partial_locked_edge_index = (partial_locked_adj.nonzero().t().contiguous())\n",
        "\n",
        "  baseline_gcn_pred, baseline_gcn_acc = evaluation(baseline_gcn, graph.features, graph.edge_index, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"baseline_gcn\")\n",
        "  partial_locked_gcn_pred, partial_locked_gcn_acc = evaluation(locked_gcn, graph.features, partial_locked_edge_index, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"locked_gcn\")\n",
        "\n",
        "  print()\n",
        "  print(f\"-- GCN --\")\n",
        "  print(f\"  {x}% Partially Corrected\")\n",
        "  print(f\"    Baseline: {baseline_gcn_acc}\")\n",
        "  print(f\"    Locked  : {partial_locked_gcn_acc}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "GvzAJQcihNRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_here"
      ],
      "metadata": {
        "id": "7_txTYRq0qp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_gcn_cs = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"baseline_gcn_cs\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "rNvPhDVQGG86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_gcn_cs = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=\"locked_gcn_cs\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "61__GMWFGVSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perturbations_gcn_cs = generate_perturbations(surrogate_gcn_cs, graph)\n",
        "\n",
        "best_gcn_cs_perturbation = get_best_perturbation(perturbations_gcn_cs, graph, surrogate_gcn_cs)"
      ],
      "metadata": {
        "id": "CzftpOzL2SxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN_CS\n",
        "locked_adj_gcn_cs = Utils.get_modified_adj(graph.adj, best_gcn_cs_perturbation)\n",
        "partial_locked_adj = torch.zeros_like(locked_adj_gcn_cs).copy_(locked_adj_gcn_cs)\n",
        "\n",
        "protected_indices = list(Utils.bool_to_idx(g0).flatten())\n",
        "\n",
        "for x in range(10, 51, 10):\n",
        "  split = round(int(g0.sum()) * (x/100))\n",
        "  partial_locked_adj = torch.zeros_like(locked_adj_gcn_cs).copy_(locked_adj_gcn_cs)\n",
        "\n",
        "  random.shuffle(protected_indices)\n",
        "  for idx in protected_indices[:split]:\n",
        "      neighbors = nxg.neighbors(int(idx))\n",
        "      partial_locked_adj[idx] = graph.adj[idx]\n",
        "      partial_locked_adj[list(neighbors), idx] = 1\n",
        "\n",
        "  partial_locked_edge_index = (partial_locked_adj.nonzero().t().contiguous())\n",
        "\n",
        "  baseline_gcn_cs_pred, baseline_gcn_cs_acc = evaluation(baseline_gcn_cs, graph.features, graph.edge_index, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"baseline_gcn_cs\")\n",
        "  partial_locked_gcn_cs_pred, partial_locked_gcn_cs_acc = evaluation(locked_gcn_cs, graph.features, partial_locked_edge_index, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"locked_gcn_cs\")\n",
        "  print()\n",
        "  print(f\"-- GCN_CS --\")\n",
        "  print(f\"  {x}% Partially Corrected\")\n",
        "  print(f\"    Baseline: {baseline_gcn_cs_acc}\")\n",
        "  print(f\"    Locked  : {partial_locked_gcn_cs_acc}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "4UWcTQF8FcpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop here"
      ],
      "metadata": {
        "id": "v1QDsCLqhpW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Graph"
      ],
      "metadata": {
        "id": "uY93kOvOlvqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build cluster dictionary #\n",
        "\n",
        "cluster_dict = {}\n",
        "available_nodes = [i for i in nxg.nodes]\n",
        "i = 0\n",
        "\n",
        "while len(available_nodes) > 0:\n",
        "  cluster = f\"cluster{i}\"\n",
        "  i += 1\n",
        "  node = available_nodes[0]\n",
        "  node_neighborhood = list(nx.single_source_shortest_path_length(nxg, node, cutoff=2).keys())\n",
        "  cluster_dict[cluster] = node_neighborhood\n",
        "  available_nodes = list(set(available_nodes) - set(node_neighborhood))"
      ],
      "metadata": {
        "id": "R_7Zl3xIruHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complement Graph"
      ],
      "metadata": {
        "id": "e_IWWvTNI-Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comp_explainer = Explainer(\n",
        "    model=gcn,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='node',\n",
        "        return_type='log_probs',\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "vPmu5gHsPfwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_path = nx.single_source_shortest_path_length(nxg, 0, cutoff=2)\n"
      ],
      "metadata": {
        "id": "xXiIFmpXhDQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = torch.zeros_like(graph.adj)\n",
        "for node in nxg.nodes():\n",
        "    n_path = nx.single_source_shortest_path_length(nxg, node, cutoff=2)\n",
        "    temp[node, list(n_path.keys())] = 1.\n",
        "    temp[node,node] = 0.\n",
        "temp = temp - graph.adj"
      ],
      "metadata": {
        "id": "o-irHmG7jMMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_two_hop_comp_adj(nxg):\n",
        "  \"\"\"\n",
        "  build 2-hop neighbor adj matrix\n",
        "  \"\"\"\n",
        "  comp_adj = torch.zeros_like(graph.adj)\n",
        "  for node in nxg.nodes():\n",
        "    n_path = nx.single_source_shortest_path_length(nxg, node, cutoff=2)\n",
        "    two_hop_nh = list(set(n_path.keys()) - set(list(nxg.neighbors(0))))\n",
        "    comp_adj[node, two_hop_nh] = 1.\n",
        "    comp_adj[node, node] = 0.\n",
        "  return comp_adj"
      ],
      "metadata": {
        "id": "fNR_CrMmbQ-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comp_adj = build_two_hop_comp_adj(nxg)\n",
        "comp_edge_index = comp_adj.nonzero().t().contiguous()"
      ],
      "metadata": {
        "id": "0a7-hlMAgbUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comp_explanation = comp_explainer(graph.features, comp_edge_index)"
      ],
      "metadata": {
        "id": "j5xvaAP9JLvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ranked_edge_weight = {}\n",
        "\n",
        "for idx in range((comp_edge_index).shape[1]):\n",
        "  edge_weight = comp_explanation.edge_mask[idx]\n",
        "  pair = comp_explanation.edge_index[:,idx]\n",
        "  ranked_edge_weight[pair] = edge_weight\n",
        "\n",
        "ranked_edge_weight = dict(sorted(ranked_edge_weight.items(), key=lambda x:x[1], reverse=True))\n"
      ],
      "metadata": {
        "id": "7E4_rR2hZnW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"baseline_gcn\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "tkz_0AIfdHr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_gcn.fit(graph.features, comp_edge_index, graph.labels, graph.idx_train, graph.idx_test, args.reg_epochs)"
      ],
      "metadata": {
        "id": "T-h1-rDZhpvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_pred = baseline_gcn(graph.features, comp_edge_index)"
      ],
      "metadata": {
        "id": "pFHZim6Yh9aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Metrics.partial_acc(base_pred, graph.labels, g0, gX)"
      ],
      "metadata": {
        "id": "siI16WyGiVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"baseline_gcn\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "6SnxBfFJinA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_edge_index = locked_adj_gcn.nonzero().t().contiguous()"
      ],
      "metadata": {
        "id": "kINY6Acpmy24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_gcn.fit(graph.features, locked_edge_index, graph.labels, graph.idx_train, graph.idx_test, args.reg_epochs)"
      ],
      "metadata": {
        "id": "CE8V9JNcit_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_pred = locked_gcn(graph.features, comp_edge_index)\n",
        "Metrics.partial_acc(locked_pred, graph.labels, g0, gX)"
      ],
      "metadata": {
        "id": "WIvwEWhpm9CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Weight Matrix"
      ],
      "metadata": {
        "id": "v3VejUPQHLZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.explain import Explainer, GNNExplainer"
      ],
      "metadata": {
        "id": "Q4Y6YwqLmjm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wm_gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"wm_gcn\"\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "REJsNq4233t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = Explainer(\n",
        "    model=wm_gcn,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='node',\n",
        "        return_type='log_probs',\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "wd8pxm96-mcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt_adj = Utils.get_modified_adj(graph.adj, best_gcn_perturbation)\n",
        "alt_edge_index = to_edge_index(alt_adj)"
      ],
      "metadata": {
        "id": "wwirooV_23yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(graph.adj != alt_adj).sum()"
      ],
      "metadata": {
        "id": "NK0OFcOOr2n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanation = explainer(graph.features, graph.edge_index)"
      ],
      "metadata": {
        "id": "GYNw0f75m6l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_matrix = torch.zeros_like(graph.adj)"
      ],
      "metadata": {
        "id": "eRA-JDeOHMRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(graph.edge_index.shape[1]):\n",
        "  u = explanation.edge_index[0, i]\n",
        "  v = explanation.edge_index[1, i]\n",
        "  weight_matrix[u,v] = explanation.edge_mask[i]\n",
        "  weight_matrix[v,u] = explanation.edge_mask[i]"
      ],
      "metadata": {
        "id": "HtVaMCLCyWBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weight Matrix Predictions"
      ],
      "metadata": {
        "id": "BPKop7avszQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to sparse matrices\n",
        "from scipy import sparse\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "scp_weight_matrix = sparse.csr_matrix(weight_matrix)\n",
        "scp_alt_adj = sparse.csr_matrix(alt_adj)"
      ],
      "metadata": {
        "id": "Ecx-8V2infaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_weight_matrix = weight_matrix.to_sparse()\n",
        "sp_alt_adj = alt_adj.to_sparse()"
      ],
      "metadata": {
        "id": "xNcyrkfqhiez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt_edge_idx = to_edge_index(alt_adj)\n",
        "alt_edge_idx"
      ],
      "metadata": {
        "id": "P_lUB33Da82X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(weight_matrix, alt_adj, test_size = 0.20, random_state = 42)"
      ],
      "metadata": {
        "id": "PVnzRSmp2Byv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_x_train = x_train.to_sparse()\n",
        "sp_y_train = y_train.to_sparse()"
      ],
      "metadata": {
        "id": "W4_a6K6skcm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Model\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler()\n",
        "#x_under y_under = rus.fit_resample(x_train, y_train)"
      ],
      "metadata": {
        "id": "vNm9rBKHplOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Net\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def fit(self, input_features, target_adjacency_matrix, num_epochs=1000):\n",
        "        for epoch in range(num_epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            predicted_adjacency_matrix = self(input_features)\n",
        "\n",
        "            flat_predicted = predicted_adjacency_matrix.view(-1)\n",
        "            flat_target = target_adjacency_matrix.view(-1)\n",
        "\n",
        "            loss = self.criterion(flat_predicted, flat_target)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "        print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "bWfrTDiKrQPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape[0]"
      ],
      "metadata": {
        "id": "_ZRn9VnqlE9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gnn = GNN(x_train.shape[0], 64, y_train.shape[1])\n",
        "#gnn.fit(x_train, y_train)\n",
        "\n",
        "#gnn_pred = gnn(x_test, y_test)"
      ],
      "metadata": {
        "id": "b7k4t6Gdt-nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)"
      ],
      "metadata": {
        "id": "qYBUl8aKncUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\"\"\"\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "rf_pred = rf.predict(x_test)\n",
        "((torch.tensor(rf_pred) == 1) and (torch.tensor(rf_pred) == y_test)).sum() / (y_test == 1).sum()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ssjFksZ91sfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#print(f'Model accuracy score : {accuracy_score(y_test, y_pred):.2%}')"
      ],
      "metadata": {
        "id": "EAFgAvLd2Zd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Property Utils"
      ],
      "metadata": {
        "id": "4TvyXotIQdCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(g, h):\n",
        "    intersection = len(set.intersection(set(g.edges), set(h.edges)))\n",
        "    union = len(set.union(set(g.edges), set(h.edges)))\n",
        "    return round(intersection/union, 3)"
      ],
      "metadata": {
        "id": "abTj1ZDtQgUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_adamic_adar_index(graph, adj, indices):\n",
        "    cnt = 0\n",
        "    total = 0\n",
        "    for i in Utils.bool_to_idx(indices):\n",
        "      for k, j in Utils.bool_to_idx(adj[i] != 0):\n",
        "        total += list(nx.adamic_adar_index(graph, [[i.item(), j.item()]]))[0][2]\n",
        "        cnt += 1\n",
        "    return total / cnt"
      ],
      "metadata": {
        "id": "b0mKjykkOlM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import dijkstra\n",
        "\n",
        "def calc_distance_matrix(adj, indices):\n",
        "    temp_graph = csr_matrix(adj)\n",
        "    dist_matrix = dijkstra(csgraph=temp_graph, directed=False, indices=indices, return_predecessors=False)\n",
        "    return dist_matrix\n",
        "\n",
        "def avg_shortest_path(adj, indices):\n",
        "    dist_mat = calc_distance_matrix(adj, indices)\n",
        "    disconnected_nodes = []\n",
        "    for i in range(len(dist_mat[0])):\n",
        "      if float.is_integer(dist_mat[0][i]) == False:\n",
        "        disconnected_nodes.append(i)\n",
        "\n",
        "    total = 0\n",
        "    length = dist_mat.shape[1]\n",
        "\n",
        "    row_mask = g0.clone()\n",
        "    row_mask[disconnected_nodes] = False\n",
        "    for row in dist_mat[row_mask]:\n",
        "        row[disconnected_nodes] = 0\n",
        "        total += (float(sum(row)) / float(length-len(disconnected_nodes)))\n",
        "\n",
        "    return (total / row_mask.sum())"
      ],
      "metadata": {
        "id": "bGiEZYsZGOHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#temp = list(nx.katz_centrality(originalGraph))\n"
      ],
      "metadata": {
        "id": "XM5ZYbSOMuZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tu05ik2pM1uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Properties"
      ],
      "metadata": {
        "id": "qnw1bn9cHptu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alt_edge_index = alt_adj.nonzero().t().contiguous()"
      ],
      "metadata": {
        "id": "7-0JlOYzMG_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Original Graph and Altered Graph as NetworkX Graphs for Analysis\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "originalData = Data(x=graph.features, y=graph.labels, edge_index=graph.edge_index)\n",
        "alteredData = Data(x=graph.features, y=graph.labels, edge_index=alt_edge_index)\n",
        "\n",
        "originalGraph = to_networkx(originalData, to_undirected=True)\n",
        "alteredGraph = to_networkx(alteredData, to_undirected=True)"
      ],
      "metadata": {
        "id": "25cWeiqVLWez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Node Level Analysis #"
      ],
      "metadata": {
        "id": "Y92EsI24OQJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- avg node degree --\\n\")\n",
        "print(f\"  originalGraph|\")\n",
        "print(f\"\\t Protected: {NXG.avg_degree(originalGraph, g0):.3f}\")\n",
        "print(f\"\\tAuthorized: {NXG.avg_degree(originalGraph, gX):.3f}\\n\")\n",
        "\n",
        "print(f\"  alteredGraph|\")\n",
        "print(f\"\\t Protected: {NXG.avg_degree(alteredGraph, g0):.3f}\")\n",
        "print(f\"\\tAuthorized: {NXG.avg_degree(alteredGraph, gX):.3f}\")"
      ],
      "metadata": {
        "id": "sIuIJy-GRILj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- avg centrality --\\n\")\n",
        "print(f\"  originalGraph|\")\n",
        "print(f\"\\t Protected: {NXG.avg_centrality(originalGraph, g0):.3f}\")\n",
        "print(f\"\\tAuthorized: {NXG.avg_centrality(originalGraph, gX):.3f}\\n\")\n",
        "\n",
        "print(f\"  alteredGraph|\")\n",
        "print(f\"\\t Protected: {NXG.avg_centrality(alteredGraph, g0):.3f}\")\n",
        "print(f\"\\tAuthorized: {NXG.avg_centrality(alteredGraph, gX):.3f}\")"
      ],
      "metadata": {
        "id": "j4HxTJtmAm-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- avg clustering --\\n\")\n",
        "print(f\"  originalGraph|\")\n",
        "print(f\"\\t Protected: {NXG.avg_clustering(originalGraph, g0):.3f}\")\n",
        "print(f\"\\tAuthorized: {NXG.avg_clustering(originalGraph, (originalGraph.nodes)):.3f}\\n\")\n",
        "\n",
        "print(f\"  alteredGraph|\")\n",
        "print(f\"\\t Protected: {NXG.avg_clustering(alteredGraph, g0):.3f}\")\n",
        "print(f\"\\tAuthorized: {NXG.avg_clustering(alteredGraph, gX):.3f}\")"
      ],
      "metadata": {
        "id": "EqHXhRy7A04C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Link Level Analysis"
      ],
      "metadata": {
        "id": "-d3qhgYOBKqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_mat = avg_shortest_path(graph.adj, originalGraph.nodes)\n",
        "\n",
        "disconnected_nodes = []\n",
        "for i in range(len(dist_mat[0])):\n",
        "  if float.is_integer(dist_mat[0][i]) == False:\n",
        "    disconnected_nodes.append(i)"
      ],
      "metadata": {
        "id": "0RSSDiv_oJ1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg distance of protected nodes to all other nodes #\n",
        "total = 0\n",
        "length = dist_mat.shape[1]\n",
        "\n",
        "row_mask = g0.clone()\n",
        "row_mask[disconnected_nodes] = False\n",
        "\n",
        "for row in dist_mat[row_mask]:\n",
        "    row[disconnected_nodes] = 0\n",
        "    total += (float(sum(row)) / float(length-len(disconnected_nodes)))\n",
        "\n",
        "total = total / row_mask.sum()"
      ],
      "metadata": {
        "id": "SPyFi7PRLqy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total"
      ],
      "metadata": {
        "id": "N_TWJ8iAMdq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- avg shortest path to all other nodes --\\n\")\n",
        "print(f\"  originalGraph|\")\n",
        "print(f\"\\t Protected: {avg_shortest_path(graph.adj, g0):.4f}\")\n",
        "print(f\"\\tAuthorized: {avg_shortest_path(graph.adj, gX):.4f}\\n\")\n",
        "\n",
        "print(f\"  alteredGraph|\")\n",
        "print(f\"\\t Protected: {avg_shortest_path(alt_adj, g0):.4f}\")\n",
        "print(f\"\\tAuthorized: {avg_shortest_path(alt_adj, gX):.4f}\")"
      ],
      "metadata": {
        "id": "sQvhvnU0LSOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- jaccard similarity --\\n\")\n",
        "print(\"originalGraph vs alteredGraph\")\n",
        "print(jaccard_similarity(originalGraph, alteredGraph))"
      ],
      "metadata": {
        "id": "f5dJkgarqS8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- avg adamic-adar index --\\n\")\n",
        "print(f\"  originalGraph|\")\n",
        "print(f\"\\t Protected: {avg_adamic_adar_index(originalGraph, graph.adj, g0):.4f}\")\n",
        "print(f\"\\tAuthorized: {avg_adamic_adar_index(originalGraph, graph.adj, gX):.4f}\\n\")\n",
        "\n",
        "print(f\"  alteredGraph|\")\n",
        "print(f\"\\t Protected: {avg_adamic_adar_index(alteredGraph, alt_adj, g0):.4f}\")\n",
        "print(f\"\\tAuthorized: {avg_adamic_adar_index(alteredGraph, alt_adj, gX):.4f}\")"
      ],
      "metadata": {
        "id": "UHNYu8cu9CoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "guZEndu287qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph Level Analysis"
      ],
      "metadata": {
        "id": "C-wSJadpqqYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "link level features:\n",
        "    Shortest-path distance between two nodes, Common neighbors,\n",
        "    Jaccard’s coefficient, Adamic-Adar index, Katz index\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uPr8aQD3PFKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "graph level features: Graph Kernels\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "d0OsM8mOPUt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deleted Edge Rankings"
      ],
      "metadata": {
        "id": "eYR_WEu92DzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff = alt_adj - graph.adj\n",
        "deleted_edge_adj_mask = (diff == -1)\n",
        "print(f\"Number of edges deleted: {deleted_edge_adj_mask.sum()}\")"
      ],
      "metadata": {
        "id": "OijhhhdjthbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge removals typically occur between g0-gX of same label\n",
        "Metrics.show_metrics(diff, graph.labels, g0, graph.device)"
      ],
      "metadata": {
        "id": "w9DUYXryoWF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_adj_idx = Utils.bool_to_idx(diff == -1)\n",
        "# deleted_index = deleted_adj_idx.nonzero().t().contiguous()"
      ],
      "metadata": {
        "id": "pb0ZogVauB4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanation.edge_index"
      ],
      "metadata": {
        "id": "cMX8K9K3RsRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_adj_idx"
      ],
      "metadata": {
        "id": "hverT-QYS6jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_mask = graph.edge_index[0] != graph.edge_index[0]"
      ],
      "metadata": {
        "id": "gHk2MOxzIAte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in deleted_edge_adj_idx:\n",
        "  for i in range(graph.edge_index.shape[1]):\n",
        "    if graph.edge_index[0, i] == pair[0] and graph.edge_index[1, i] == pair[1]:\n",
        "      deleted_edge_mask[i] = True"
      ],
      "metadata": {
        "id": "3YU0d3yYIvJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_mask.sum()"
      ],
      "metadata": {
        "id": "SGHOrL4uD_SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_mask_idx = Utils.bool_to_idx(deleted_edge_mask).reshape(-1)\n",
        "deleted_edge_mask_idx"
      ],
      "metadata": {
        "id": "No29VEBjIKHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanation.edge_index[:,124]"
      ],
      "metadata": {
        "id": "XMbIexK3Evab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_edges_deleted_per_node = {}\n",
        "\n",
        "for i in range(graph.num_nodes):\n",
        "  if ((diff[i] == -1).sum() > 0):\n",
        "    num_edges_deleted_per_node[i] = int((diff[i] == -1).sum())"
      ],
      "metadata": {
        "id": "Q1TMNktpFBYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_edges_deleted_per_node = dict(sorted(num_edges_deleted_per_node.items(), key=lambda x:x[1], reverse=True))"
      ],
      "metadata": {
        "id": "poRSx8bFHIb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_edges_deleted_per_node"
      ],
      "metadata": {
        "id": "0zdqmmLVHaxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of protected nodes that had edge removed\n",
        "g0[list(num_edges_deleted_per_node.keys())].sum()"
      ],
      "metadata": {
        "id": "ifjdvDoSsbaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_weight_dic = {}\n",
        "\n",
        "for idx in deleted_edge_mask_idx:\n",
        "  edge_weight = explanation.edge_mask[idx]\n",
        "  pair = explanation.edge_index[:,idx]\n",
        "  deleted_edge_weight_dic[pair] = edge_weight"
      ],
      "metadata": {
        "id": "aYfAF1Ta_LHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_weight_dic = dict(sorted(deleted_edge_weight_dic.items(), key=lambda x:x[1], reverse=True))"
      ],
      "metadata": {
        "id": "whOJP4DK_TKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_edge_weight_dic"
      ],
      "metadata": {
        "id": "MJignEI1teBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in list(deleted_edge_weight_dic.keys()):\n",
        "  print(f\"Edge: {pair}\\tWeight: {deleted_edge_weight_dic[pair]}\\tSame node type: {g0[pair[0]]==g0[pair[1]]}\\tSame label: {graph.labels[pair[0]]==graph.labels[pair[1]]}\")"
      ],
      "metadata": {
        "id": "d0BbgAnu_PUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIc1oiI_JZW3"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Sample Evaluation"
      ],
      "metadata": {
        "id": "44aF5J9fbh5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"baseline\"\n",
        ").to(device)\n",
        "\n",
        "baseline.fit(graph.features, graph.adj, graph.labels, graph.idx_train, graph.idx_test, args.reg_epochs)\n",
        "\n",
        "baseline_pred = baseline.predict(graph.features, graph.adj)\n",
        "baseline_acc = Metrics.partial_acc(baseline_pred, graph.labels, g0, gX)\n",
        "\n",
        "print('Baseline Acc:')\n",
        "print(baseline_acc)\n"
      ],
      "metadata": {
        "id": "rdyNZ1U_1tDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_post_pred = baseline.post(graph.features, graph.adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "baseline_post_acc = Metrics.partial_acc(baseline_post_pred, graph.labels, g0, gX)\n",
        "\n",
        "print('Baseline Post Acc:')\n",
        "print(baseline_post_acc)"
      ],
      "metadata": {
        "id": "hb-xZdfP1_kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation function"
      ],
      "metadata": {
        "id": "l9KNh3rhbeCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, features, adj, labels, idx_train, idx_val, idx_test, name=\"\"):\n",
        "  model.fit(features, adj, labels, idx_train, idx_test, args.reg_epochs)\n",
        "\n",
        "  model_pred = model.predict(features, adj, labels, idx_train, idx_val, idx_test)\n",
        "  model_acc = Metrics.partial_acc(model_pred, labels, g0, gX)\n",
        "\n",
        "  print('{} Acc:'.format(name))\n",
        "  print(model_acc)\n",
        "\n",
        "  return model_pred, model_acc"
      ],
      "metadata": {
        "id": "N1q3fFbVZtsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"baseline\"\n",
        ").to(device)\n",
        "baseline_pred, baseline_acc = evaluation(baseline, graph.features, graph.adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"baseline\")"
      ],
      "metadata": {
        "id": "gNFPEus-bMtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_post_pred = baseline.post(graph.features, graph.adj, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "baseline_post_acc = Metrics.partial_acc(baseline_post_pred, graph.labels, g0, gX)\n",
        "\n",
        "print('baseline post Acc:')\n",
        "print(baseline_post_acc)"
      ],
      "metadata": {
        "id": "atk7LDwCbl1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_adj_gcn = Utils.get_modified_adj(graph.adj, best_gcn_perturbation)\n",
        "locked_gcn = GCN(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=\"locked_gcn\"\n",
        ").to(device)\n",
        "locked_gcn_pred, locked_gcn_acc = evaluation(locked_gcn, graph.features, locked_adj_gcn, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"locked_gcn\")"
      ],
      "metadata": {
        "id": "MS22cHyu16bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_gcn_post_pred = locked_gcn.post(graph.features, locked_adj_gcn, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "locked_gcn_post_acc = Metrics.partial_acc(locked_gcn_post_pred, graph.labels, g0, gX)\n",
        "print('locked gcn post Acc:')\n",
        "print(locked_gcn_post_acc)"
      ],
      "metadata": {
        "id": "pK7CBmzXtsnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cs = Correct_and_Smooth(locked_adj_gcn, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "manual_locked_gcn_post_pred = cs.correct_and_smooth(locked_gcn_pred, a1=0.9, a2=0.9, eps=1e-6)\n",
        "manual_locked_gcn_post_acc = Metrics.partial_acc(manual_locked_gcn_post_pred, graph.labels, g0, gX)\n",
        "print('manual locked gcn post Acc:')\n",
        "print(manual_locked_gcn_post_acc)"
      ],
      "metadata": {
        "id": "rSS0G86b0L6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment: Correct and Smooth on the predictions of locked_gcn model shows no significant improvement. It suggests that the locked model achieves its goal in protecting the protected nodes. Hence, there is no need to improve the locked model to secure againts correct and smooth."
      ],
      "metadata": {
        "id": "vYqr9c2p5o5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "locked_adj_gcn_cs = Utils.get_modified_adj(graph.adj, best_gcn_cs_perturbation)\n",
        "locked_gcn_cs = GCN_CS(\n",
        "    dim_input_features= graph.num_features,\n",
        "    dim_output_classes= graph.labels.max().item()+1,\n",
        "    num_hidden_layers=args.hidden_layers,\n",
        "    device=device,\n",
        "    lr=args.model_lr,\n",
        "    dropout=args.dropout,\n",
        "    weight_decay=args.weight_decay,\n",
        "    name=f\"locked_gcn_cs\"\n",
        ").to(device)\n",
        "locked_gcn_cs_pred, locked_gcn_cs_acc = evaluation(locked_gcn_cs, graph.features, locked_adj_gcn_cs, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test, \"locked_gcn_cs\")"
      ],
      "metadata": {
        "id": "5yT2TQoio0fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locked_gcn_cs_post_pred = locked_gcn_cs.post(graph.features, locked_adj_gcn_cs, graph.labels, graph.idx_train, graph.idx_val, graph.idx_test)\n",
        "locked_gcn_cs_post_acc = Metrics.partial_acc(locked_gcn_cs_post_pred, graph.labels, g0, gX)\n",
        "\n",
        "print('locked gcn cs post Acc:')\n",
        "print(locked_gcn_cs_post_acc)"
      ],
      "metadata": {
        "id": "kd9hl15SubDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment: Using surrogate locked_gcn_cs gives higher accuracy to protected nodes. This indicates that the using locked_gcn_cs model gives worst result than locked_gcn_model."
      ],
      "metadata": {
        "id": "fG-MELiE59wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "z8kmOXqG1tpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: What's the following for?"
      ],
      "metadata": {
        "id": "MIM6tc4odwG6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ep31bbhVTwO"
      },
      "outputs": [],
      "source": [
        "args.reg_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsrOqDFdUybD"
      },
      "outputs": [],
      "source": [
        "# Timer\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzdBITnOMNNJ"
      },
      "outputs": [],
      "source": [
        "base_acc = {'g0':0, 'gX':0}\n",
        "locked_accu = {'g0':0, 'gX':0}\n",
        "\n",
        "gcn_time = []\n",
        "\n",
        "avg_dgX = 0\n",
        "avg_dg0 = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cs = Correct_and_Smooth(graph)"
      ],
      "metadata": {
        "id": "N2ZjuUIgONZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from torch_geometric.nn.models.correct_and_smooth import CorrectAndSmooth\n",
        "cs = CorrectAndSmooth(num_correction_layers=5, correction_alpha=0.8,\n",
        "                      num_smoothing_layers=5, smoothing_alpha=0.8)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AVDgFzswQ0QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "print(graph.labels.size(0))\n",
        "numel = int(graph.idx_train.sum()) if graph.idx_train.dtype == torch.bool else graph.idx_train.size(0)\n",
        "train_indices = (Utils.bool_to_idx(graph.idx_train))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A3wP8_GbVbGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7qgx9qOyK66"
      },
      "outputs": [],
      "source": [
        "ep = 5\n",
        "for i in range(ep):\n",
        "    start = time.time()\n",
        "\n",
        "    baseline_model = GNN(\n",
        "        input_features=graph.features.shape[1],\n",
        "        output_classes=graph.labels.max().item()+1,\n",
        "        hidden_layers=args.hidden_layers,\n",
        "        device=device,\n",
        "        lr=args.model_lr,\n",
        "        dropout=args.dropout,\n",
        "        weight_decay=args.weight_decay,\n",
        "        name=f\"baseline\"\n",
        "        ).to(device)\n",
        "\n",
        "    baseline_model.fit(graph, args.reg_epochs)\n",
        "\n",
        "    base_pred = baseline_model.post(graph.features, graph.adj)\n",
        "    #base_pred = baseline_model(graph.features, graph.adj)\n",
        "    #base_pred = cs.correct_and_smooth(base_pred, a1=0.9, a2=0.9, eps=1e-6, show_p=True)\n",
        "\n",
        "    baseline_acc = Metrics.partial_acc(base_pred, graph.labels, g0, gX)\n",
        "\n",
        "    print('Baseline Acc:')\n",
        "    print(baseline_acc)\n",
        "\n",
        "    locked_adj = Utils.get_modified_adj(graph.adj, best)\n",
        "\n",
        "    locked_model = GNN(\n",
        "        input_features=graph.features.shape[1],\n",
        "        output_classes=graph.labels.max().item()+1,\n",
        "        hidden_layers=args.hidden_layers,\n",
        "        device=device,\n",
        "        lr=args.model_lr,\n",
        "        dropout=args.dropout,\n",
        "        weight_decay=args.weight_decay,\n",
        "        name=f\"locked\")\n",
        "\n",
        "    locked_model.fitManual(graph.features, locked_adj, graph.labels, graph.idx_train, graph.idx_test, args.reg_epochs)\n",
        "    locked_pred = locked_model.post(graph.features, locked_adj).to(device)\n",
        "    #locked_pred = locked_model(graph.features, locked_adj).to(device)\n",
        "    #locked_pred = cs.correct_and_smooth(locked_pred, a1=0.9, a2=0.9, eps=1e-5)\n",
        "\n",
        "    locked_acc = Metrics.partial_acc(locked_pred, graph.labels, g0, gX)\n",
        "\n",
        "    print('Locked Acc:')\n",
        "    print(locked_acc)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    base_acc['g0'] += baseline_acc['g0']\n",
        "    base_acc['gX'] += baseline_acc['gX']\n",
        "\n",
        "    locked_accu['g0'] += locked_acc['g0']\n",
        "    locked_accu['gX'] += locked_acc['gX']\n",
        "\n",
        "    gcn_time.append(end-start)\n",
        "\n",
        "    dg0 = (locked_acc[\"g0\"] - baseline_acc[\"g0\"]) / baseline_acc[\"g0\"]\n",
        "    dgX = (locked_acc[\"gX\"] - baseline_acc[\"gX\"]) / baseline_acc[\"gX\"]\n",
        "\n",
        "    avg_dgX += dgX\n",
        "    avg_dg0 += dg0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHlgJO885Hfp"
      },
      "outputs": [],
      "source": [
        "print(f\"base_acc: gX={base_acc['gX'] / ep:.1%} g0={base_acc['g0'] / ep:.1%}\")\n",
        "print(f\"locked_acc: gX={locked_accu['gX'] / ep:.1%} g0={locked_accu['g0'] / ep:.1%}\\n\")\n",
        "\n",
        "print(f'avg_dgX: {avg_dgX / ep:.1%}')\n",
        "print(f'avg_dg0: {avg_dg0 / ep:.1%}\\n')\n",
        "\n",
        "loss = F.cross_entropy(locked_pred[g0], graph.labels[g0]) - F.cross_entropy(locked_pred[gX], graph.labels[gX])\n",
        "print(f'loss: {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvRZ4nM2go-W"
      },
      "outputs": [],
      "source": [
        "dg0 = (locked_accu[\"g0\"] - base_acc[\"g0\"]) / base_acc[\"g0\"]\n",
        "dgX = (locked_accu[\"gX\"] - base_acc[\"gX\"]) / base_acc[\"gX\"]\n",
        "\n",
        "print(\"==== Accuracies ====\")\n",
        "print(f\"         ΔG0\\tΔGX\")\n",
        "print(f\"task1 | {dg0:.1%}\\t{dgX:.1%}\")\n",
        "print()\n",
        "diff = locked_adj - graph.adj\n",
        "\n",
        "diffSummary = Metrics.show_metrics(diff, graph.labels, g0, device)\n",
        "print(diffSummary)\n",
        "\n",
        "########################\n",
        "#region Saving perturbations\n",
        "\n",
        "#from Utils import Export\n",
        "\n",
        "# if args.save_perturbations != 'N':\n",
        "#     Export.save_var(f'pertubations-{args.dataset}@{args.ptb_rate}', [best.cpu().numpy().tolist(), g0.cpu().numpy().tolist()], \"./perturbations.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nojHTbqS_JTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMjDRAvsVlm0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "args.num_gc_layers = 2\n",
        "args.mask_act = 'ReLU'\n",
        "args.mask_bias = False\n",
        "args.gpu = True #\n",
        "args.opt = 'adam' # optimizer used\n",
        "args.lr = args.model_lr # learning rate\n",
        "args.opt_scheduler = 'step' #\n",
        "\n",
        "args.opt_decay_step = 0\n",
        "args.opt_decay_rate = args.reg_epochs # Number of epochs before decay\n",
        "args.opt_restart = None\n",
        "args.num_epochs = args.reg_epochs\n",
        "# Explain Predictions\n",
        "explainer = Explainer(model=locked_model, adj=locked_adj,\n",
        "                         feat=graph.features, label=graph.labels,\n",
        "                         pred=locked_pred, train_idx=graph.idx_train,\n",
        "                         args=args, graph_mode=False, graph_idx=False)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWAhlugUWTuz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "idx = Utils.bool_to_idx(g0).detach().cpu().numpy().reshape(1, -1)[0]\n",
        "print(idx)\n",
        "explainer.explain_nodes(idx, args)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvXKGq4DxvzC"
      },
      "source": [
        "# Feature Measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67XfR_Hvx7R9"
      },
      "outputs": [],
      "source": [
        "## Get indices of (in)correctly predicted nodes ##\n",
        "base_incorrect = NXG.get_correct_idx(baseline_post_pred, graph.labels, get_correct=False).cpu()\n",
        "base_correct = NXG.get_correct_idx(baseline_post_pred, graph.labels, get_correct=True).cpu()\n",
        "\n",
        "locked_incorrect = NXG.get_correct_idx(locked_gcn_cs_post_pred, graph.labels, get_correct=False).cpu()\n",
        "locked_correct = NXG.get_correct_idx(locked_gcn_cs_post_pred, graph.labels, get_correct=True).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn9O-6GAyBeO"
      },
      "outputs": [],
      "source": [
        "print(\"-- precentage of protected nodes in incorrectly predicted nodes --\")\n",
        "print(f\"      Base |  {NXG.protected_node_ratio(base_incorrect, g0):.2%}\")\n",
        "print(f\"    Locked |  {NXG.protected_node_ratio(locked_incorrect, g0):.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwpxX5za2wQN"
      },
      "outputs": [],
      "source": [
        "print(\"-- avg node clustering coefficient --\")\n",
        "print(f\"      Base|\")\n",
        "print(f\"\\t  Correct: {NXG.avg_clustering(graph.nx_g, base_correct):.3f}   |   Label count: {NXG.label_count(base_correct)}\")\n",
        "print(f\"\\tIncorrect: {NXG.avg_clustering(graph.nx_g, base_incorrect):.3f}   |   Label count: {NXG.label_count(base_incorrect)}\\n\")\n",
        "\n",
        "print(f\"    Locked|\")\n",
        "print(f\"\\t  Correct: {NXG.avg_clustering(graph.nx_g, locked_correct):.3f}   |   Label count: {NXG.label_count(locked_correct)}\")\n",
        "print(f\"\\tIncorrect: {NXG.avg_clustering(graph.nx_g, locked_incorrect):.3f}   |   Label count: {NXG.label_count(locked_incorrect)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4mtZIPvTHoZ"
      },
      "outputs": [],
      "source": [
        "## 100 epoch avg random-walk\n",
        "avg_r_walk = NXG.avg_random_walk_protected_ratio(graph.nx_g, g0, walk_len=5, n_walks=100)\n",
        "\n",
        "print(\"-- avg random walk protected ratio: (protected nodes visited / total nodes visited) --\")\n",
        "print(f\"      Base|\")\n",
        "print(f\"\\t  Correct: {avg_r_walk[base_correct].sum() / len(base_correct):.5f}\")\n",
        "print(f\"\\tIncorrect: {avg_r_walk[base_incorrect].sum() / len(base_incorrect):.5f}\\n\")\n",
        "\n",
        "print(f\".   Locked|\")\n",
        "print(f\"\\t  Correct: {avg_r_walk[locked_correct].sum() / len(locked_correct):.5f}\")\n",
        "print(f\"\\tIncorrect: {avg_r_walk[locked_incorrect].sum() / len(locked_incorrect):.5f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4PS2DYR21Y_"
      },
      "outputs": [],
      "source": [
        "print(\"-- avg degree centrality --\\n\")\n",
        "print(f\"      Base|\")\n",
        "print(f\"\\t  Correct: {NXG.avg_centrality(graph.nx_g, base_correct):.5f}\")\n",
        "print(f\"\\tIncorrect: {NXG.avg_centrality(graph.nx_g, base_incorrect):.5f}\\n\")\n",
        "\n",
        "print(f\"    Locked|\")\n",
        "print(f\"\\t  Correct: {NXG.avg_centrality(graph.nx_g, locked_correct):.5f}\")\n",
        "print(f\"\\tIncorrect: {NXG.avg_centrality(graph.nx_g, locked_incorrect):.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- avg node degree (post-CS predcitions) --\")\n",
        "print(f\"      Base|\")\n",
        "print(f\"\\t  Correct: {NXG.avg_degree(graph.nx_g, base_correct):.3f}\")\n",
        "print(f\"\\tIncorrect: {NXG.avg_degree(graph.nx_g, base_incorrect):.3f}\\n\\n\")\n",
        "\n",
        "print(f\"    Locked|\")\n",
        "print(f\"\\t  Correct: {NXG.avg_degree(graph.nx_g, locked_correct):.3f}\")\n",
        "print(f\"\\tIncorrect: {NXG.avg_degree(graph.nx_g, locked_incorrect):.3f}\\n\\n\")"
      ],
      "metadata": {
        "id": "4byycv9fdfDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_protected_ratio = NXG.protected_neighbor_ratio(graph.nx_g, g0)\n",
        "\n",
        "print(\"-- avg ratio degree: (protected neighbors / total neighbors) --\")\n",
        "print(f\"      Base|\")\n",
        "print(f\"\\t  Correct: {avg_protected_ratio[base_correct].sum() / len(base_correct):.5f}\")\n",
        "print(f\"\\tIncorrect: {avg_protected_ratio[base_incorrect].sum() / len(base_incorrect):.5f}\\n\")\n",
        "\n",
        "print(f\"    Locked|\")\n",
        "print(f\"\\t  Correct: {avg_protected_ratio[locked_correct].sum() / len(locked_correct):.5f}\")\n",
        "print(f\"\\tIncorrect: {avg_protected_ratio[locked_incorrect].sum() / len(locked_incorrect):.5f}\\n\")"
      ],
      "metadata": {
        "id": "_8I91LSVdsHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o7HIId92_t3"
      },
      "outputs": [],
      "source": [
        "k_neighborhood_ratio = NXG.k_nearest_neighbor_ratio(graph.nx_g, g0, k=2)\n",
        "\n",
        "print(\"-- avg k-neighborhood protected nodes ratio: (protected nodes / total nodes) --\")\n",
        "print(f\"  Baseline|\")\n",
        "print(f\"\\t  Correct: 1-hop: {NXG.avg_k_hop_ratio(base_correct, 1, k_neighborhood_ratio):.5f} \\\n",
        " |  2-hop: {NXG.avg_k_hop_ratio(base_correct, 2, k_neighborhood_ratio):.5f}\")\n",
        "print(f\"\\tIncorrect: 1-hop: {NXG.avg_k_hop_ratio(base_incorrect, 1, k_neighborhood_ratio):.5f} \\\n",
        " |  2-hop: {NXG.avg_k_hop_ratio(base_incorrect, 2, k_neighborhood_ratio):.5f}\\n\")\n",
        "\n",
        "print(f\"    Locked|\")\n",
        "print(f\"\\t  Correct: 1-hop: {NXG.avg_k_hop_ratio(locked_correct, 1, k_neighborhood_ratio):.5f} \\\n",
        " |  2-hop: {NXG.avg_k_hop_ratio(locked_correct, 2, k_neighborhood_ratio):.5f}\")\n",
        "print(f\"\\tIncorrect: 1-hop: {NXG.avg_k_hop_ratio(locked_incorrect, 1, k_neighborhood_ratio):.5f} \\\n",
        " |  2-hop: {NXG.avg_k_hop_ratio(locked_correct, 2, k_neighborhood_ratio):.5f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop"
      ],
      "metadata": {
        "id": "TQIGFr1Jganz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNzEZFPqJyx8"
      },
      "source": [
        "# Summarize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3hdCzeyidcu"
      },
      "outputs": [],
      "source": [
        "################################################\n",
        "# Save\n",
        "################################################\n",
        "'''\n",
        "if (args.save == \"Y\"):\n",
        "    #import Utils.Export as Export\n",
        "\n",
        "    def getDiff(location, changeType):\n",
        "        num = diffSummary[location][changeType][\"total\"]\n",
        "        if num == 0: return f\"{num:.0f}\"\n",
        "        pct = diffSummary[location][changeType][\"same\"] / num\n",
        "        return f\"{num:.0f} ({pct:.2%} similar)\"\n",
        "\n",
        "    results = {\n",
        "        \"seed\": args.seed,\n",
        "        \"dataset\": args.dataset,\n",
        "        \"protect_size\": args.protect_size,\n",
        "        \"reg_epochs\": args.reg_epochs,\n",
        "        \"ptb_epochs\": args.ptb_epochs,\n",
        "        \"ptb_rate\": args.ptb_rate,\n",
        "        \"ptb_sample_num\": args.num_samples,\n",
        "        \"ptb_sample_size\": args.sample_size,\n",
        "        \"ratio_g0\": samplingMatrix.g0_ratio.item(),\n",
        "        \"ratio_gX\": samplingMatrix.gX_ratio.item(),\n",
        "        \"ratio_g0gX\": samplingMatrix.g0gX_ratio.item(),\n",
        "        \"base_g0\": baseline_acc[\"g0\"],\n",
        "        \"base_gX\": baseline_acc[\"gX\"],\n",
        "        \"d_g0\": dg0,\n",
        "        \"d_gX\": dgX,\n",
        "        \"edges\": diff.abs().sum().item(),\n",
        "        \"add_g0\": getDiff(\"g0\", \"add\"),\n",
        "        \"add_gX\": getDiff(\"gX\", \"add\"),\n",
        "        \"add_g0gX\": getDiff(\"g0gX\", \"add\"),\n",
        "        \"remove_g0\": getDiff(\"g0\", \"remove\"),\n",
        "        \"remove_gX\": getDiff(\"gX\", \"remove\"),\n",
        "        \"remove_g0gX\": getDiff(\"g0gX\", \"remove\"),\n",
        "\n",
        "    }\n",
        "\n",
        "    Export.saveData(args.save_location, results)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3aKEPdeNIst"
      },
      "source": [
        "# Graphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FccfXp3NJxO"
      },
      "outputs": [],
      "source": [
        "#NXG.plot_edge_changes(graph.nx_g, locked_adj, graph.adj)\n",
        "\n",
        "#NXG.plot_incorrect_subgraph_k_neighborhood(graph.nx_g, postCS_incorrect, k_hops=2, with_labels=True, protected_set=g0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1gyIz2gNkeA"
      },
      "outputs": [],
      "source": [
        "#NXG.plot_wrong_pred(nxg, cs_locked_pred, graph.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GV0jEvcM1Qk"
      },
      "outputs": [],
      "source": [
        "#NXG.plot_sets(graph.nx_g, g0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FPl6j6_TIQzo",
        "KilfcTHnCp2u",
        "8mqtcEnSCtR9",
        "ZOIBE78ICxPH",
        "dBMaJM-O_fkC",
        "2-RL3WcGINfJ",
        "m_zW17gsJsno",
        "UcUtPEMvIYEx",
        "v3GeEB_9zR0R",
        "Og-ZR9iB6jud",
        "57n7bfprFJeg",
        "DkKV4kAY3gSZ",
        "IdqgQ2648Vlv",
        "MohyT9B2I-zo",
        "KpYq-kQKJCUc",
        "tG_c0QTiJGiN",
        "cNB4Un4Nrzv0",
        "xzVbdSl6fcIZ",
        "uY93kOvOlvqr",
        "e_IWWvTNI-Ko",
        "v3VejUPQHLZt",
        "BPKop7avszQi",
        "4TvyXotIQdCm",
        "qnw1bn9cHptu",
        "44aF5J9fbh5o",
        "l9KNh3rhbeCX",
        "vYqr9c2p5o5e",
        "MIM6tc4odwG6",
        "xvXKGq4DxvzC",
        "gNzEZFPqJyx8",
        "D3aKEPdeNIst"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}